<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Jupyter 多kernel配置</title>
      <link href="/2019/06/20/jupyter/"/>
      <url>/2019/06/20/jupyter/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>Jupyter NB</b></font></center><a id="more"></a><p>网上的都是抄来抄去的辣鸡文档，直接搜<a href="http://ipython.readthedocs.io/en/latest/install/kernel_install.html" target="_blank" rel="noopener">官方文档</a></p><p>这里我使用 conda 管理多 Python 环境</p><h1 id="Python-多环境"><a href="#Python-多环境" class="headerlink" title="Python 多环境"></a>Python 多环境</h1><h2 id="增加自己的-kernel"><a href="#增加自己的-kernel" class="headerlink" title="增加自己的 kernel"></a>增加自己的 kernel</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要安装：</span></span><br><span class="line">conda install ipykernel</span><br><span class="line"></span><br><span class="line"><span class="comment"># (可选)创建你自己的环境, 指定某python环境</span></span><br><span class="line">conda create -n 环境名称 python=3.6</span><br><span class="line"><span class="comment"># 比如</span></span><br><span class="line">conda create -n wy36 python=3.6</span><br><span class="line"></span><br><span class="line"><span class="comment"># 你要有自己的环境之后且能切换到对应的conda环境, 即下面命令有效</span></span><br><span class="line"><span class="built_in">source</span> activate 环境名称</span><br><span class="line"><span class="comment"># 新版本的 conda 已经将命令改为</span></span><br><span class="line">conda activate 环境名称</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将环境写入notebook的kernel中</span></span><br><span class="line">python -m ipykernel install --user --name 环境名称 --display-name <span class="string">"自定义显示的昵称"</span></span><br><span class="line"><span class="comment"># 比如</span></span><br><span class="line">python -m ipykernel install --user --name wy35 --display-name <span class="string">"Python (wy35)"</span></span><br><span class="line">python -m ipykernel install --user --name nlp --display-name <span class="string">"nlp"</span></span><br><span class="line">python -m ipykernel install --user --name at --display-name <span class="string">"autokeras"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新打开notebook, 即可在新建文件的时候看到选项</span></span><br><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure><h1 id="其他语言"><a href="#其他语言" class="headerlink" title="其他语言"></a>其他语言</h1><h2 id="R-语言"><a href="#R-语言" class="headerlink" title="R 语言"></a>R 语言</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">install.packages(<span class="string">'devtools'</span>)</span><br><span class="line">devtools::install_github(<span class="string">'IRkernel/IRkernel'</span>)</span><br><span class="line">IRkernel::installspec()</span><br><span class="line">install.packages(<span class="string">'ggplot2'</span>)</span><br></pre></td></tr></table></figure><p>IRkernel::installspec()可能出现问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Error <span class="keyword">in</span> IRkernel::installspec() : </span><br><span class="line">  jupyter-client has to be installed but “jupyter kernelspec --version<span class="string">" exited with code 127.</span></span><br></pre></td></tr></table></figure><p>解决方法</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">install.packages(c(<span class="string">'repr'</span>, <span class="string">'IRdisplay'</span>, <span class="string">'evaluate'</span>, <span class="string">'crayon'</span>, <span class="string">'pbdZMQ'</span>, <span class="string">'devtools'</span>, <span class="string">'uuid'</span>, <span class="string">'digest'</span>))</span><br></pre></td></tr></table></figure><h2 id="Julia"><a href="#Julia" class="headerlink" title="Julia"></a>Julia</h2><p><a href="https://github.com/JuliaLang/IJulia.jl" target="_blank" rel="noopener">IJulia</a></p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> Pkg</span><br><span class="line">Pkg.add(<span class="string">"IJulia"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> IJulia</span><br><span class="line">notebook()</span><br><span class="line"></span><br><span class="line">退出 julia prompt 不影响 jupyter 可以使用:</span><br><span class="line">julia&gt; <span class="keyword">using</span> IJulia; notebook(detached=<span class="literal">true</span>)</span><br></pre></td></tr></table></figure><h1 id="安装-Jupyter-notebook-插件"><a href="#安装-Jupyter-notebook-插件" class="headerlink" title="安装 Jupyter notebook 插件"></a>安装 Jupyter notebook 插件</h1><p><a href="https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/" target="_blank" rel="noopener">jupyter-contrib-nbextensions</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 pip 安装</span></span><br><span class="line">pip install jupyter_contrib_nbextensions</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 conda</span></span><br><span class="line">conda install -c conda-forge jupyter_contrib_nbextensions</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 Github repo 安装</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/ipython-contrib/jupyter_contrib_nbextensions.git</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Python </tag>
            
            <tag> Jupyter </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无 sudo 权限下载安装 tmux</title>
      <link href="/2018/09/10/tmux-download-without-sudo/"/>
      <url>/2018/09/10/tmux-download-without-sudo/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>自动安装 tmux 脚本 </b></font></center><a id="more"></a><blockquote><p>很多时候用服务器没有 sudo 权限的时候用起来很麻烦.<br>tmux 是一个很好用的一个终端复用工具, 服务器跑程序的时候很好用.</p></blockquote><p>改编自一个非常旧的版本 <a href="https://gist.github.com/ryin/3106801" target="_blank" rel="noopener">gist</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Script for installing tmux on systems where you don't have root access.</span></span><br><span class="line"><span class="comment"># tmux will be installed in $HOME/local/bin.</span></span><br><span class="line"><span class="comment"># It's assumed that wget and a C/C++ compiler are installed.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># exit on error</span></span><br><span class="line"><span class="built_in">set</span> -e</span><br><span class="line"></span><br><span class="line">TMUX_VERSION=2.3</span><br><span class="line"></span><br><span class="line"><span class="comment"># create our directories</span></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/<span class="built_in">local</span> <span class="variable">$HOME</span>/tmux_tmp</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>/tmux_tmp</span><br><span class="line"></span><br><span class="line"><span class="comment"># download source files for tmux, libevent, and ncurses</span></span><br><span class="line"><span class="comment">#wget -O tmux-$&#123;TMUX_VERSION&#125;.tar.gz https://github.com/tmux/tmux/releases/download/$&#123;TMUX_VERSION&#125;/tmux-$&#123;TMUX_VERSION&#125;.tar.gz </span></span><br><span class="line"><span class="comment">#wget https://github.com/libevent/libevent/releases/download/release-2.1.8-stable/libevent-2.1.8-stable.tar.gz </span></span><br><span class="line"><span class="comment">#wget http://invisible-island.net/datafiles/release/ncurses.tar.gz</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># extract files, configure, and compile</span></span><br><span class="line"></span><br><span class="line"><span class="comment">############</span></span><br><span class="line"><span class="comment"># libevent #</span></span><br><span class="line"><span class="comment">############</span></span><br><span class="line">tar xvzf libevent-2.1.8-stable.tar.gz</span><br><span class="line"><span class="built_in">cd</span> libevent-2.1.8-stable</span><br><span class="line">./configure --prefix=<span class="variable">$HOME</span>/<span class="built_in">local</span> --<span class="built_in">disable</span>-shared</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"></span><br><span class="line"><span class="comment">############</span></span><br><span class="line"><span class="comment"># ncurses  #</span></span><br><span class="line"><span class="comment">############</span></span><br><span class="line">tar xvzf ncurses.tar.gz</span><br><span class="line"><span class="built_in">cd</span> ncurses-6.1</span><br><span class="line">./configure --prefix=<span class="variable">$HOME</span>/<span class="built_in">local</span></span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"></span><br><span class="line"><span class="comment">############</span></span><br><span class="line"><span class="comment"># tmux     #</span></span><br><span class="line"><span class="comment">############</span></span><br><span class="line">tar xvzf tmux-<span class="variable">$&#123;TMUX_VERSION&#125;</span>.tar.gz</span><br><span class="line"><span class="built_in">cd</span> tmux-<span class="variable">$&#123;TMUX_VERSION&#125;</span></span><br><span class="line">./configure CFLAGS=<span class="string">"-I<span class="variable">$HOME</span>/local/include -I<span class="variable">$HOME</span>/local/include/ncurses"</span> LDFLAGS=<span class="string">"-L<span class="variable">$HOME</span>/local/lib -L<span class="variable">$HOME</span>/local/include/ncurses -L<span class="variable">$HOME</span>/local/include"</span></span><br><span class="line">CPPFLAGS=<span class="string">"-I<span class="variable">$HOME</span>/local/include -I<span class="variable">$HOME</span>/local/include/ncurses"</span> LDFLAGS=<span class="string">"-static -L<span class="variable">$HOME</span>/local/include -L<span class="variable">$HOME</span>/local/include/ncurses -L<span class="variable">$HOME</span>/local/lib"</span> make</span><br><span class="line">cp tmux <span class="variable">$HOME</span>/<span class="built_in">local</span>/bin</span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"></span><br><span class="line"><span class="comment"># cleanup</span></span><br><span class="line">rm -rf <span class="variable">$HOME</span>/tmux_tmp</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$HOME</span>/local/bin/tmux is now available. You can optionally add <span class="variable">$HOME</span>/local/bin to your PATH."</span></span><br></pre></td></tr></table></figure><p>结束后把路径加入到环境变量即可.</p><p>如果服务器不具备访问外网或者下载速度很慢的时候, 可以手动注释掉上面下载部分, 本机下载好之后上传到服务器, 也可以进行安装.</p>]]></content>
      
      
      <categories>
          
          <category> Bash </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tools </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>搭建自己的 JetBrains 系列 server</title>
      <link href="/2018/05/29/idea-server/"/>
      <url>/2018/05/29/idea-server/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>搭建自己的 JetBrains server</b></font></center><a id="more"></a><h1 id="宇宙第一-IDE-的License-Sever-搭建"><a href="#宇宙第一-IDE-的License-Sever-搭建" class="headerlink" title="宇宙第一 IDE 的License Sever 搭建"></a>宇宙第一 IDE 的License Sever 搭建</h1><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><hr><p>【PS: 请支持正版！！（我是正版用户）】</p><hr><p>感谢 <a href="http://blog.lanyus.com/archives/174.html" target="_blank" rel="noopener">lanyu 大神</a> 提供的 server</p><p>先找到自己需要的服务器文件</p><p>百度云盘提供下载，来自 <a href="https://github.com/a252937166/new-idea-server" target="_blank" rel="noopener">Github</a>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">https://pan.baidu.com/s/1w3n-KLDqYEWCYJMMf6C-0g</span><br><span class="line">密码：zmya</span><br></pre></td></tr></table></figure><h2 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h2><p>下面脚本的 <code>../IntelliJIDEALicenseServer_linux_amd64</code> 替换成你自己下载的文件和地址</p><p>可以加参数:</p><p><code>-u</code> 设置用户名 $\in$ <code>^[a-zA-Z0-9]+$</code> 中文无效<br><code>-p</code> 设定端口<br><code>-l</code> 指定绑定监听到哪个IP(私人用)<br> <code>-prolongationPeriod</code> 指定过期时间参数，一般不设置</p><p><code>start.sh</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line">rm -f tpid</span><br><span class="line"></span><br><span class="line">nohup ../IntelliJIDEALicenseServer_linux_amd64 &gt; info.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> $! &gt; tpid</span><br></pre></td></tr></table></figure><p><code>stop.sh</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line">tpid=`cat tpid | awk <span class="string">'&#123;print $1&#125;'</span>`</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$&#123;tpid&#125;</span> ]; <span class="keyword">then</span></span><br><span class="line"> tpid=`ps -aef | grep <span class="variable">$tpid</span> | awk <span class="string">'&#123;print $2&#125;'</span> |grep <span class="variable">$tpid</span>`</span><br><span class="line"> <span class="keyword">if</span> [ <span class="variable">$&#123;tpid&#125;</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">kill</span> -9 <span class="variable">$tpid</span></span><br><span class="line"> <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">&gt; tpid</span><br></pre></td></tr></table></figure><p>设置执行权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chmod +x IntelliJIDEALicenseServer_linux_amd64</span><br><span class="line">chmod +x start.sh</span><br><span class="line">chmod +x stop.sh</span><br></pre></td></tr></table></figure><p>启动脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start.sh</span><br></pre></td></tr></table></figure><p>同级目录下面生成 <code>info.log</code> 文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cat info.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2018/05/29 22:28:02 *************************************************************</span><br><span class="line">2018/05/29 22:28:02 ** IntelliJ IDEA License Server                            **</span><br><span class="line">2018/05/29 22:28:02 ** by: ilanyu                                              **</span><br><span class="line">2018/05/29 22:28:02 ** http://www.lanyus.com/                                  **</span><br><span class="line">2018/05/29 22:28:02 ** Alipay donation: lanyu19950316@gmail.com                **</span><br><span class="line">2018/05/29 22:28:02 ** Please support genuine!!!                               **</span><br><span class="line">2018/05/29 22:28:02 ** listen on 0.0.0.0:1027...                               **</span><br><span class="line">2018/05/29 22:28:02 ** You can use http://127.0.0.1:1027 as license server     **</span><br><span class="line">2018/05/29 22:28:02 *************************************************************</span><br></pre></td></tr></table></figure><p>1027 为端口号，配置完成，可以使用 nginx 把域名和 idea 映射到 1017 端口</p><h2 id="主页"><a href="#主页" class="headerlink" title="主页"></a>主页</h2><p>在 Server 同级目录下面 创建同名的 <code>.html</code> 文件即可, 假设使用 <code>IntelliJIDEALicenseServer</code>，就命名 <code>IntelliJIDEALicenseServer.html</code> 如果没有 IntelliJIDEALicenseServer.html，会给出 <code>not found</code> 字符串。 如果使用脚本文件启动，比如start.sh，请把IntelliJIDEALicenseServer.html放在脚本文件的同级目录。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> IDEA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scrapy-deltafetch 的安装</title>
      <link href="/2018/05/13/scrapy-deltafetch/"/>
      <url>/2018/05/13/scrapy-deltafetch/</url>
      
        <content type="html"><![CDATA[<h1 id="scrapy-deltafetch-的安装"><a href="#scrapy-deltafetch-的安装" class="headerlink" title="scrapy-deltafetch 的安装"></a>scrapy-deltafetch 的安装</h1><h2 id="Mac-通过-homebrew-安装"><a href="#Mac-通过-homebrew-安装" class="headerlink" title="Mac 通过 homebrew 安装"></a>Mac 通过 homebrew 安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brew install db</span><br><span class="line">YES_I_HAVE_THE_RIGHT_TO_USE_THIS_BERKELEY_DB_VERSION=1 BERKELEYDB_DIR=/usr/<span class="built_in">local</span>/Cellar/berkeley-db/6.2.32/ pip install bsddb3</span><br></pre></td></tr></table></figure><h2 id="Linux-系列编译运行"><a href="#Linux-系列编译运行" class="headerlink" title="Linux 系列编译运行"></a>Linux 系列编译运行</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">wget http://download.oracle.com/berkeley-db/db-6.2.32.tar.gz</span><br><span class="line">tar xvf db-6.2.32.tar.gz</span><br><span class="line"><span class="built_in">cd</span> db-6.2.32.tar.gz</span><br><span class="line"><span class="built_in">cd</span> build_unix/  </span><br><span class="line">../dist/configure -prefix=/home/XXX/app/BerkeleyDB</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">YES_I_HAVE_THE_RIGHT_TO_USE_THIS_BERKELEY_DB_VERSION=1 BERKELEYDB_DIR=/home/XXX/app/BerkelyDB/ pip install bsddb3</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> Spider </category>
          
          <category> Scrapy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>读 Re-id 论文</title>
      <link href="/2017/11/28/AlignedReID/"/>
      <url>/2017/11/28/AlignedReID/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>读 AlignedReID</b></font></center><a id="more"></a><h1 id="AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification-1"><a href="#AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification-1" class="headerlink" title="AlignedReID: Surpassing Human-Level Performance in Person Re-Identification[^1]"></a>AlignedReID: Surpassing Human-Level Performance in Person Re-Identification[^1]</h1><p>这是旷视的一篇新论文,暂时挂在 arXiv 上</p><p>提出了现有的基于 CNN 学习全局特征的缺点:</p><ol><li>不准确的检测框影响特征学习</li><li>姿态变化和非刚体变化使得度量变得困难</li><li>遮挡问题</li><li>外表特征比较相似</li></ol><p>本文提出了一种方法 <code>AlignedReID</code></p><p>AlignedReID 学习的任然是全局特征, 但学习的时候会执行自动部分对齐, 不需要额外的监督或者明确的姿态估计.</p><p>实验在 Market1501, CUHK03, MARS 和 CUHK-SYSU 数据集上均达到目前最好的成果.</p><p>在 Market1501 和 CUHK03 数据集上得到比人类高的结果…</p><h2 id="AlignedReID"><a href="#AlignedReID" class="headerlink" title="AlignedReID"></a>AlignedReID</h2><p>学习时,分为局部特征和全局特征,在局部特征时引入<code>最短路径损失(shortest path loss)</code>计算两组 local features.</p><p>测试(inference)时,抛弃局部特征,只提取全局特征. 主要原因是 local features 是由 global features 得来的.</p><p>而且没有局部特征的匹配可以节省很大的开销.</p><blockquote><p> We find that only applying the global feature is almost as good as combining global and local features. In other words, the global feature itself, with the aid of local features learning, can greatly address the draw backs we mentioned above, in our new joint learning framework.</p></blockquote><p>使用 AlignedReID,对一张图产生一个全局的特征作为最终输出, 使用 <code>L2距离</code> 作为相似性度量, 全局特征和本地特征是一起学习的.</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1flxt4rkpfpj31bk0jo7gp.jpg" alt></p><p>如图:</p><p>对于每个图像,使用 CNN 提取特征, 本文使用的是 Resnet50. (最后一层卷积层大小 $C \times H \times W$, $C$ 为通道数, $H \times W$是大小.) 全局特征是直接通过在特征图上做global pooling得到. 局部特征是通过 horizontal pooling (global pooling with horizontal orientation) , 先对global feature (C * d)每一行提取特征, 然后做 1 * 1 卷积, 把通道由 C 减到 c,这样, 每一个local feature(c * d vector) 都可以表示成一张行人图片的一个水平方向的分割,最终一张图片可以得到一个 global feature 和 H 个 local features.</p><h3 id="计算-local-feature-的距离"><a href="#计算-local-feature-的距离" class="headerlink" title="计算 local feature 的距离"></a>计算 local feature 的距离</h3><p>f g 表示两张图片, $f_i$ 和 $g_j$ 表示图片的第 i 或 j 行. (猜测,文章中没明确表示)</p><p>即 $f = {  f_1,f_2,\dots,f_H }^T$ , $g = {  g_1,g_2,\dots,g_H }^T$ </p><p>对 f 和 g 值归一化.</p>$$ d_{i,j} = {{e^{\lVert f_i-g_j \rVert_2 - 1 }} \over {e^{\lVert f_i-g_j \rVert_2 + 1 }}} \qquad i,j \in 1,2,3,\dots,H $$<p>然后定义距离矩阵 D 是由第(i,j)个距离为元素组成的矩阵.</p><p>定义 local feature 为(1,1)到(H,H)的最短路径之和.</p><p>具体最短路径为:</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fly3ejj19qj30m206adh7.jpg" alt></p><p>$S_{i,j}$表示从(1, 1)到(i, j)的最短路径的总距离.$S_{H,H}$表示最短路径的总距离.</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fly533ymqsj30n00h6k1n.jpg" alt></p><p>由上图可以看出 A 和 B 之间, A 的 1 和 B 的 4 已经对齐, 称之为 <code>corresponding body parts</code>, 其余没对齐的例如(A1,B1)之间就称之为<code>non-corresponding body parts</code>, non-corresponding body parts 这一部分也是比较有用的, 在之前的计算 $d_{i,j}$的公式中可以看出, 没对齐的部分有较大的 L2 距离, 会导致计算是梯度接近于0, 对整体的最短路径的计算贡献较小, 即两幅图之间的局部距离主要取决于corresponding body parts.</p><h3 id="距离的损失函数"><a href="#距离的损失函数" class="headerlink" title="距离的损失函数"></a>距离的损失函数</h3><p>损失函数基于 TriHard Loss[^2] 的思想, 把最短距离相差最多的另一行人和最短距离最相近的另一个行人组成三元组.</p><p>注意在训练的时候, 取得是global 和 local distance 两种计算相似度.</p><p>但在测试的时候, 这里取的最短距离是指对 global distance 取最短. 并不是同时使用 local distance 原因是效率高, 其次这两个距离并没有实质性的区别(个人理解). </p><h2 id="Mutual-Learning"><a href="#Mutual-Learning" class="headerlink" title="Mutual Learning"></a>Mutual Learning</h2><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fly54315fmj31bc0lyjx1.jpg" alt></p><ul><li><p>为了增强模型效果</p></li><li><p>前人贡献</p><ul><li>Distillation-Based models 将知识从预训练好的 large teacher network 传递给 small student network</li><li>只是采用了 Kullback-leibler 方法[^3] 计算 distance between classification probabilities</li></ul></li><li><p>本文贡献</p><ul><li><p>同时训练一组 student models ,相互教学</p></li><li><p>采取了多个损失函数(4个)</p><p>损失函数一共分为四组:</p></li></ul><ol><li><p>metric loss: global &amp; local distance 决定</p></li><li><p>metric mutual loss: global distance 决定</p></li><li><p>classification loss</p></li><li><p>classification mutual loss: 由 KL divergence  for classification  决定</p><p>$$ L_M = {1\over N^2} \sum \limits_{i}^{N} \sum \limits_{j}^{N}([ZG(M_{ij}^{\theta_1})-M_{ij}^{\theta_2}]^2 + [M_{ij}^{\theta_1}+ZG(M_{ij}^{\theta_2})]^2) $$</p><p>其中 $ZG(\cdot)$ 表示 <code>zero gradient function</code>, 在计算梯度的时候讲变量视为常量, 在学习的时候停止反向传播(没动具体怎么做)</p><p>计算二阶梯度:</p><p>$$ {\partial ^2 L_M \over \partial M_{ij}^{\theta_1} \partial M_{ij}^{\theta_2}} = 0 $$</p><p>优点:</p></li><li><p>可以加速收敛</p></li><li><p>使用 ZG 比使用 mutual loss 的效果更好</p></li></ol></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从实验结果来看..</p><p>基本上不给 reid 其他人活路了.. 去年做到60%以上就可以发论文..今年70-80%.. 这篇直接到了90+%…</p><hr><p>[^1]: X. Zhang, H. Luo, X. Fan, W. Xiang, Y. Sun, Q. Xiao, W. Jiang, C. Zhang, and J. Sun. AlignedReID: Surpassing Human-Level Performance in Person Re-Identification. arXiv :1711.08184, 2017</p><p>[^2]: A. Hermans, L. Beyer, and B. Leibe.In defense of the triplet loss for person re-identification.arXiv preprint arXiv:1703.07737, 2017.</p><p>[^3]: Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu. Deep mutual learning. arXiv preprint arXiv:1706.00384, 2017.</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tracking </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>读 Re-id 论文</title>
      <link href="/2017/11/16/re-id-mscan/"/>
      <url>/2017/11/16/re-id-mscan/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>读 \<learning deep context-aware features over body and latent parts for person re-identification\></learning></b></font></center><a id="more"></a><h1 id="Learning-Deep-Context-aware-Features-over-Body-and-Latent-Parts-for-Person-Re-identification-CVPR17"><a href="#Learning-Deep-Context-aware-Features-over-Body-and-Latent-Parts-for-Person-Re-identification-CVPR17" class="headerlink" title="Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification (CVPR17)"></a>Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification (CVPR17)</h1><p>提出一个新的网络结构:  Multi-Scale Context- Aware Network (MSCAN)</p><p>使用深度卷积网络(DCNN)学习<a href="http://cn.arxiv.org/abs/1604.02531" target="_blank" rel="noopener">IDE特征(ID-discriminative Embedding)</a>[^1]</p><p>提出了现有的网络的一些问题:</p><ol><li>传统的DCNN把single-scale convolution和max polling堆起来组成一个深层网络.但是层数增长会导致很容易忽视一些细节,例如眼镜,鞋子,帽子等,但是这些特征对识别人很有用.</li><li>姿态变化和行人检测器的不完善,导致图像可能出现错位,或者把某些背景包含进,又或者是缺少某些部分,例如腿的缺失.</li></ol><h2 id="本文创新点"><a href="#本文创新点" class="headerlink" title="本文创新点"></a>本文创新点</h2><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1flhzz00gg1j315c0w8tl5.jpg" alt></p><ol><li>为了解决第一个问题,提出了将<code>行人的部分和整体</code>一起学习.由图可以看出,在MSCAN中不同感受野的多个卷积核获得多个feature map.不同卷积核得到的feature map连接在一起作为当前层的一个输出.为了减少不同卷积核之间的相关性,采用了<code>dilated convolution</code>[^2].通过这种方法,多层次的信息会在同一层获得,这样局部细节会被增强.逐层加入embedding contextual features, MSCAN可以从输入图片获得更多的context-aware representation.(当然我是认为这里是论文吹的一波.)</li><li>为了解决第二个问题,基于<code>Spatial Transform Networks(STN)</code>[^3],加了三个新的约束条件.可以减少背景内容的干扰.</li></ol><p>全局的身体和局部的身体部件通常是有互补的信息的,本文为了更好的利用这一关系,将全身和身体的各个部分的特征级联,最后在测试阶段,计算两个<code>L2-Norm 欧式距离</code>.</p><h2 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h2><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fli1j7m1nij31kw0s0k99.jpg" alt></p><h3 id="Multi-scale-Context-Aware-Network"><a href="#Multi-scale-Context-Aware-Network" class="headerlink" title="Multi-scale Context-Aware Network"></a>Multi-scale Context-Aware Network</h3><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fli1nmkcw8j30k40aa42n.jpg" alt></p><p>使用了三种不同的dilated convolution. 在每个卷积层之后和传统的网络一样,使用BN和ReLU.</p><h3 id="改进STN"><a href="#改进STN" class="headerlink" title="改进STN"></a>改进STN</h3><p>原始的STN是用来学习图像转换的参数的,比如缩放和旋转.</p><p>优点:</p><ol><li>易扩展;</li><li>可以学习翻译,缩放,裁剪等一个没有明确给定的区域.</li></ol><p>STN包括,学习变换参数的空间定位网络和使用图像插值内核对输入图像采样的生成器.</p><p>本文采用<code>双线性插值内核</code>进行采样.</p><hr><p>[^1]: L. Zheng, H. Zhang, S. Sun, M. Chandraker, and Q. Tian. Person re-identification in the wild. arXiv:1604.02531, 2016. (CVPR17)</p><p>[^2]: F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. In Proc. ICLR, 2016</p><p>[^3]: M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. In Proc. NIPS, 2015.</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Metric Learning </category>
          
          <category> re-ID </category>
          
          <category> Read Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Read Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>读 Re-id 论文</title>
      <link href="/2017/11/09/re-id-lomo-xqda/"/>
      <url>/2017/11/09/re-id-lomo-xqda/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>读 \<person re-identification by local maximum occurrence representation and metric learning\></person></b></font></center><a id="more"></a><h1 id="Person-Re-identification-by-Local-Maximum-Occurrence-Representation-and-Metric-Learning-1"><a href="#Person-Re-identification-by-Local-Maximum-Occurrence-Representation-and-Metric-Learning-1" class="headerlink" title="Person Re-identification by Local Maximum Occurrence Representation and Metric Learning[^1]"></a>Person Re-identification by Local Maximum Occurrence Representation and Metric Learning[^1]</h1><p>Local Maximal Occurrence Feature. XQDA(CVPR 2015)</p><hr><h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p><code>multiscale Retinex algorithm</code></p><h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><h3 id="引入SILTP"><a href="#引入SILTP" class="headerlink" title="引入SILTP"></a>引入SILTP</h3><p>SILTP并不是作者创新,只是将之引入到LOMO的特征一部分.</p><p>SITLP是LBP/LTP的一种改进,如下图,SILTP对光照和噪音都不敏感.</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1flc7qy2gdyj30xe088412.jpg" alt></p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1flc7rexzw3j30ik0bgtar.jpg" alt></p><p>其中<code>⊕</code>表示将$S_\tau(I_c,I_k)$得到的二进制数值串联到一起形成一个二进制字符串.</p><p>LBP 有灰度不变的性质，但对噪声不鲁棒</p><p>SILTP 通过引入尺度不变局部比较容差，提高 LBP，实现强度尺度变化的不变性和图像噪声的鲁棒性。</p><h3 id="LOMO"><a href="#LOMO" class="headerlink" title="LOMO"></a>LOMO</h3><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1flc7rpb6nmj315u0q6n0p.jpg" alt></p><p>采用$10\times10$的子窗口,在$128\times48$的图像中,使用步长为5的像素,对每个子窗口提取2种SILTP直方图,加上一个$8\times8\times8$的HSV直方图.</p><h3 id="XQDA"><a href="#XQDA" class="headerlink" title="XQDA"></a>XQDA</h3><p>作者提出了XQDA的一种新的度量学习的方式,原理主要是利用交叉二次判别降低子空间的维度.</p><p>通过计算广义瑞利商，通过广义特征值分解获得封闭式的解。这里得到闭式解的方法和LDA里面的方法一样.周志华那本西瓜书也有详细的推导.</p><p>其主要是基于KISSME[^2], Bayesian face recognition[^3] 改进而来的方法.</p><p>KISSME度量在我之前的<a href="http://heywe.cn/2017/11/03/reid-kissme/">文章</a>中有详细的提到.</p><p>XQDA利用了相似的原理,它定义了两个概率密度函数分别表示同一类的样本,和不同一类的样本.</p><p>$$P(\Delta|\Omega_I)={1 \over (2\pi)^{d/2}|\Sigma_I|^{1/2}}e^{-{1\over 2}\Delta^T \Sigma_I^{-1}\Delta}$$</p><p>$$P(\Delta|\Omega_E)={1 \over (2\pi)^{d/2}|\Sigma_E|^{1/2}}e^{-{1\over 2}\Delta^T \Sigma_E^{-1}\Delta}$$</p><p>然后相除,化简得到</p><p>$$f(\Delta)=\Delta^T(\Sigma_I^{-1}-\Sigma_E^{-1})\Delta$$</p><h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><p>作者在其中一个实验结果中有点问题??得不到论文中那么好的结果??是我们做的工作有问题?</p><hr><p>[^1]: Liao S, Hu Y, Zhu X, et al. Person re-identification by local maximal occurrence representation and metric learning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 2197-2206.</p><p>[^2]: Köstinger M, Hirzer M, Wohlhart P, et al. Large scale metric learning from equivalence constraints[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2012:2288-2295.</p><p>[^3]: Moghaddam B, Jebara T, Pentland A. Bayesian face recognition[J]. Pattern Recognition, 2000, 33(11): 1771-1782.</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Metric Learning </category>
          
          <category> re-ID </category>
          
          <category> Read Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Read Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>re-ID 系列笔记(一)</title>
      <link href="/2017/11/03/reid-kissme/"/>
      <url>/2017/11/03/reid-kissme/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>度量学习--KISSME</b></font></center><a id="more"></a><h2 id="KISSME-keep-it-simple-and-straightforward-Metric-Learning-1"><a href="#KISSME-keep-it-simple-and-straightforward-Metric-Learning-1" class="headerlink" title="KISSME(keep it simple and straightforward Metric Learning)[^1]"></a>KISSME(keep it simple and straightforward Metric Learning)[^1]</h2><p>通过似然比判断两个样本点是否相似.如果$H_0$表示样本对不相似的空间,$H_1$表示样本对相似的空间.则似然比:</p><p>$$\delta(x_i,x_j) = log(\cfrac{p(x_i,x_j|H_0)}{p(x_i,x_j|H_1)})$$</p><p>如果$\delta(x_i,x_j)$较高,则偏向于不相似.反之值较低,则样本对相似.</p><p>为了使样本独立于原始特征空间的位置,原始特征被投影到一个零均值的差分空间,令$x_{ij}=x_i-x_j$.</p><p>则上式可以写为</p><p>$$\delta(x_{ij}) = log(\cfrac{p(x_{ij}|H_0)}{p(x_{ij}|H_1)}) = log(\cfrac{f(x_{ij}|\theta_0)}{f(x_{ij}|\theta_1)})$$</p><p>f表示对应的概率密度函数</p><p>相关行人对和不相关行人对在特征空间中服从均值0,协方差矩阵为$\Sigma_{y_{ij}=1}、\Sigma_{y_{ij}=0}$的多维正态分布:</p><p>$$\delta(x_{ij}) = log \left(\cfrac{1/{\sqrt{2\pi|{\Sigma_{y_{ij}=0}}|}exp(-1/2x_{ij}^T\Sigma_{y_{ij}=0}^{-1}x_{ij})}}{1/{\sqrt{2\pi|{\Sigma_{y_{ij}=1}}|}exp(-1/2x_{ij}^T\Sigma_{y_{ij}=1}^{-1}x_{ij})}}\right)$$</p><p>其中:</p><p>$$\Sigma_{y_{ij}=1}=\sum\limits_{y_{ij}=1}(x_i-x_j)(x_i-x_j)^T$$</p><p>$$\Sigma_{y_{ij}=0}=\sum\limits_{y_{ij}=0}(x_i-x_j)(x_i-x_j)^T$$</p><p>把log去掉,相似度函数中的常量不影响行人相似度的度量,故省略$log(.)$项</p><p>$$\delta({x_{ij}})=x_{ij}^T\Sigma_{y_{ij}=1}^{-1}x_{ij}+log(|\Sigma_{y_{ij}=1}|) - x_{ij}^T\Sigma_{y_{ij}=0}^{-1}x_{ij}-log(|\Sigma_{y_{ij}=0}|)$$</p><p>化简为:</p><p>$$\delta(x_{ij}) = x_{ij}^T(\Sigma_{y{ij}=1}^{-1}-\Sigma_{y{ij}=0}^{-1})x_{ij}$$</p><p>得到反映对数似然比性质的马氏矩阵.</p><p>$$d_M^2(x_i, x_j)=(x_i-x_j)^TM(x_i-x_j)$$</p><p>令$\hat{M}=\Sigma_{y{ij}=1}^{-1}-\Sigma_{y{ij}=0}^{-1}$</p><p>$\hat{M}$即为所求的测度矩阵,把得到的$\hat{M}$再次投影得到半正定矩阵.</p><h3 id="一种KISSME的改进方法-引入核学习的方法-3"><a href="#一种KISSME的改进方法-引入核学习的方法-3" class="headerlink" title="一种KISSME的改进方法,引入核学习的方法[^3]"></a>一种KISSME的改进方法,引入核学习的方法[^3]</h3><p>核学习的主要思想是将原始线性特征空间投影到区分性好的非线性空间. 原始特征空间中的特征xi通过函数 Φ 投影到非线性空间, 则非线性空间的特征表示为 Φ(xi). 非线性映射函数一般是隐性函数, 则很难得到显式表达式, 可以利用核函数求解特征空间中样本点的内积来解决.</p><p>(未完待续)</p><hr><p>[^1]:Köstinger M, Hirzer M, Wohlhart P, et al. Large scale metric learning from equivalence constraints[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2012:2288-2295.</p><p>[^2]:齐美彬, 胡龙飞, 蒋建国,等. 多特征融合与独立测度学习的行人再识别[J]. 中国图象图形学报, 2016, 21(11):1464-1472.</p><p>[^3]:齐美彬, 檀胜顺, 王运侠,等. 基于多特征子空间与核学习的行人再识别[J]. 自动化学报, 2016, 42(2):299-308.</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Metric Learning </category>
          
          <category> re-ID </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> re-ID </tag>
            
            <tag> Metric Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>re-ID 系列笔记(零)</title>
      <link href="/2017/10/08/reid-distance/"/>
      <url>/2017/10/08/reid-distance/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>度量学习--常用距离</b></font></center><a id="more"></a><h1 id="Metric-Learning"><a href="#Metric-Learning" class="headerlink" title="Metric Learning"></a>Metric Learning</h1><hr><p>ps:由于mathjax的缺陷,可能导致矢量箭头渲染有问题..就将就着看吧..</p><hr><p>度量学习的概念由Eric Xing 在 NIPS 2002 提出.</p><p>度量学习通常的目标是使同类样本之间的距离尽可能缩小,不同类样本之间的距离尽可能放大.</p><p>度量学习的第一个要接触的概念就是距离.</p><h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><h3 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h3><h4 id="曼哈顿距离-Manhattan-distance"><a href="#曼哈顿距离-Manhattan-distance" class="headerlink" title="曼哈顿距离 Manhattan distance"></a>曼哈顿距离 Manhattan distance</h4><p>$d = |x_1 - x_2| + |y_1 - y_2|$</p><center>![](https://ws1.sinaimg.cn/large/006tNc79gy1fkb7wyw050j307v07vmxo.jpg)<p>图来自维基百科</p></center><p></p><p>红蓝黄皆为曼哈顿距离，绿色为欧式距离。</p><h4 id="欧几里得距离-Euclidean-Metric"><a href="#欧几里得距离-Euclidean-Metric" class="headerlink" title="欧几里得距离 Euclidean Metric"></a>欧几里得距离 Euclidean Metric</h4><ul><li><p>在欧几里得空间中，点$x =(x_1,…,x_n)$和 $y =(y_1,…,y_n)$之间的欧氏距离为</p><p>  $d(x,y) := \sqrt {\sum (x_{i}-y_{i})^2},i=1,2,…$</p></li><li><p>向量 $\vec{x}$ 的自然长度，即该点到原点的距离为</p><p>  $||\vec{x}||_2 = \sqrt{|x_1|^2+\cdots+|x_n|^2}$</p><p>  在欧几里得度量下，两点之间线段最短。</p></li><li><p>缺点</p><p>  它将样品的不同属性（即各指标或各变量）之间的差别等同看待，这一点有时不能满足实际要求。</p></li></ul><h4 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h4><p>马氏距离表示数据的协方差距离。与欧式距离不同的是它考虑到各种特性之间的联系,例如身高体重之间是有一定关联的,并且是尺度无关的(scale-invariant),即独立与测量尺度.对于一个均值为$\mu = (\mu_1,\mu_2,\cdots,\mu_p)^T$,协方差矩阵为$\sum$的多变量向量$x=(x_1,x_2,\cdots,x_p)^T$,其马氏距离为</p><p>$D_M(x)=\sqrt {(x-\mu)^T\sum^{-1}(x-\mu)}$</p><p>马氏距离也可以定义为两个服从统一发布的并且协方差矩阵为$\sum$的随机变量$\vec{x}$与$\vec{y}$的差异程度</p><p>$d(\vec{x},\vec{y})=\sqrt{(\vec{x}-\vec{y})^T \sum^{-1}(\vec{x}-\vec{y})}$</p><h4 id="切比雪夫距离-Chebyshev-distance"><a href="#切比雪夫距离-Chebyshev-distance" class="headerlink" title="切比雪夫距离 Chebyshev distance"></a>切比雪夫距离 Chebyshev distance</h4><blockquote><p>数学上，切比雪夫距离（Chebyshev distance）或是$L_∞$度量是向量空间中的一种度量，二个点之间的距离定义为其各座标数值差的最大值。以(x1,y1)和(x2,y2)二点为例，其切比雪夫距离为$max(|x_2-x_1|,|y_2-y_1|)$。切比雪夫距离得名自俄罗斯数学家切比雪夫。<br>若将国际象棋棋盘放在二维直角座标系中，格子的边长定义为1，座标的x轴及y轴和棋盘方格平行，原点恰落在某一格的中心点，则王从一个位置走到其他位置需要的步数恰为二个位置的切比雪夫距离，因此切比雪夫距离也称为棋盘距离。例如位置F6和位置E2的切比雪夫距离为4。任何一个不在棋盘边缘的位置，和周围八个位置的切比雪夫距离都是1。</p></blockquote><br><blockquote><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fkb7wur1qqj306t0a1404.jpg" alt></p></blockquote><blockquote><p>图文摘自维基百科</p></blockquote><h4 id="明可夫斯基距离-明氏距离-Minkowski-distance"><a href="#明可夫斯基距离-明氏距离-Minkowski-distance" class="headerlink" title="明可夫斯基距离(明氏距离) Minkowski distance"></a>明可夫斯基距离(明氏距离) Minkowski distance</h4><blockquote><p>明氏距离又叫做明可夫斯基距离，是欧氏空间中的一种测度，被看做是欧氏距离和曼哈顿距离的一种推广。</p></blockquote><ul><li><p>定义:</p><p>  两点$P=(x_1,x_2,\cdots,x_n)$ , $Q=(y_1,y_2,\cdots,y_n)\in \mathbb{R}^n$</p><p>  之间的明氏距离是:</p><p>  $$(\sum_{i=1}^n{|x_i-y_i|^p})^{1/p}$$</p><p>  p取1或2时的明氏距离是最为常用的，p=2即为欧氏距离，而p=1时则为曼哈顿距离。当p取无穷时的极限情况下，可以得到切比雪夫距离：</p><p>  $$\lim\limits_{p \rightarrow \infty}(\sum_{i=1}^n|x_i-y_i|^p)^{1/p}=\max_{i=1}^n|x_i-y_i|$$</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Metric Learning </category>
          
          <category> re-ID </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tracking </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python常见的高级特性</title>
      <link href="/2017/07/29/python-high-level/"/>
      <url>/2017/07/29/python-high-level/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>Python高级特性(迭代器&生成器等)</b></font></center><a id="more"></a><h1 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h1><p>正负索引就不提及了，都是比较基本的用法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#正数索引切片</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = list(range(<span class="number">11</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">2</span>:<span class="number">8</span>]</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#负数索引切片</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">-4</span>:<span class="number">-2</span>]</span><br><span class="line">[<span class="number">7</span>, <span class="number">8</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#指定步长切片</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[::<span class="number">2</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[::<span class="number">-2</span>]</span><br><span class="line">[<span class="number">10</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#列表赋值切片</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b[<span class="number">2</span>:<span class="number">3</span>] = [<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b[<span class="number">1</span>:<span class="number">1</span>] = [<span class="number">8</span>,<span class="number">9</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">[<span class="number">1</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b[<span class="number">1</span>:<span class="number">-1</span>] = []</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">[<span class="number">1</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#用slice()函数切片</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = list(range(<span class="number">6</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sa = slice(<span class="number">-3</span>,<span class="literal">None</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sa</span><br><span class="line">slice(<span class="number">-3</span>, <span class="literal">None</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[sa]</span><br><span class="line">[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br></pre></td></tr></table></figure><hr><h1 id="迭代"><a href="#迭代" class="headerlink" title="迭代"></a>迭代</h1><h2 id="基本迭代"><a href="#基本迭代" class="headerlink" title="基本迭代"></a>基本迭代</h2><p>给定 <strong>list</strong> 或 <strong>tuple</strong> 可以通过 <strong>for</strong> 循环来遍历，这种遍历称为<strong>迭代</strong>（iteration）</p><p>许多语言中迭代是通过下标完成的，比如 java：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;list.length; i++) &#123;</span><br><span class="line">    n = list[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>而 python 的 for 循环不仅仅可以用在 list 或是 tuple 上，还可以用在其他的可迭代对象上，比如 dict。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">3</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> key <span class="keyword">in</span> d:</span><br><span class="line"><span class="meta">... </span>    print(key)</span><br><span class="line">...</span><br><span class="line">a</span><br><span class="line">c</span><br><span class="line">b</span><br></pre></td></tr></table></figure><blockquote><p>要注意的是，dict 的存储顺序不是按照 list 顺序排列，所以迭代出的顺序很可能不一样</p></blockquote><blockquote><p>默认情况下，<strong>dict</strong> 迭代的是 <strong>key</strong>。如果要迭代 <strong>value</strong>，可以用 <strong>for value in d.values()</strong>，如果要同时迭代 <strong>key</strong> 和 <strong>value</strong> ，可以用 <strong>for k, v in d.items()</strong> 。</p></blockquote><p>字符串也可迭代，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> ch <span class="keyword">in</span> <span class="string">'ABC'</span>:</span><br><span class="line"><span class="meta">... </span>    print(ch)</span><br><span class="line">...</span><br><span class="line">A</span><br><span class="line">B</span><br><span class="line">C</span><br></pre></td></tr></table></figure><p>判断是否是可迭代对象：</p><p>通过 <strong>collections</strong> 模块的 <strong>Iterable 类型</strong> 判断</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> Iterable</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance(<span class="string">'abc'</span>, Iterable) <span class="comment"># str是否可迭代</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], Iterable) <span class="comment"># list是否可迭代</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance(<span class="number">123</span>, Iterable) <span class="comment"># 整数是否可迭代</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure><p>如果在 python 中想实现上面 java 一样的通过下表循环的需要使用 python 内置的 <strong>enumerate</strong> 函数，使得 list 变成 <strong>索引-元素</strong> 对，这样就可以在 for 循环中同时迭代索引和元素本身：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i , value <span class="keyword">in</span> enumerate([<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>]):</span><br><span class="line"><span class="meta">... </span>    print(i,value)</span><br><span class="line">...</span><br><span class="line"><span class="number">0</span> A</span><br><span class="line"><span class="number">1</span> B</span><br><span class="line"><span class="number">2</span> C</span><br></pre></td></tr></table></figure><h2 id="Iterator"><a href="#Iterator" class="headerlink" title="Iterator"></a>Iterator</h2><p>可以直接作用于for循环的对象统称为可迭代对象：<strong>Iterable</strong> .<br>可以被 <strong>next()函数</strong> 调用并不断返回下一个值的对象称为迭代器： <strong>Iterator</strong> 。</p><p>生成器都是 <strong>Iterator</strong> 对象，但 <strong>list</strong>、<strong>dict</strong>、<strong>str</strong> 虽然是 <strong>Iterable</strong> ，却不是 <strong>Iterator</strong> 。</p><blockquote><p>在官方文档中的原文：</p></blockquote><blockquote><p>iterable</p></blockquote><pre> An object capable of returning its members one at a time. Examples of iterables include all sequence types (such as list, str, and tuple) and some non-sequence types like dict, file objects, and objects of any classes you define with an __iter__() or __getitem__() method. Iterables can be used in a for loop and in many other places where a sequence is needed (zip(), map(), ...). When an iterable object is passed as an argument to the built-in function iter(), it returns an iterator for the object. This iterator is good for one pass over the set of values. When using iterables, it is usually not necessary to call iter() or deal with iterator objects yourself. The for statement does that automatically for you, creating a temporary unnamed variable to hold the iterator for the duration of the loop. See also iterator, sequence, and generator. </pre><blockquote><p>iterator</p></blockquote><pre> An object representing a stream of data. Repeated calls to the iterator’s __next__() method (or passing it to the built-in function next()) return successive items in the stream. When no more data are available a StopIteration exception is raised instead. At this point, the iterator object is exhausted and any further calls to its __next__() method just raise StopIteration again. Iterators are required to have an __iter__() method that returns the iterator object itself so every iterator is also iterable and may be used in most places where other iterables are accepted. One notable exception is code which attempts multiple iteration passes. A container object (such as a list) produces a fresh new iterator each time you pass it to the iter() function or use it in a for loop. Attempting this with an iterator will just return the same exhausted iterator object used in the previous iteration pass, making it appear like an empty container. </pre><p>把list、dict、str等 Iterable 变成 Iterator 可以使用iter()函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这是在 python2.7 中的 iter 对象</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = (<span class="number">123</span>,<span class="string">'abc'</span>,<span class="number">3.14</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i = iter(t)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i</span><br><span class="line">&lt;tupleiterator object at <span class="number">0x10b2d2210</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i.next()</span><br><span class="line"><span class="number">123</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i.next()</span><br><span class="line"><span class="string">'abc'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i.next()</span><br><span class="line"><span class="number">3.14</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i.next()</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">StopIteration</span><br><span class="line"></span><br><span class="line"><span class="comment">#当你或是一个循环机制(例如 for 语句)需要下一个项时，调用迭代器的 next() 方法就可以获得它。条目全部取出后，会引发一个 StopIteration 异常，这并不表示错误发生，只是告诉外部调用者，迭代完成.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#在 python3 会报错 原因是没有 iter 对象没有这个方法，可以用 next() 函数实现相同作用</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = (<span class="number">123</span>,<span class="string">'abc'</span>,<span class="number">3.14</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i = iter(t)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i</span><br><span class="line">&lt;tuple_iterator object at <span class="number">0x1022288d0</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i.next()</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">AttributeError: <span class="string">'tuple_iterator'</span> object has no attribute <span class="string">'next'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(i)</span><br><span class="line"><span class="number">123</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(i)</span><br><span class="line"><span class="string">'abc'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(i)</span><br><span class="line"><span class="number">3.14</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(i)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">StopIteration</span><br><span class="line"></span><br><span class="line"><span class="comment">#看一下这个是不是可迭代对象</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> Iterable</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance(i,Iterable)</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><p>在 python 中的 Iterator 对象表示的是一个数据流，它可以被 next() 函数调用并且不断返回下一个数据，直到没有数据时，抛出 <strong>StopIteration</strong> 异常。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过 next()  函数实现按需计算下一个数据，所以 Iterator 的计算是惰性的，只有在需要返回下一个数据时它才会计算。</p><p>Iterator 甚至可以表示一个无限大的数据流，例如全体自然数。而使用 list 是永远不可能存储全体自然数的。</p><p>python 的 for 循环的本质上就是不断调用 next() 实现的 例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###等价于###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先获得Iterator对象:</span></span><br><span class="line">it = iter([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="comment"># 循环:</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 获得下一个值:</span></span><br><span class="line">        x = next(it)</span><br><span class="line">    <span class="keyword">except</span> StopIteration:</span><br><span class="line">        <span class="comment"># 遇到StopIteration就退出循环</span></span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h2 id="迭代器的解压缩"><a href="#迭代器的解压缩" class="headerlink" title="迭代器的解压缩"></a>迭代器的解压缩</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#列表的解压缩</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = [<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = zip(a,b)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z</span><br><span class="line">&lt;zip object at <span class="number">0x102229b48</span>&gt;</span><br><span class="line"><span class="comment">#如果是 python2 那么可以直接输出列表形式的 z :[(1, 'a'), (2, 'b'), (3, 'c')]</span></span><br><span class="line"><span class="comment">#若果是 python3 则显示如上</span></span><br><span class="line"><span class="comment">#python3中 zip() 是可迭代对象，使用时必须包含在一个 list 的中，可以显示所有结果。</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(z)</span><br><span class="line">[(<span class="number">1</span>, <span class="string">'a'</span>), (<span class="number">2</span>, <span class="string">'b'</span>), (<span class="number">3</span>, <span class="string">'c'</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance(z,Iterable)</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><ul><li>列表相邻元素的压缩:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = list(range(<span class="number">1</span>,<span class="number">7</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>zip(*([iter(a)]*<span class="number">2</span>))</span><br><span class="line">&lt;zip object at <span class="number">0x102229dc8</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = zip(*([iter(a)]*<span class="number">2</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(z)</span><br><span class="line">[(<span class="number">1</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">4</span>), (<span class="number">5</span>, <span class="number">6</span>)]</span><br><span class="line"><span class="comment">#这就产生了一个很实用的分组函数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>group_adjacent = <span class="keyword">lambda</span> a,k:zip(*([iter(a)]*k))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = list(range(<span class="number">9</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>gz = group_adjacent(z,<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(gz)</span><br><span class="line">[(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>), (<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>gz = group_adjacent(z,<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(gz)</span><br><span class="line">[(<span class="number">0</span>,), (<span class="number">1</span>,), (<span class="number">2</span>,), (<span class="number">3</span>,), (<span class="number">4</span>,), (<span class="number">5</span>,), (<span class="number">6</span>,), (<span class="number">7</span>,), (<span class="number">8</span>,)]</span><br></pre></td></tr></table></figure><p><a href="https://stackoverflow.com/questions/2233204/how-does-zipitersn-work-in-python" target="_blank" rel="noopener">可以参考 stack overflow 理解</a></p><ul><li>滑动取值窗口</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = list(range(<span class="number">1</span>,<span class="number">7</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>n_grams = <span class="keyword">lambda</span> a,n:zip(*([iter(a[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(n_grams(a,<span class="number">3</span>))</span><br><span class="line">[(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), (<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), (<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>), (<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)]</span><br><span class="line"><span class="comment">#这里的这个就很像 nlp 中的滑窗</span></span><br></pre></td></tr></table></figure><ul><li>展开列表</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#方法1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> itertools</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(itertools.chain.from_iterable(a))</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"><span class="comment">#itertools模块提供的全部是处理迭代功能的函数，它们的返回值不是list，而是迭代对象，只有用for循环迭代的时候才真正计算。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#方法2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sum(a,[])</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#方法3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[x <span class="keyword">for</span> l <span class="keyword">in</span> a <span class="keyword">for</span> x <span class="keyword">in</span> l]</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], [[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[x <span class="keyword">for</span> l1 <span class="keyword">in</span> a <span class="keyword">for</span> l2 <span class="keyword">in</span> l1 <span class="keyword">for</span> x <span class="keyword">in</span> l2]</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>, <span class="number">2</span>, [<span class="number">3</span>, <span class="number">4</span>], [[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>flatten = <span class="keyword">lambda</span> x: [y <span class="keyword">for</span> l <span class="keyword">in</span> x <span class="keyword">for</span> y <span class="keyword">in</span> flatten(l)] <span class="keyword">if</span> type(x) <span class="keyword">is</span> list <span class="keyword">else</span> [x]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>flatten(a)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]</span><br></pre></td></tr></table></figure><p><a href="https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001415616001996f6b32d80b6454caca3d33c965a07611f000" target="_blank" rel="noopener">可参考廖雪峰</a></p><h1 id="生成式"><a href="#生成式" class="headerlink" title="生成式"></a>生成式</h1><ul><li>列表生成式</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(range(<span class="number">1</span>, <span class="number">11</span>))</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</span><br><span class="line"><span class="comment">#但如果要生成[1x1, 2x2, 3x3, ..., 10x10]</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>L = []</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line"><span class="meta">... </span>   L.append(x * x)</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>L</span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>, <span class="number">36</span>, <span class="number">49</span>, <span class="number">64</span>, <span class="number">81</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#可以用一行语句代上面的式子</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[x * x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>)]</span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>, <span class="number">36</span>, <span class="number">49</span>, <span class="number">64</span>, <span class="number">81</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#可以加入判断</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>L1 = [<span class="string">'Hello'</span>, <span class="string">'World'</span>, <span class="number">18</span>, <span class="string">'Apple'</span>, <span class="literal">None</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>L2 = [s.lower() <span class="keyword">for</span> s <span class="keyword">in</span> L1 <span class="keyword">if</span> isinstance(s,str)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>L2</span><br><span class="line">[<span class="string">'hello'</span>, <span class="string">'world'</span>, <span class="string">'apple'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#之前的 flatten() 函数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>flatten = <span class="keyword">lambda</span> x: [y <span class="keyword">for</span> l <span class="keyword">in</span> x <span class="keyword">for</span> y <span class="keyword">in</span> flatten(l)] <span class="keyword">if</span> type(x) <span class="keyword">is</span> list <span class="keyword">else</span> [x]</span><br></pre></td></tr></table></figure><ul><li>其他</li></ul><p>生成式也可用于字典中，也被称为字典推导：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>n = &#123;x : x ** <span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">5</span>)&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>n</span><br><span class="line">&#123;<span class="number">0</span>: <span class="number">0</span>, <span class="number">1</span>: <span class="number">1</span>, <span class="number">2</span>: <span class="number">4</span>, <span class="number">3</span>: <span class="number">9</span>, <span class="number">4</span>: <span class="number">16</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = &#123;x: <span class="string">'A'</span> + str(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">5</span>)&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m</span><br><span class="line">&#123;<span class="number">0</span>: <span class="string">'A0'</span>, <span class="number">1</span>: <span class="string">'A1'</span>, <span class="number">2</span>: <span class="string">'A2'</span>, <span class="number">3</span>: <span class="string">'A3'</span>, <span class="number">4</span>: <span class="string">'A4'</span>&#125;</span><br></pre></td></tr></table></figure><p>字典推导反转字典:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = &#123;<span class="string">'a'</span>:<span class="number">1</span>,<span class="string">'b'</span>:<span class="number">2</span>,<span class="string">'c'</span>:<span class="number">3</span>,<span class="string">'d'</span>:<span class="number">4</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m</span><br><span class="line">&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'d'</span>: <span class="number">4</span>, <span class="string">'c'</span>: <span class="number">3</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;v : k <span class="keyword">for</span> k,v <span class="keyword">in</span> m.items()&#125;</span><br><span class="line">&#123;<span class="number">1</span>: <span class="string">'a'</span>, <span class="number">2</span>: <span class="string">'b'</span>, <span class="number">3</span>: <span class="string">'c'</span>, <span class="number">4</span>: <span class="string">'d'</span>&#125;</span><br></pre></td></tr></table></figure><h1 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h1><p>如果把列表的生成式的 <strong>[]</strong> 变为 <strong>()</strong> 就得到了一个 generator</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>L = [x * x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>L</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>, <span class="number">36</span>, <span class="number">49</span>, <span class="number">64</span>, <span class="number">81</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>g = (x * x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>g</span><br><span class="line">&lt;generator object &lt;genexpr&gt; at <span class="number">0x102a0dba0</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">#g的值可以通过 next() 函数打印出来</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(g)</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(g)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(g)</span><br><span class="line"><span class="number">4</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(g)</span><br><span class="line"><span class="number">81</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(g)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">StopIteration</span><br><span class="line"><span class="comment">#和之前提到的 Iterator 类似，也可以通过 for 循环迭代</span></span><br></pre></td></tr></table></figure><p>使用生成器很容易写出前 n 个斐波那契数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 函数定义</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">(max)</span>:</span></span><br><span class="line"><span class="meta">... </span>    n, a, b = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">while</span> n &lt; max:</span><br><span class="line"><span class="meta">... </span>        print(b)</span><br><span class="line"><span class="meta">... </span>        a, b = b, a + b</span><br><span class="line"><span class="meta">... </span>        n = n + <span class="number">1</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> <span class="string">'done'</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib(<span class="number">5</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="string">'done'</span></span><br><span class="line"><span class="comment"># 定义 generator</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">(max)</span>:</span></span><br><span class="line"><span class="meta">... </span>    n, a, b = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">while</span> n &lt; max:</span><br><span class="line"><span class="meta">... </span>        <span class="keyword">yield</span> b</span><br><span class="line"><span class="meta">... </span>        a, b = b, a + b</span><br><span class="line"><span class="meta">... </span>        n = n + <span class="number">1</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> <span class="string">'done'</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = fib(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f</span><br><span class="line">&lt;generator object fib at <span class="number">0x102a0db48</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> n <span class="keyword">in</span> f:</span><br><span class="line"><span class="meta">... </span>    print(n)</span><br><span class="line">...</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure><p>但是用for循环调用generator时，发现拿不到generator的return语句的返回值。如果想要拿到返回值，必须捕获StopIteration错误，返回值包含在StopIteration的value中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>g = fib(<span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">try</span>:</span><br><span class="line"><span class="meta">... </span>        x = next(g)</span><br><span class="line"><span class="meta">... </span>        print(<span class="string">'g:'</span>, x)</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">except</span> StopIteration <span class="keyword">as</span> e:</span><br><span class="line"><span class="meta">... </span>        print(<span class="string">'Generator return value:'</span>, e.value)</span><br><span class="line"><span class="meta">... </span>        <span class="keyword">break</span></span><br><span class="line">...</span><br><span class="line">g: <span class="number">1</span></span><br><span class="line">g: <span class="number">1</span></span><br><span class="line">g: <span class="number">2</span></span><br><span class="line">g: <span class="number">3</span></span><br><span class="line">g: <span class="number">5</span></span><br><span class="line">g: <span class="number">8</span></span><br><span class="line">Generator <span class="keyword">return</span> value: done</span><br></pre></td></tr></table></figure><p>参考：</p><p><a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/0014317568446245b3e1c8837414168bcd2d485e553779e000" target="_blank" rel="noopener">廖雪峰python高级特性</a></p><p><a href="http://www.phperz.com/article/14/0805/14985.html" target="_blank" rel="noopener">Python编程小技巧35则</a></p><p>以及官方文档、 Stack Overflow、百度、 Google 等。。。# 切片<br>正负索引就不提及了，都是比较基本的用法。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>macOS Sierra源码编译 opencv3.2</title>
      <link href="/2017/07/10/opencv-install/"/>
      <url>/2017/07/10/opencv-install/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>macOS Sierra 安装 opencv3</b></font></center><a id="more"></a><br><h1 id="macOS-Sierra源码编译-opencv3-2"><a href="#macOS-Sierra源码编译-opencv3-2" class="headerlink" title="macOS Sierra源码编译 opencv3.2"></a>macOS Sierra源码编译 opencv3.2</h1><hr><p>ps：之所以要编译安装原因是用了很多种网上的方法最终发现不行，或者是有一次行，下一次就不行了。。</p><hr><h2 id="网上流传的简单的安装-python-版本的-opencv-的方法"><a href="#网上流传的简单的安装-python-版本的-opencv-的方法" class="headerlink" title="网上流传的简单的安装 python 版本的 opencv 的方法"></a>网上流传的简单的安装 python 版本的 opencv 的方法</h2><h3 id="使用-conda-安装"><a href="#使用-conda-安装" class="headerlink" title="使用 conda 安装"></a>使用 conda 安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用 anaconda 安装 opencv</span></span><br><span class="line">anaconda search -t conda opencv</span><br><span class="line"><span class="comment"># 假设有如下的包 ver228/opencv3            |    3.2.0 | conda           | linux-64, osx-64, win-64</span></span><br><span class="line"><span class="comment"># 然后直接下载</span></span><br><span class="line">conda install -c ver228 opencv</span><br></pre></td></tr></table></figure><p>这种方法的问题，就是我之前提到的，有时候有这个库，但有时候又没有这个库。。而且兼容性一般都存在着问题，有时候会和 numpy 库冲突。。</p><h3 id="使用-brew-安装"><a href="#使用-brew-安装" class="headerlink" title="使用 brew 安装"></a>使用 brew 安装</h3><p>直接用 brew 装。。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brew tap homebrew/science</span><br><span class="line">brew install opencv</span><br></pre></td></tr></table></figure><p>但是使用 brew 装这个会有很多 bug 。。我也不推荐这样安装。。<br>可以参考 <a href="https://segmentfault.com/a/1190000003742411" target="_blank" rel="noopener">这个网址</a></p><h2 id="源码编译安装"><a href="#源码编译安装" class="headerlink" title="源码编译安装"></a>源码编译安装</h2><p>这个是我这里要提到的重头戏。。<br>是我觉得最稳、最不容易翻车的方法。<br>这里还是得先用 brew 安装一些依赖库</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>先安装 python、pip 这些我就不说了。。 <br><br>然后安装 Xcode 还有 Xcode Command Line Tools。。这里我也不再赘述，具体怎么安装 Google 一下。<br>然后就是用 pip 安装一下 numpy 库，这个是必须的。scipy matplotlib 可选，这三个一般放一起，是做科学计算的 python 库。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --ignore-installed numpy scipy matplotlib</span><br></pre></td></tr></table></figure><p>另外在啰嗦一下，我推荐在 mac 上使用 anaconda 来进行分离 python 版本，和安装一些库，比较方便。</p><p>具体可以参考我的另外一篇<a href="http://heywe.cn/2017/05/26/mlenv/#下载-anaconda">博文</a>的 <code>下载anaconda</code> 部分。</p><h3 id="安装依赖库"><a href="#安装依赖库" class="headerlink" title="安装依赖库"></a>安装依赖库</h3><p>后面的依赖库使用 <a href="https://brew.sh" target="_blank" rel="noopener">brew</a> 下载，如果没有，可以下载一下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/ruby -e <span class="string">"<span class="variable">$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)</span>"</span></span><br></pre></td></tr></table></figure><p>然后安装一些依赖包和图像处理，这会用掉比较长的时间。建议备好梯子。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install cmake pkg-config ffmpeg jpeg libpng libtiff eigen tbb gcc git openni libgphoto2 jasper webp</span><br></pre></td></tr></table></figure><p>其中 cmake 是我们等下要对源码进行编译的包，其余的是图像和视频处理相关的包。</p><p>另外在 mac 上面我还推荐使用 Cmake.app 替代手工敲 makefile。。。</p><p><a href="https://cmake.org/download/" target="_blank" rel="noopener">https://cmake.org/download/</a></p><h3 id="正式开始"><a href="#正式开始" class="headerlink" title="正式开始"></a>正式开始</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~</span><br><span class="line"><span class="comment"># 源码</span></span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/opencv/opencv.git</span><br><span class="line"><span class="comment">#  opencv 3.0之后的添加的额外库</span></span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/opencv/opencv_contrib.git</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> ~/opencv </span><br><span class="line">$ mkdir build</span><br></pre></td></tr></table></figure><p>如果使用的是 cmake.app 的话打开得到类似如下界面（没进行 configure 时会是红色，刚打开应该是什么都没有的），把源码和刚刚建立的build目录添加进来。<br><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fhf5nr7jo2j31120y443w.jpg" alt></p><p>点击 configure出现下面这个界面<br><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fhf5r2wsg2j314m0pgwgx.jpg" alt></p><p>点击 done 等待configure 完成之后进行调整。<br>找到 OPENCV_EXTRA_MODULES_PATH 添加自己的额外库的地址。<br>例如  /Users/username/opencv_contrib/modules/<br><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fhf5ui7mqhj30za07otay.jpg" alt></p><p>然后配置里面的 python 环境，默认的也行。<br>我使用的是 anaconda 的虚拟环境的位置。</p><p>类似于这样子，具体根据<code>自己的情况</code>调整：<br><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fhf7a1wx7sj319e0ggn3q.jpg" alt></p><p>我这里的路径和最终配置好的也不完全一样,仅供参考.</p><p>然后就可以 generate 了。</p><p>这里面值得提的一点的是其中有些第三方库实在是下载速度。例如 ippicv。。</p><p>推荐直接去<a href="https://github.com/opencv/opencv_3rdparty/branches" target="_blank" rel="noopener">opencv_3rdparty</a>里面找你需要的文件，然后覆盖掉.cache里面的正在下载的文件，重新进行configure and generate。</p><p>然后就可以编译了，大概几十分钟吧。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~/opencv/build </span><br><span class="line">$ make -j8</span><br><span class="line">$ sudo make install</span><br></pre></td></tr></table></figure><p>如果没有安装 cmake.app 的话也可以在终端中进行编译：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~/opencv/build</span><br><span class="line">$ cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span> -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/modules ..</span><br><span class="line">$ make -j8</span><br><span class="line">$ sudo make install</span><br></pre></td></tr></table></figure><h3 id="收尾工作"><a href="#收尾工作" class="headerlink" title="收尾工作"></a>收尾工作</h3><p>如果上面之后能够正常工作,后面的就可以忽略了.</p><p>macOS Sierra 加入了被称作 <a href="https://support.apple.com/en-us/HT204899" target="_blank" rel="noopener">SIP</a> (System Integrity Protection)的东西，简单点来说就是系统完整性保护。。这就导致了编译之后 python 调用时候路径产生了问题。</p><p>我是选择了关闭 SIP。。具体做法就是重启按住 command+R ， 再在上面实用工具找到终端</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">csrutil <span class="built_in">disable</span></span><br><span class="line">reboot</span><br></pre></td></tr></table></figure><p>然后使用如下的脚本对相对路径进行处理，脚本来源是<a href="https://stackoverflow.com/questions/33260490/python-opencv-error-import-cv2-importerror-dlopen-after-update-of-os-x-ei-capta" target="_blank" rel="noopener"> stackflow 的一个回答</a>：</p><p>这里面需要根据自己的路径进行微调</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"></span><br><span class="line">ABSPATH = <span class="string">"/usr/local/lib/"</span>  <span class="comment"># absolute path to relative libraries</span></span><br><span class="line"><span class="comment"># libraries to correct</span></span><br><span class="line">LIBPATHS = [<span class="string">'/usr/local/lib/python2.7/site-packages/cv2.so'</span>, <span class="string">'/usr/local/lib/libopencv*'</span>] </span><br><span class="line">PREFIX = <span class="string">'sudo install_name_tool -change '</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(ABSPATH.startswith(<span class="string">'/'</span>) <span class="keyword">and</span> ABSPATH.endswith(<span class="string">'/'</span>), </span><br><span class="line">    <span class="string">'please provide absolute library path ending with /'</span>)</span><br><span class="line"></span><br><span class="line">libs = []</span><br><span class="line"><span class="keyword">for</span> path <span class="keyword">in</span> LIBPATHS:</span><br><span class="line">  libs += glob.glob(path)</span><br><span class="line"></span><br><span class="line">cmd =  []</span><br><span class="line">err = []</span><br><span class="line"><span class="keyword">for</span> lib <span class="keyword">in</span> libs:</span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(lib):</span><br><span class="line">    err.append(lib+<span class="string">" library not found"</span>) <span class="comment"># glob should take care</span></span><br><span class="line">  datastr = subprocess.check_output([<span class="string">'otool'</span>,<span class="string">'-l'</span>,<span class="string">'-v'</span>, lib])</span><br><span class="line">  data = datastr.split(<span class="string">'\n'</span>) </span><br><span class="line">  <span class="keyword">for</span> line <span class="keyword">in</span> data:</span><br><span class="line">    ll = line.split()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> ll: <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">if</span> (ll[<span class="number">0</span>] == <span class="string">'name'</span> <span class="keyword">and</span> ll[<span class="number">1</span>].endswith(<span class="string">'.dylib'</span>) <span class="keyword">and</span> <span class="keyword">not</span> ll[<span class="number">1</span>].startswith(<span class="string">'/'</span>)):</span><br><span class="line">      libname = ll[<span class="number">1</span>].split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">      <span class="keyword">if</span> os.path.isfile(ABSPATH+libname):  </span><br><span class="line">        cmd.append(PREFIX+ll[<span class="number">1</span>]+<span class="string">" "</span>+ABSPATH+libname+<span class="string">' '</span>+lib)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        err.append(ABSPATH+libname+<span class="string">" does not exist, hence can't correct: "</span>+ll[<span class="number">1</span>]+<span class="string">" in: "</span>+lib)</span><br><span class="line"></span><br><span class="line">ohandle = open(<span class="string">"rpathChangeCmd.txt"</span>, <span class="string">'w'</span>)</span><br><span class="line"><span class="keyword">for</span> lib <span class="keyword">in</span> cmd:</span><br><span class="line">  ohandle.write(lib+<span class="string">'\n'</span>)</span><br><span class="line">ohandle.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err:</span><br><span class="line">  ehandle = open(<span class="string">"rpathChangeErr.txt"</span>, <span class="string">'w'</span>)</span><br><span class="line">  <span class="keyword">for</span> e <span class="keyword">in</span> err:</span><br><span class="line">    ehandle.write(e+<span class="string">'\n'</span>)</span><br><span class="line">  ehandle.close()</span><br></pre></td></tr></table></figure><p>处理完之后还是有个一个小问题。</p><blockquote><p>/anaconda/envs/py27/lib/libjpeg.9.dylib does not exist, hence can’t correct: @rpath/libjpeg.9.dylib in: /anaconda/envs/py27/lib/libopencv_highgui.2.4.12.dylib</p><p>/anaconda/envs/py27/lib/libjpeg.9.dylib does not exist, hence can’t correct: @rpath/libjpeg.9.dylib in: /anaconda/envs/py27/lib/libopencv_highgui.2.4.dylib</p><p>/anaconda/envs/py27/lib/libjpeg.9.dylib does not exist, hence can’t correct: @rpath/libjpeg.9.dylib in: /anaconda/envs/py27/lib/libopencv_highgui.dylib</p></blockquote><p>这时候进行 python import cv2的时候会报错。。实际根据自己情况手工输入地址 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 大概是这样子的具，体可以看rpathChangeCmd.txt里面的格式选择自己的文件位置（自己找一下啦）：</span></span><br><span class="line">sudo install_name_tool -change @rpath/libjpeg.9.dylib /xxx/xxx/x/x/x/xxx.dylib /xx/xx/xx/cv2.so</span><br></pre></td></tr></table></figure><p>我是直接找到那个文件，然后直接复制进对应的位置就没报错了。。:）不知道之后有没有问题，跑了一个例子正常显示。我觉得不太影响。。</p><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><blockquote><p>/anaconda3/envs/cv/lib/python3.6/site-packages/numpy/core/include/numpy/npy_common.h:17:5: warning:<br>      ‘NPY_INTERNAL_BUILD’ is not defined, evaluates to 0 [-Wundef]</p></blockquote><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fkp2bxa5pmj30zu09gdhx.jpg" alt></p><p>编辑相应的目录下面 <strong>npy_common.h</strong> </p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> NPY_INTERNAL_BUILD</span></span><br><span class="line"><span class="comment">//改为</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> NPY_INTERNAL_BUILD</span></span><br><span class="line"><span class="comment">//再在之后加入</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> NPY_INTERNAL_BUILD</span></span><br></pre></td></tr></table></figure><p>重新make即可</p>]]></content>
      
      
      <categories>
          
          <category> Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>目标跟踪</title>
      <link href="/2017/06/06/track/"/>
      <url>/2017/06/06/track/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>没事读读目标跟踪的资料论文</b></font></center><a id="more"></a><h1 id="目标跟踪"><a href="#目标跟踪" class="headerlink" title="目标跟踪"></a>目标跟踪</h1><h2 id="视频目标跟踪系统框架和关键技术"><a href="#视频目标跟踪系统框架和关键技术" class="headerlink" title="视频目标跟踪系统框架和关键技术"></a>视频目标跟踪系统框架和关键技术</h2><h3 id="运动模型"><a href="#运动模型" class="headerlink" title="运动模型"></a>运动模型</h3><p>目前 运动模型主要分为三种:</p><ol><li>均值漂移 (Mean shift)</li><li>滑动窗口 (Slide window)</li><li>粒子滤波 (Particle filter)</li></ol><blockquote><p>1) 均值漂移 (Mean shift)<br>均值漂移, 是一种基于核密度估计的非参数估计方法.为经典跟踪方法. 在跟踪时, 需要设定一个目标函数来计算目标与候选窗口的核密度, 而后利用 Bhattacharyya 准则作为匹配条件, 通过移动均值向量来不断优化目标函数从而完成目标搜索. 由于通过梯度优化来完成搜索, 因此基于均值漂移的跟踪算法运行速度快、实时性高.</p></blockquote><blockquote><p>2) 滑动窗口 (Slide window)<br>在目标周边正方形或者圆形范围内进行穷举搜 索的采样策略, 也称为密集采样. 这种方式将搜索范围内所有可能的潜在位置都予以考虑, 但是要付出 较大的计算代价.</p></blockquote><blockquote><p>3) 粒子滤波 (Particle filter)<br>粒子滤波在经典的卡尔曼滤波的基础上发展而来, 先验概率密度用加权粒采样样本 (粒子) 来近 似表示. 每个粒子的权值表示了该样本的重要程度. 每次跟踪结果确定后, 会根据不同粒子的重要程度进行重采样. 粒子滤波方法具有较高的计算效率, 同 可以融入仿射变换信息, 因此目前在一些较好的跟踪算法中应用较多. [^1]</p></blockquote><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>对于视频目标跟踪而言,好的特征应该具备两个基本性质:</p><ol><li>具有较强的区分度</li><li>具有较高的计算效率,以满足跟踪的实时性</li></ol><blockquote><p>目前跟踪算法采用的特征分为人工特征和学习特征两类.人工特征可以分为外观特征和运动特征.外观特征是从目标的物理直观出发, 通过结合数学工具设计出来的特征.运动特征是针对视频的特点, 从视频帧之间的时间关联性出发设计的特征, 这些 特征是静态图像中所没有的. 由机器自动学习到的特征为学习特征. 这些特征通过机器学习的方式自动提取, 无需事先知道目标的物理性质, 从而可以大大提高特征提取的效率. 目前以深度学习为代表的特征学习方法已经成为计算机领域的前沿和热点.</p></blockquote><h4 id="人工特征"><a href="#人工特征" class="headerlink" title="人工特征"></a>人工特征</h4><ul><li>外观特征</li></ul><blockquote><p>外观特征总体上可以分为四类: 灰度特征、颜色特征、梯度特征和纹理特征.</p></blockquote><blockquote><p>灰度特征计算效率高,可以分为原始灰度特征、灰度直方图特征、区域灰度变化特征 (Haar 特征) 三种表征形式. </p></blockquote><blockquote><p>颜色特征主要分为两种: 一种以颜色直方图来表征,另一种则是近年来兴起的具有更好表征能力的 Color name 特征. 颜色特征对姿态、尺度等不敏感, 用于非刚体跟踪时具有一定优势. 但其受光照影响较大, 同时易受颜色相近背景的干扰.</p></blockquote><blockquote><p>纹理特征是对目标外观细节、规则程度的量化,目前跟踪算法中常用的纹理特征是局部二值模式 (Local binary pattern, LBP),纹理特征可以较好地描述目标外观的细节, 但是对于纹理细节少、小尺度、远距离或者背景纹理复杂的目标描述能力较差, 此时跟踪效果往往不理想.</p></blockquote><blockquote><p>梯度特征通过统计目标图像局部的梯度分布来表征外观. 目前主要有 SIFT (Scale invariant feature transform) 特征及其加速版本 SURF (Speeded up robust features) 特征,实时性较差.另一种梯度特征是 HOG (Histogram of oriented gradient) </p></blockquote><ul><li>运动特征</li></ul><p>运动特征旨在挖掘视频帧之间的时空关联性.</p><blockquote><p>光流法: 是对局部图像运动的一种近似表达, 主要通过计算给定视频中局部图像的时间与空间导数, 近 似得出二维运动场.目前有 LK 算法和 HS 算法,LK 算法运算效率较高.</p></blockquote><h3 id="学习特征"><a href="#学习特征" class="headerlink" title="学习特征"></a>学习特征</h3><p>PCA 可以被视为最早的自动特征提取的方法.近几年深度学习的方法成为目前最强的自动特征提取方法.</p><h3 id="外观模型"><a href="#外观模型" class="headerlink" title="外观模型"></a>外观模型</h3><p>之后的会介绍到的产生式和判别式模型正是属于这里的外观模型.</p><h3 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h3><blockquote><p>目前更新策略研究相对较少, 主要的更新策略有:</p></blockquote><blockquote><p>1) 每一帧都进行更新. 该方式较简单, 目前应用较多. 但由于太过频繁, 增加了漂移的可能性.</p></blockquote><blockquote><p>2) 每隔一定的帧数才更新一次.</p></blockquote><blockquote><p>3) 当响应分数 (匹配或分类得分) 低于一定阈值时才更新. 低于阈值往往说明目标外观已发生较大变化. 在该策略中, 增加了对外观变化程度的判断, 减少了更新频率, 因而比策略 1) 效果好一些.</p></blockquote><blockquote><p>4) 分别计算正负样本的响应分数, 当两者的差值低于一定阈值时更新. 该方式在判别式模型中采用. 由于考虑了前景与背景的差异度量, 可使跟踪器具有更好的鉴别能力.</p></blockquote><h2 id="经典的目标跟踪方法分类"><a href="#经典的目标跟踪方法分类" class="headerlink" title="经典的目标跟踪方法分类"></a>经典的目标跟踪方法分类</h2><p>目前分为两大类别</p><ul><li>产生式(generative model)</li></ul><blockquote><p>产生式方法运用生成模型描述目标的表观特征，之后通过搜索候选目标来最小化重构误差。比较有代表性的算法有稀疏编码(sparse coding)，在线密度估计(online density estimation)和主成分分析(PCA)等。产生式方法着眼于对目标本身的刻画，忽略背景信息，在目标自身变化剧烈或者被遮挡时容易产生漂移。</p></blockquote><ul><li>判别式(discriminative model)</li></ul><blockquote><p>与之相对的，判别式方法通过训练分类器来区分目标和背景。这种方法也常被称为tracking-by-detection。近年来，各种机器学习算法被应用在判别式方法上，其中比较有代表性的有多示例学习方法(multiple instance learning), boosting和结构SVM(structured SVM)等。</p></blockquote><blockquote><p>判别式方法因为显著区分背景和前景的信息，表现更为鲁棒，逐渐在目标跟踪领域占据主流地位。值得一提的是，目前大部分深度学习目标跟踪方法也归属于判别式框架。</p></blockquote><h2 id="基于深度学习的目标跟踪"><a href="#基于深度学习的目标跟踪" class="headerlink" title="基于深度学习的目标跟踪"></a>基于深度学习的目标跟踪</h2><p>训练数据较小,目标跟踪仅提供第一帧的bounding-box作为训练数据。</p><p>在线训练深层网络比较困难,在视觉对象跟踪中,缺乏对一个对象的先验知识.</p><h3 id="利用辅助图片预训练深度模型，在线跟踪时微调"><a href="#利用辅助图片预训练深度模型，在线跟踪时微调" class="headerlink" title="利用辅助图片预训练深度模型，在线跟踪时微调"></a>利用辅助图片预训练深度模型，在线跟踪时微调</h3><h4 id="DLT-和-SO-DLT-王乃岩"><a href="#DLT-和-SO-DLT-王乃岩" class="headerlink" title="DLT 和 SO-DLT  王乃岩"></a>DLT 和 SO-DLT  王乃岩</h4><h6 id="DLT"><a href="#DLT" class="headerlink" title="DLT"></a>DLT</h6><p><img src="http://olq8hji1u.bkt.clouddn.com/20170512149457817358698.jpg" alt="20170512149457817358698.jpg"></p><p>使用栈式降噪自编码器(stacked denoising autoencoder，SDAE)在Tiny Images dataset上进行无监督离线预训练。将32*32的彩色图片转为灰度图(文章说彩色图也可以)，传入的是1024维向量，进行训练。应用SDAE 1024-2560-2014-512-256主要的瓶颈式结构，如图b。</p><p>跟踪时采取图c的策略，取得b的encoding部分，加入sigmoid分类层，组成分类网络，这时候没有获取对当前被跟踪物体的特定表达能力，此时利用第一帧获取正负样本，对分类网络径向fine-tune获得对当前跟踪目标和背景更有针对性的分类网络。在跟踪过程中，对当前帧采用粒子滤波(particle filter)的方式提取一批候选的patch(相当于detection中的proposal)，这些patch输入分类网络中，置信度最高的成为最终的预测目标。</p><p>在目标跟踪上，论文采用了限定阈值的方式，即去所有粒子中最高的confidence低于阈值时，认为目标已经发生了较大变化，当前分类网络已经无法适应，需要更新。</p><blockquote><p>存在的问题：<br><br>(1)离线预训练采用的数据集Tiny Images dataset只包含32*32大小的图片，分辨率明显低于主要的跟踪序列，因此SDAE很难学到足够强的特征表示。<br><br>(2)离线阶段的训练目标为图片重构，这与在线跟踪需要区分目标和背景的目标相差甚大。<br><br>(3)SDAE全连接的网络结构使其对目标的特征刻画能力不够优秀，虽然使用了4层的深度模型，但效果仍低于一些使用人工特征的传统跟踪方法如Struck等。</p></blockquote><h5 id="SO-DLT"><a href="#SO-DLT" class="headerlink" title="SO-DLT"></a>SO-DLT</h5><p><img src="http://olq8hji1u.bkt.clouddn.com/20170512149458086758677.jpg" alt="20170512149458086758677.jpg"></p><p>主要延续了DLT利用非跟踪数据预训练加在线微调的策略，来解决跟踪过程中训练数据不足的问题，同时对DLT问题做了很大改进。<br><br>使用CNN作为提取特征和分类的网络模型，结构类似于AlexNet，根据跟踪候选区域的大小输入为100*100，而不是分类或者检测的224*224。输出为50*50，值是在0-1之间的概率图。概率图的每一个像素代表着原图2*2的区域，输出值越高表示该点在目标bounding box中的概率越高，意思就是是否属于分类检测出来的物体。<br><br>在卷积层和全连接层采用SPP-NET中的空间金字塔池化(spatial pyramid pooling)提高精度。<a href="http://blog.csdn.net/liyaohhh/article/details/50614380" target="_blank" rel="noopener">这里有SPP的简单介绍</a></p><h3 id="Hierarchical-Convolutional-Features-for-Visual-Tracking-ICCV15"><a href="#Hierarchical-Convolutional-Features-for-Visual-Tracking-ICCV15" class="headerlink" title="Hierarchical Convolutional Features for Visual Tracking(ICCV15)"></a>Hierarchical Convolutional Features for Visual Tracking(ICCV15)</h3><p>采用了 CNN 提取特征.具体的可以看下面的代码和中文笔记.</p><p><a href="https://github.com/jbhuang0604/CF2" target="_blank" rel="noopener">Github源码</a></p><p><a href="http://blog.csdn.net/autocyz/article/details/50827010" target="_blank" rel="noopener">中文笔记</a></p><h3 id="MDNET-Multi-Domain-Network"><a href="#MDNET-Multi-Domain-Network" class="headerlink" title="MDNET(Multi-Domain Network)"></a>MDNET(Multi-Domain Network)</h3><p>paper: Learning Multi-Domain Convolutional Neural Networks for Visual Tracking<br>CVPR 2016</p><p>CNN 中因为训练数据较少的原因,用很难直接套用之前的网络提取特征.之前分类问题表现较好的 CNN 就很难直接在跟踪定位问题方面应用.</p><p>文章中提到的之前基于 CNN 的目标跟踪算法</p><blockquote><p>J. Fan, W. Xu, Y. Wu, and Y. Gong. Human tracking us-<br>ing convolutional neural networks. IEEE Trans. Neural Net-<br>works, 21(10):1610–1623, 2010. </p></blockquote><blockquote><p>S. Hong, T. You, S. Kwak, and B. Han. Online tracking by learning discriminative saliency map with convolutional neural network. In ICML, 2015.</p></blockquote><blockquote><p>H. Li, Y. Li, and F. Porikli. DeepTrack: Learning discrimina- tive feature representations by convolutional neural networks for visual tracking. In BMVC, 2014.</p></blockquote><blockquote><p>N. Wang, S. Li, A. Gupta, and D.-Y. Yeung. Transferring rich feature hierarchies for robust visual tracking. arXiv preprint arXiv:1501.04587, 2015.</p></blockquote><p><img src="http://olq8hji1u.bkt.clouddn.com/20170601149628621696254.png" alt="20170601149628621696254.png"></p><center>MDNet的结构</center><p>MDNet 中 CNN 只有 3层 CONV 和 2层 FC ,很浅.</p><p>论文中提到了为什么选用这种结构,大概意思就是视觉跟踪对于复杂分类的 ImageNet 复杂度较低,就是区分目标和背景2类.第二 CNN 对于精确目标定位的效率变低,因为随着网络的深入,空间信息会被稀释.第三,视觉跟踪的目标比较小,所以期望输入的尺寸小,所以降低了网络的深度,在网络更新测试时一个小的网络会更有效率..(这里有人提出了质疑)..反正这个肯定是作者先试了很多个结构,最后发现这个不错,最后再编原因的…论文嘛,你懂的.</p><blockquote><p>Our network architecture is substantially smaller than the ones commonly used in typical recognition tasks such as AlexNet [27] and VGG-Nets [5, 34]. We believe that such a simple architecture is more appropriate for visual tracking due to the following reasons. First, visual tracking aims to distinguish only two classes, target and background, which requires much less model complexity than general visual recognition problems such as ImageNet classification with 1000 classes. Second, a deep CNN is less effective for pre- cise target localization since the spatial information tends to be diluted as a network goes deeper [20]. Third, since targets in visual tracking are typically small, it is desirable to make input size small, which reduces the depth of the network naturally. Finally, a smaller network is obviously more efficient in visual tracking problem, where training and testing are performed online. When we tested larger networks, the algorithm is less accurate and becomes slower significantly.</p></blockquote><p>学习的算法大概原理是训练一个 Multi-domain CNN 区分一个任意 domain 的目标和背景.<br>虽然 domian 不同但是对于某些属性特征是通用的.例如光照变化,运动的模糊,尺寸变化的鲁棒性等..<br>CNN 用的是 SGD 训练.</p><p><a href="http://backnode.github.io/pages/2015/11/02/CNN-for-tracking.html" target="_blank" rel="noopener">论文中文笔记:backnode</a></p><p><a href="http://www.cnblogs.com/wangxiaocvpr/p/5598608.html" target="_blank" rel="noopener">论文中文笔记:wangxiao</a></p><h3 id="siamese-fc"><a href="#siamese-fc" class="headerlink" title="siamese-fc"></a>siamese-fc</h3><p><a href="https://arxiv.org/abs/1606.09549" target="_blank" rel="noopener">paper:Fully-Convolutional Siamese Networks for Object Tracking</a></p><p><a href="https://github.com/bertinetto/siamese-fc" target="_blank" rel="noopener">Github-source</a></p><p>牛津视觉研究所的发的论文.</p><p><img src="http://olq8hji1u.bkt.clouddn.com/20170603149645819879466.jpg" alt="20170603149645819879466.jpg"></p><p>提出一种<code>全卷积</code>的孪生网络结构</p><p>学习去跟踪任意的物体，通过相似性学习的方法来解决。利用卷积神经网络来解决相似性函数的学习问题。并且通常用 Siamese architecture 来充当深度卷积网络。</p><p>先说自己是目前实时性最高,效果最好的方法.<br><br>然后论文套路说之前一些网络存在的问题,对浅层的方法没有充分利用到端到端的好处,而且在跟踪过程中使用 SGD 没有办法在达到好结果的前提下达到实时性.</p><p>利用 ImageNet video 进行 offline 的训练。这个数据集有 80多G 标注好的 video。<br><br>然后继续数据处理,比如排除一些类别:sanke,train,whale,lizard 等,这些物体经常只出现身体的某一个部分.而且在边缘出现. 排除太小或者太大的物体.  排除离边界很近的.</p><p>由于采用了全卷积,优点很明显,一是可以接受任意大小的输入图像，而不用要求所有的训练图像和测试图像具有同样的尺寸。二是更加高效，因为避免了由于使用像素块而带来的重复存储和计算卷积的问题。</p><p><a href="http://www.cnblogs.com/wangxiaocvpr/p/5897461.html" target="_blank" rel="noopener">中文笔记:wangxiao</a></p><h3 id="CFNet"><a href="#CFNet" class="headerlink" title="CFNet"></a>CFNet</h3><p>依然是牛津视觉研究所(CVPR 17).</p><p><a href="https://arxiv.org/abs/1704.06036v1" target="_blank" rel="noopener">paper:End-to-end representation learning for Correlation Filter based tracking</a></p><p><a href="https://github.com/bertinetto/cfnet" target="_blank" rel="noopener">github-source</a></p><p><img src="http://olq8hji1u.bkt.clouddn.com/20170603149645808433027.jpg" alt="20170603149645808433027.jpg"></p><p>提出 CFNet 的结构,在训练时加入一个 CF 滤波器.然后进行 crop .</p><p>介绍说是能够学习到更深层次的特征.而且可以实现实时.<br><br>第二个版本相比较于第一个版本利用了 CF 进行在线学习,它的速度比使用 SGD 更有效率.<br><br>这个提出将 CF 的在线学习的效率与 CNN 的辨别能力结合起来.然后说在足够深的网络中, CF 不能改善结果.<br>后面是大量的数学公式….</p><h2 id="目标跟踪算法的评价方法"><a href="#目标跟踪算法的评价方法" class="headerlink" title="目标跟踪算法的评价方法"></a>目标跟踪算法的评价方法</h2><p>三个要求: 准确性 鲁棒性 高效性</p><ul><li>准确性</li></ul><ol><li>偏移(Deviation)</li><li>误检(False positive)</li><li>漏检(False negative)</li></ol><ul><li>鲁棒性</li></ul><p>在大多数不同的视频中表现性能都较高.能应对复杂多样的场景.</p><ul><li>高效性</li></ul><p>实时性!</p><h3 id="总体性能要求的评价准则"><a href="#总体性能要求的评价准则" class="headerlink" title="总体性能要求的评价准则"></a>总体性能要求的评价准则</h3><ol><li><p>中心误差 (Center location error): 每一帧中跟踪器输出的矩形框中心与实际中心位置的欧氏距 离. 加和后取平均值为平均中心误差. 中心误差越小, 说明跟踪效果越好.</p></li><li><p>重叠率 (Overlap rate): 设 $S_T$ 是跟踪器输出的跟踪框区域, $S_G$ 为实际目标区域, 则重叠率的定义为两者的交集与并集的比值, 即: </p>$ R = {{area({S_T}∩{S_G})} \over {area({S_T}∪{S_G})}} $ <p>, 重叠率越高, 说明跟踪效果越好.</p></li><li><p>成功率 (Success rate): 对于每一帧而言, 若中心误差小于一定阈值或重叠率大于一定阈值则认为该帧跟踪成功. 跟踪成功的帧数同视频序列总帧数的比值称为成功率.</p></li><li><p>精度图 (Precision plot) 与成功图 (Success plot): 将 3) 中所设置的阈值在一定范围内变动时, 会得到一系列的成功率数值所构成的曲线图, 当对应于中心误差时构成的曲线称为精度图; 对应于重 叠率时称为成功图.</p></li><li><p>时间鲁棒性度量 (Temporal robustness eval- uation, TRE) 和空间鲁棒性度量 (Spatial robust- ness evaluation, SRE). TRE 跟踪器用测 试视频序列中的随机的一帧进行初始化而不是第一帧, 作出其相应的成功图, 以此来衡量跟踪器在时间 轴上的鲁棒性. SRE 跟踪器用第一帧初始化, 但对 初始跟踪框位置进行了一定的平移、缩放等微小扰动, 做出相应的成功图, 以此来测试跟踪器能否在随后帧中稳定跟踪住目标.</p></li><li><p>FPS (Frames per second) 每秒处理的帧数, 是一个用来衡量跟踪算法处理效率和速度的常用指标.</p></li></ol><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>持续更新…</p><ul><li><p>图像识别 ImageNet</p></li><li><p>目标检测 Pascal VOC </p></li><li><p>视频检索 TRECVID</p></li><li><p>视频跟踪 VTB , Object Tracking Benchmark , VOT</p></li></ul><p>参考：</p><p><a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&mid=2650325425&idx=1&sn=3f3a9ce9aef82af63b28286081c93386#rd" target="_blank" rel="noopener">深度学习在目标跟踪中的应用</a></p><p>[^1]: 管皓, 薛向阳, 安志勇. 深度学习在视频目标跟踪中的应用进展与展望. 自动化学报, 2016, 42(6): 834−847</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tracking </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>搭建一个机器学习的环境</title>
      <link href="/2017/05/26/mlenv/"/>
      <url>/2017/05/26/mlenv/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>ubuntu配置一个python机器学习的环境</b></font></center><center>python3.5.2(anaconda3)+tensorflow1.1+keras2+cuda8.0+cudnn5.1+...</center><a id="more"></a><p>首先介绍一下一共用到的一些配置</p><p>查了很多资料,基本上没有基于 ubuntu16.04+py3+keras2+tensorflow1.1+cuda8+cudnn5.1这样一个机器学习环境</p><p>配置了2天,查了很多资料,这里总结一下..</p><h2 id="下载-anaconda"><a href="#下载-anaconda" class="headerlink" title="下载 anaconda"></a>下载 anaconda</h2><p>anaconda 是一个科学计算的python包,很方便..</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 官网的下载地址</span></span><br><span class="line">wget https://repo.continuum.io/archive/Anaconda3-4.3.1-Linux-x86_64.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者使用清华源的镜像</span></span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-4.3.1-Linux-x86_64.sh</span><br><span class="line"></span><br><span class="line">然后 进入下载目录</span><br><span class="line">bash Anaconda3-4.3.1-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p>之后创建一个 python3.5.2的镜像,因为 TensorFlow 这玩意不支持3.6(截止至这篇文章打出来,官网已经说是支持3.6,但是 github 上面的仓库并没有更新)具体的可以去看<a href="https://www.tensorflow.org" target="_blank" rel="noopener">TensorFlow官网</a>或<a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">github-repo</a>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个 python3.5.2的名字叫 tensorflow 的一个环境</span></span><br><span class="line">conda create --name tensorflow python=3.5.2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活这个环境</span></span><br><span class="line"><span class="built_in">source</span> activate tensorflow </span><br><span class="line"></span><br><span class="line"><span class="comment"># 不使用这个环境时</span></span><br><span class="line">deactivate</span><br></pre></td></tr></table></figure><h2 id="下载一些依赖包"><a href="#下载一些依赖包" class="headerlink" title="下载一些依赖包"></a>下载一些依赖包</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get --assume-yes upgrade</span><br><span class="line">sudo apt-get --assume-yes install tmux build-essential gcc g++ make binutils</span><br><span class="line">sudo apt-get --assume-yes install software-properties-common</span><br></pre></td></tr></table></figure><h2 id="下载-GPU-驱动"><a href="#下载-GPU-驱动" class="headerlink" title="下载 GPU 驱动"></a>下载 GPU 驱动</h2><p>去<a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">这里</a> 选择适合自己版本的驱动下载下来</p><p>或者 <a href="http://developer.download.nvidia.com/compute/cuda/repos/" target="_blank" rel="noopener">http://developer.download.nvidia.com/compute/cuda/repos/</a> 这里也下载</p><p>下载 deb 文件之后安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 后面的改成你安装的版本</span></span><br><span class="line">sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install cuda</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入驱动</span></span><br><span class="line">sudo modprobe nvidia</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><h2 id="安装python库-配置一些东西"><a href="#安装python库-配置一些东西" class="headerlink" title="安装python库,配置一些东西"></a>安装python库,配置一些东西</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在刚刚创建的 tensorflow 的这个 python3.5.2的环境中</span></span><br><span class="line"><span class="comment"># 注意这里的 TensorFlow 安装 GPU 版本的,但是会在运行时候出现警告,提示如果支持一些指令集就能提高速度,这时候你想的话就去编译源码...这里不多说了</span></span><br><span class="line"><span class="comment"># 这里安装用 pip 也行</span></span><br><span class="line"><span class="comment"># 如果速度过慢,可以去找清华源的镜像</span></span><br><span class="line">conda install numpy</span><br><span class="line">conda install scipy</span><br><span class="line">conda install matplotlib</span><br><span class="line">conda install tensorflow-gpu</span><br><span class="line">conda install theano</span><br><span class="line">conda install keras</span><br><span class="line">...等等...</span><br><span class="line"><span class="comment"># 把你需要的下载下来</span></span><br><span class="line"><span class="comment"># 之后主要的是</span></span><br><span class="line">conda install bcolz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新库</span></span><br><span class="line">conda upgrade -y --all</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置 theano</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"[global]</span></span><br><span class="line"><span class="string">device = gpu</span></span><br><span class="line"><span class="string">floatX = float32</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[cuda]</span></span><br><span class="line"><span class="string">root = /usr/local/cuda"</span> &gt; ~/.theanorc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 keras , backend 可以选择 theano 或者 tensorflow</span></span><br><span class="line">mkdir ~/.keras</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'&#123;</span></span><br><span class="line"><span class="string">    "image_dim_ordering": "th",</span></span><br><span class="line"><span class="string">    "epsilon": 1e-07,</span></span><br><span class="line"><span class="string">    "floatx": "float32",</span></span><br><span class="line"><span class="string">    "backend": "theano"</span></span><br><span class="line"><span class="string">&#125;'</span> &gt; ~/.keras/keras.json</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 cudnn</span></span><br><span class="line">wget <span class="string">"http://platform.ai/files/cudnn.tgz"</span> -O <span class="string">"cudnn.tgz"</span></span><br><span class="line">tar -zxf cudnn.tgz</span><br><span class="line"><span class="built_in">cd</span> cuda</span><br><span class="line">sudo cp lib64/* /usr/<span class="built_in">local</span>/cuda/lib64/</span><br><span class="line">sudo cp include/* /usr/<span class="built_in">local</span>/cuda/include/</span><br></pre></td></tr></table></figure><h2 id="设置-jupyter"><a href="#设置-jupyter" class="headerlink" title="设置 jupyter"></a>设置 jupyter</h2><p>运行下面的 bash 脚本 将 * 改成你想设置的密码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br><span class="line">jupass=`python -c <span class="string">"from notebook.auth import passwd; print(passwd())"</span>`</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"c.NotebookApp.password = u'"</span><span class="variable">$jupass</span><span class="string">"'"</span> &gt;&gt; <span class="variable">$HOME</span>/.jupyter/jupyter_notebook_config.py</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"c.NotebookApp.ip = '*'</span></span><br><span class="line"><span class="string">c.NotebookApp.open_browser = False"</span> &gt;&gt; <span class="variable">$HOME</span>/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure><h2 id="OK"><a href="#OK" class="headerlink" title="OK"></a>OK</h2><p>浏览器打开试一试</p><p><a href="http://localhost:8888" target="_blank" rel="noopener">http://localhost:8888</a></p><h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><p>你在远程服务器上面配置好了之后,却打不开相关的 jupyter 页面</p><p>有时候你可以建立 ssh 的隧道 就可以实现本地访问</p><p>下面是实现本地<a href="http://127.0.0.1:1234" target="_blank" rel="noopener">127.0.0.1:1234</a>访问</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ssh usrname@yourhost -L127.0.0.1:1234:yourhost:port</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后进入py3.5.2的环境</span></span><br><span class="line"><span class="built_in">source</span> activate tensorflow</span><br><span class="line">jupyter notebook XXX</span><br></pre></td></tr></table></figure><p>成功之后打开 jupyter 的网页</p><p><code>import theano</code> 成功就如下显示</p><blockquote><p>Using Theano backend.<br>WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:<br> <a href="https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29" target="_blank" rel="noopener">https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29</a></p></blockquote><blockquote><p>Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5103)</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Linux </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器搭建Hexo博客!</title>
      <link href="/2017/05/12/hexo/"/>
      <url>/2017/05/12/hexo/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>云服务器搭建Hexo博客</b></font></center><a id="more"></a><hr><h1 id="云服务器搭建Hexo博客"><a href="#云服务器搭建Hexo博客" class="headerlink" title="云服务器搭建Hexo博客"></a>云服务器搭建Hexo博客</h1><p>这里介绍一下云服务器搭建Hexo博客,并且利用Nginx做代理,并且利用<a href="https://coding.net/" target="_blank" rel="noopener">Coding.net</a>或者<a href="https://github.com" target="_blank" rel="noopener">Github</a>同步页面,最后发布。</p><p>效果参考:<a href="http://b.heywe.cn" target="_blank" rel="noopener">我的博客</a></p><p>我还有一个<a href="http://heywe.cn">博客</a>架设在github pages上面,为什么我不喜欢用github pages创建呢,因为中国伟大的墙,导致github不稳定,这会使得没梯子时经常抽风,虽然理论上是称github是没被墙的,但是访问经常缓慢,什么的很不方便(做开发、搞研究还是得自备梯子)。另外没事练练动手能力吧。</p><h2 id="搭建Hexo本地环境"><a href="#搭建Hexo本地环境" class="headerlink" title="搭建Hexo本地环境"></a>搭建Hexo本地环境</h2><p>先在本地搭建好hexo环境。</p><p>这个我不多说,网上教程一大堆。最好是看到那种搭建并且有创建Github Pages的博客教程。</p><p><code>需要注意的是Hexo自带的Markdown解析器解析Markdown格式时会有bug存在。</code></p><p>例如在表格中转义“|”这个竖杠时产生BUG,在生成html时会产生大量<code>&lt;br&gt;</code>,我在写正则表达式笔记时候在Macdown和Atom这些当作都可以正常解析,但是在Hexo中解析不了。</p><p>网上提出的解决方法是使用<code>pandoc</code>,进入博客目录:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-renderer-pandoc --save</span><br><span class="line"><span class="comment"># ps: 但是我当初没有采用这个,好像还是存在问题,又或者产生了新的问题</span></span><br></pre></td></tr></table></figure><p>还有一种解决方法是在<code>&#39;youblog&#39;/node_modules/marked/lib</code>中<code>marked.js</code></p><p>备份一下然后</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*在450行左右找到*/</span></span><br><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([\\`*&#123;&#125;\[\]()#+\-.!_&gt;])/</span>,</span><br><span class="line"><span class="comment">/*改为*/</span></span><br><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*\[\]()#+\-.!_&gt;])/</span>,</span><br><span class="line"><span class="comment">/*850行左右找到*/</span></span><br><span class="line"><span class="keyword">return</span> <span class="string">'&lt;em&gt;'</span> + text + <span class="string">'&lt;/em&gt;'</span>;</span><br><span class="line"><span class="comment">/*改为*/</span></span><br><span class="line"><span class="keyword">return</span> <span class="string">'_'</span> + text + <span class="string">'_'</span>;</span><br></pre></td></tr></table></figure><p>如果文章中有公式使用Mathjax引擎,在使用默认主题的时候</p><p>进入<code>&#39;youblog&#39;/themes/landscape/layout/_partial/</code></p><ul><li>仿照别的主题用法在 after-footer.ejs 加入</li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;% <span class="keyword">if</span> (theme.mathjax)&#123; %&gt;</span><br><span class="line">&lt;%- partial(<span class="string">'mathjax'</span>) %&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure><ul><li>再新建文件 mathjax.ejs 里面写入</li></ul><p>这里因为mathjax提供的官方CDN关闭，采用了第三方的CDN，你也可以自己修改CDN，或者自己搭建mathjax。。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=<span class="string">"text/x-mathjax-config"</span>&gt;</span><br><span class="line">    MathJax.Hub.Config(&#123;<span class="string">"HTML-CSS"</span>: &#123; <span class="attr">preferredFont</span>: <span class="string">"TeX"</span>, <span class="attr">availableFonts</span>: [<span class="string">"STIX"</span>,<span class="string">"TeX"</span>], <span class="attr">linebreaks</span>: &#123; <span class="attr">automatic</span>:<span class="literal">true</span> &#125;, <span class="attr">EqnChunk</span>: (MathJax.Hub.Browser.isMobile ? <span class="number">10</span> : <span class="number">50</span>) &#125;,</span><br><span class="line">        tex2jax: &#123; <span class="attr">inlineMath</span>: [ [<span class="string">"$"</span>, <span class="string">"$"</span>], [<span class="string">"\\("</span>,<span class="string">"\\)"</span>] ], <span class="attr">processEscapes</span>: <span class="literal">true</span>, <span class="attr">ignoreClass</span>: <span class="string">"tex2jax_ignore|dno"</span>,<span class="attr">skipTags</span>: [<span class="string">'script'</span>, <span class="string">'noscript'</span>, <span class="string">'style'</span>, <span class="string">'textarea'</span>, <span class="string">'pre'</span>, <span class="string">'code'</span>]&#125;,</span><br><span class="line">        TeX: &#123;  <span class="attr">noUndefined</span>: &#123; <span class="attr">attributes</span>: &#123; <span class="attr">mathcolor</span>: <span class="string">"red"</span>, <span class="attr">mathbackground</span>: <span class="string">"#FFEEEE"</span>, <span class="attr">mathsize</span>: <span class="string">"90%"</span> &#125; &#125;, <span class="attr">Macros</span>: &#123; <span class="attr">href</span>: <span class="string">"&#123;&#125;"</span> &#125; &#125;,</span><br><span class="line">        messageStyle: <span class="string">"none"</span></span><br><span class="line">    &#125;);</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&lt;script type="text/</span>x-mathjax-config<span class="string">"&gt;</span></span><br><span class="line"><span class="string">    MathJax.Hub.Queue(function() &#123;</span></span><br><span class="line"><span class="string">        var all = MathJax.Hub.getAllJax(), i;</span></span><br><span class="line"><span class="string">        for(i=0; i &lt; all.length; i += 1) &#123;</span></span><br><span class="line"><span class="string">            all[i].SourceElement().parentNode.className += ' has-jax';</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    &#125;);</span></span><br><span class="line"><span class="string">&lt;/script&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&lt;script src="</span><span class="comment">//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt;</span></span><br></pre></td></tr></table></figure><ul><li>之后在theme/landscape/_config.yml配置中加入</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mathjax: true</span><br></pre></td></tr></table></figure><p>这时候你应该可以使用mathjax引擎了,测试一下,新建一个文章<code>hexo n &quot;test&quot;</code>,然后输入:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin &#123;aligned&#125; </span><br><span class="line">\frac&#123;\partial&#125;&#123;\partial \theta_i&#125;J(\theta) &amp;= \frac&#123;\partial&#125;&#123;\partial \theta_i&#125;&#123;\frac&#123;1&#125;&#123;2&#125;&#125;&#123;(h_\theta(x)-y)^2&#125; \\\</span><br><span class="line">&amp;=(h_\theta(x)-y) \frac&#123;\partial&#125;&#123;\partial \theta_i&#125;(\theta_0x_0+\cdots+\theta_nx_n-y) \\\</span><br><span class="line">&amp;=(h_\theta(x)-y)x_i</span><br><span class="line">\end &#123;aligned&#125;  \\</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">\theta_i:=\theta_i-\alpha\sum_&#123;j=1&#125;^n (h_\theta(x^&#123;(j)&#125;)-y^&#123;(j)&#125;)x_i^&#123;(j)&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>之后<code>hexo g</code>,<code>hexo s</code>测试一下</p><p>成功的效果应该如下:</p><p>$$<br>\begin {aligned}<br>\frac{\partial}{\partial \theta_i}J(\theta) &amp;= \frac{\partial}{\partial \theta_i}{\frac{1}{2}}{(h_\theta(x)-y)^2} \<br>&amp;=(h_\theta(x)-y) \frac{\partial}{\partial \theta_i}(\theta_0x_0+\cdots+\theta_nx_n-y) \<br>&amp;=(h_\theta(x)-y)x_i<br>\end {aligned}  \<br>$$</p><p>$$<br>\theta_i:=\theta_i-\alpha\sum_{j=1}^n (h_\theta(x^{(j)})-y^{(j)})x_i^{(j)}<br>$$</p><ul><li>如果你使用主题,如next,yilia等,一般都会在<code>主题配置文件</code>,<code>_config.yml</code>中找到mathjax的相关设置。一般直接启用就可以了。</li></ul><h2 id="服务器"><a href="#服务器" class="headerlink" title="服务器"></a>服务器</h2><p>之后需要使用云服务器</p><p>我使用的是腾讯云或者阿里云的ubuntu服务器,centos也行,操作差不多。</p><p>之后就要下载一些常用的软件</p><h3 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h3><ul><li>这里用到了<code>git</code>:</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ubuntu</span></span><br><span class="line">sudo apt-get install git</span><br><span class="line"><span class="comment"># centos</span></span><br><span class="line">yum install git</span><br><span class="line"><span class="comment"># 你要喜欢hub也可以用hub ←_←</span></span><br></pre></td></tr></table></figure><ul><li>设置Git的user name和email:</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面的引号里面的东西替换成你的名字和邮箱</span></span><br><span class="line">$ git config --global user.name <span class="string">"username"</span></span><br><span class="line">$ git config --global user.email <span class="string">"youmail@mail.com"</span></span><br></pre></td></tr></table></figure><ul><li>之后设置SSH密钥:</li></ul><p>如果你之前有了ssh密钥: <code>cat ~/.ssh/id_rsa.pub</code></p><p>没有的话生成密钥: <code>ssh-keygen -t rsa -C &quot;youmail@mail.com&quot;</code></p><p>要不怕以后每次git什么的都要密码的话就直接留空,敲三次回车。</p><p>之后一样<code>cat ~/.ssh/id_rsa.pub</code>查看密钥内容:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-rsa ....这里是一大串字母什么的.... youmail@mail.com</span><br></pre></td></tr></table></figure><p>注：这git什么的你本地配置hexo的时候也需要在本地创建,你看的配置hexo的教程应该都会提到这个。这里不多说。</p><ul><li>然后复制这些东西加入到你的github/coding.net的设置里面的SSH keys中,就是找到ssh keys的设置中,点新建密钥(new keys什么的)之后把刚刚cat到的复制到里面,随便起一个名字。</li></ul><p>然后测试一下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br><span class="line"></span><br><span class="line">如果没问题会显示如下信息:</span><br><span class="line">Hi xxx! You<span class="string">'ve successfully authenticated, but GitHub does not provide shell access.</span></span><br></pre></td></tr></table></figure><p>这时候git就配置好了。</p><p>然后找个目录我就以个人目录为例吧</p><blockquote><p>之后的操作可能需要权限,之后的操作为了方便我就不写sudo了,如果出现Permission denied什么的就自己加权限。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line">mkdir blog</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后把这个目录权限提高,这个目录将作为我的网站目录地址</span></span><br><span class="line">chmod -R 755 ~/blog</span><br></pre></td></tr></table></figure><h3 id="nignx的配置"><a href="#nignx的配置" class="headerlink" title="nignx的配置"></a>nignx的配置</h3><p>下载nginx什么的就不说了,而且一般云服务器都会自带有nignx</p><p>我以我的Ubuntu服务器为例,自带的是在<code>/usr/local/nginx/</code>中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> conf</span><br><span class="line"><span class="comment"># 配置文件是nginx.conf先看一下,回车往下翻,q退出</span></span><br><span class="line">more nginx.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把第一行的user改为你想要设置的用户,这里我用ubuntu(不推荐,图方便点),一般的是给nginx</span></span><br><span class="line"><span class="comment"># 如果你设置用户为nginx,把nginx程序使用的用户(nginx)给予目录权限,这里目录就是我刚刚mkdir的目录:</span></span><br><span class="line">chown -R nginx ~/blog</span><br><span class="line"></span><br><span class="line"><span class="comment"># 之后就是配置nginx.conf</span></span><br><span class="line">sudo vim nginx.conf</span><br><span class="line"><span class="comment"># 第一行不说了</span></span><br><span class="line"><span class="comment"># 然后在http&#123;&#125;中填入类似于:</span></span><br><span class="line">server&#123;</span><br><span class="line">              listen        80;</span><br><span class="line">              server_name   www.xxx.com;</span><br><span class="line">              root          /home/ubuntu/blog;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"><span class="comment"># root中填入的是你刚刚创建的博客目录的地址也就是`cd ~/blog` 然后`pwd`的值</span></span><br><span class="line"><span class="comment"># 我试了一下用~/blog不对。。具体我也不清楚,nginx没怎么接触太多</span></span><br><span class="line"><span class="comment"># server_name 填写的是你的域名,要没有的话,就填服务器ip地址。。</span></span><br><span class="line"><span class="comment"># 如果是域名的话看下面</span></span><br></pre></td></tr></table></figure><ul><li>域名解析,刚刚填写的server_name,你要到你自己域名的解析中添加。</li></ul><p>我以腾讯云为例,买了域名之后,控制台-云解析-你自己的域名</p><p>添加记录类型为A的,主机记录随便你,之后记录值就是你服务器的地址。</p><p><img src="http://olq8hji1u.bkt.clouddn.com/2017051214945922734243.png" alt="2017051214945922734243.png"></p><p>然后刚刚的server_name就填你刚刚设置的<code>主机记录.域名.com</code>什么的。这里就不再多说。</p><p>然后回到刚刚的配置问题,编辑好之后按<code>esc</code>,输如<code>:wq</code>保存退出。</p><p>然后</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 载入nginx.conf配置,重新启动nginx</span></span><br><span class="line">/usr/<span class="built_in">local</span>/nginx/sbin/nginx -s reload /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关于nginx更多的可以自行查阅资料</span></span><br></pre></td></tr></table></figure><h3 id="然后就是git-clone一些源代码"><a href="#然后就是git-clone一些源代码" class="headerlink" title="然后就是git clone一些源代码"></a>然后就是git clone一些源代码</h3><p><code>回到本地</code></p><p>在你创建好的博客中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>这时候你的博客目录中的public应该会被发送到github/coding.net的仓库中。</p><p>然后再在<code>服务器</code>中,克隆你自己的仓库,推荐coding国内比较快…</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/blog</span><br><span class="line"><span class="comment"># coding</span></span><br><span class="line">git <span class="built_in">clone</span> git@git.coding.net:xxx/blog.git</span><br><span class="line"><span class="comment"># github</span></span><br><span class="line">git <span class="built_in">clone</span> git@github.com:xxx/blog.git</span><br></pre></td></tr></table></figure><p>之后你访问自己的域名或者服务器地址,应该可以看见你的博客了!</p><p>然后你以后发文章更新博客很简单了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本地</span></span><br><span class="line">hexo n <span class="string">"新文章"</span></span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 服务器</span></span><br><span class="line"><span class="built_in">cd</span> ~/blog</span><br><span class="line">git pull</span><br></pre></td></tr></table></figure><p>之后刷新网页,every thing is OK~</p><p>你应该能感受到比Github Pages快很多。</p><p>ps: 此教程纯手打,转载请注明原文来自<a href="http://heywe.cn/2017/05/12/hexo/">http://heywe.cn</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Nginx </tag>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记(五)</title>
      <link href="/2017/04/21/tensorflow5/"/>
      <url>/2017/04/21/tensorflow5/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>TensorFlow实现卷积神经网络</b></font></center><a id="more"></a><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>卷积神经网络(Convolutional Neural Network,CNN)当初是为了解决图像识别等问题设计的，但是现在的应用不止在图像方面，也可用于视频、音频、文本等。</p><p>下图是一个简单的卷积神经网络的示意图</p><p><img src="http://ww4.sinaimg.cn/large/006tNbRwgy1femgby82kkj30y20jm40j.jpg" alt></p><p>一般的卷积神经网络由多个卷积层构成，每个卷积层通常会解析如下几个操作。</p><ol><li>图像通过多个不同的卷积核的滤波，并加偏置，提取出局部特征，每个卷积核会映射出一个新的2D图像</li><li>将前面卷积核的滤波输出结果，解析非线性的激活函数处理。目前最常见的是使用ReLU。</li><li>对激活函数的结果在进行池化操作，，目前一般使用的是最大池化，保留最显著的特征，并提升模型的鲁棒性</li></ol><p>这几个步骤就构成了最常见的卷积层，目前比较通用的还会加入一层<code>批量归一化</code>(<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization</a>)等。在实践中，使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。</p><p>总的来说，卷积神经网络的要点就是局部连接(local connection)、权值共享(weights sharing)、池化层(pooling)的降采样(down-sampling)，其中局部连接和权值共享降低了参数量，使训练复杂度大大下降，减轻了过拟合。权值共享赋予了卷积神经网络的平移的容忍性，提高了模型的泛化能力。</p><h1 id="TensorFlow实现一个简单的卷积神经网络"><a href="#TensorFlow实现一个简单的卷积神经网络" class="headerlink" title="TensorFlow实现一个简单的卷积神经网络"></a>TensorFlow实现一个简单的卷积神经网络</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="comment"># 这里仍然根据路径选择MNIST数据集</span></span><br><span class="line">mnist = input_data.read_data_sets(</span><br><span class="line">    <span class="string">"/Users/wuyong/Desktop/网课的作业笔记/pythondemo/TensorFlowdemo/mnist/data"</span>,</span><br><span class="line">    one_hot=<span class="literal">True</span>)</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个初始化函数方便重复使用。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要个权重制造一些随机的噪声来打破完全对称，比如截断的正态分布噪声</span></span><br><span class="line"><span class="comment"># 标准差为0.1，同时使用ReLU，也给偏置增加一些小的正直来避免死亡节点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积层、池化层都是重复使用的，这里的tf.nn.conv2d是TensorFlow中的</span></span><br><span class="line"><span class="comment"># 二维卷积函数，参数中x是输入，W是卷积的参数，比如[5,5,1,32]前面的</span></span><br><span class="line"><span class="comment"># 5，5表示卷积核的尺寸，第三个数字代表有多少个channel。</span></span><br><span class="line"><span class="comment"># 因为MNIST只有灰度单色，所以为1，如果是彩色RGB图像，则这里应该是3</span></span><br><span class="line"><span class="comment"># 最后一个表示卷积核的数量，也就是这个卷积层会提取多少类的特征</span></span><br><span class="line"><span class="comment"># strides表示卷积模板移动的步长，都是1表示会不遗漏地划过图片的每一个点。</span></span><br><span class="line"><span class="comment"># padding表示便捷的处理方式，这里SAME表示给边界加速padding让卷积的输入和输出保持同样的尺寸</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.nn.max_pool是TensorFlow中的最大护花函数，这里使用2x2的最大池化，即将一个2x2的</span></span><br><span class="line"><span class="comment"># 像素快降维1x1的像素，最大池化会保留原始像素块中灰度值最高的哪一个像素，即保留最明显的特征</span></span><br><span class="line"><span class="comment"># 因为希望整体上缩小尺寸，因此池化层的strides也设为横竖两个方向以2位步长。如果步长还是1，</span></span><br><span class="line"><span class="comment"># 那么我们会得到一个尺寸不变的图片</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(</span><br><span class="line">        x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入的placeholder，x是特征，y_是真实的labels</span></span><br><span class="line"><span class="comment"># 因为卷积神经网络会利用到空间结构信息，所以需要将1D的输入向量转为2D的图片结构</span></span><br><span class="line"><span class="comment"># 即从1*784的形式转为原始的28*28的结构，同事因为只有一颜色通道，故最终尺寸为[-1,28,28,1]</span></span><br><span class="line"><span class="comment"># -1表示样本数量不固定，最后的1表示颜色通道数。</span></span><br><span class="line"><span class="comment"># 这里使用的tensor变形函数是tf.reshape</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义第一个卷积层，使用前面写好的函数进行参数初始化。包括weights和bias</span></span><br><span class="line"><span class="comment"># 这里的[5,5,1,32]代表卷积核尺寸为5*5,一个颜色通道，32个不同的卷积核</span></span><br><span class="line"><span class="comment"># 然后使用conv2d函数进行卷积操作，并加上偏置，接着在使用ReLU激活函数进行非线性处理</span></span><br><span class="line"><span class="comment"># 最后，使用醉倒池化函数max_pool_2x2对卷积的输出结果进行池化操作</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义第二个卷积层，不同的是卷积核的数量变成了64，也就是这一层的卷积会提取64种特征</span></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图片的尺寸变化了，第二个卷积核数量为64，输出的tensor尺寸即为7*7*64</span></span><br><span class="line"><span class="comment"># 使用tf.reshape对第二个卷积层的输出tensor进行变形，将其转成1D的向量</span></span><br><span class="line"><span class="comment"># 连接一个全连接层，隐含节点为1024，并使用ReLU</span></span><br><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 减轻过拟合，使用dropout层 通过一个placeholder传入keep_prob来控制。</span></span><br><span class="line"><span class="comment"># 训练时则保留全部数据来追求最好的预测性能</span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后将dropout层的输出连接一个softmax层，得到最后的概率输出</span></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们定义损失函数cross_entropy，优化器使用adam 学习速率1e4</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(</span><br><span class="line">    y_ * tf.log(y_conv), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义准确率</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练过程，初始化所有参数，设置训练时dropout的keep_prob比率为0.5</span></span><br><span class="line"><span class="comment"># 然后使用大小为50的mini-batch，共进行20000次训练迭代，参与训练的样本数量总共为100万</span></span><br><span class="line"><span class="comment"># 其中每100次训练，我们会对准确率进行一次评测</span></span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        train_accuracy = accuracy.eval(</span><br><span class="line">            feed_dict=&#123;x: batch[<span class="number">0</span>],</span><br><span class="line">                       y_: batch[<span class="number">1</span>],</span><br><span class="line">                       keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">        print(<span class="string">"step %d, training accuracy %g"</span> % (i, train_accuracy))</span><br><span class="line">    train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全部训练完，在最终的测试集上进行全面的测试，得到整体的分类准确率</span></span><br><span class="line">print(<span class="string">"test accuracy %g"</span> % accuracy.eval(</span><br><span class="line">    feed_dict=&#123;x: mnist.test.images,</span><br><span class="line">               y_: mnist.test.labels,</span><br><span class="line">               keep_prob: <span class="number">1.0</span>&#125;))</span><br><span class="line"></span><br><span class="line"><span class="string">'''输出：</span></span><br><span class="line"><span class="string">Extracting data/train-images-idx3-ubyte.gz</span></span><br><span class="line"><span class="string">Extracting data/train-labels-idx1-ubyte.gz</span></span><br><span class="line"><span class="string">Extracting data/t10k-images-idx3-ubyte.gz</span></span><br><span class="line"><span class="string">Extracting data/t10k-labels-idx1-ubyte.gz</span></span><br><span class="line"><span class="string">step 0, training accuracy 0.1</span></span><br><span class="line"><span class="string">step 100, training accuracy 0.88</span></span><br><span class="line"><span class="string">step 200, training accuracy 0.94</span></span><br><span class="line"><span class="string">step 300, training accuracy 0.88</span></span><br><span class="line"><span class="string">step 400, training accuracy 0.98</span></span><br><span class="line"><span class="string">step 500, training accuracy 0.92</span></span><br><span class="line"><span class="string">step 600, training accuracy 1</span></span><br><span class="line"><span class="string">step 700, training accuracy 1</span></span><br><span class="line"><span class="string">step 800, training accuracy 0.9</span></span><br><span class="line"><span class="string">step 900, training accuracy 1</span></span><br><span class="line"><span class="string">step 1000, training accuracy 0.96</span></span><br><span class="line"><span class="string">step 1100, training accuracy 0.98</span></span><br><span class="line"><span class="string">step 1200, training accuracy 1</span></span><br><span class="line"><span class="string">step 1300, training accuracy 0.94</span></span><br><span class="line"><span class="string">step 1400, training accuracy 0.98</span></span><br><span class="line"><span class="string">step 1500, training accuracy 0.94</span></span><br><span class="line"><span class="string">step 1600, training accuracy 0.98</span></span><br><span class="line"><span class="string">step 1700, training accuracy 1</span></span><br><span class="line"><span class="string">step 1800, training accuracy 0.96</span></span><br><span class="line"><span class="string">step 1900, training accuracy 1</span></span><br><span class="line"><span class="string">step 2000, training accuracy 0.98</span></span><br><span class="line"><span class="string">。。。。。中间太多省略了</span></span><br><span class="line"><span class="string">step 19500, training accuracy 1</span></span><br><span class="line"><span class="string">step 19600, training accuracy 1</span></span><br><span class="line"><span class="string">step 19700, training accuracy 1</span></span><br><span class="line"><span class="string">step 19800, training accuracy 1</span></span><br><span class="line"><span class="string">step 19900, training accuracy 1</span></span><br><span class="line"><span class="string">可以看到最后的结果基本上都为1。。。并不是完全正确</span></span><br><span class="line"><span class="string">这是最后的结果在99.5%以上</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><h1 id="实现进阶的卷积网络"><a href="#实现进阶的卷积网络" class="headerlink" title="实现进阶的卷积网络"></a>实现进阶的卷积网络</h1><p>MNIST玩够了接下来玩点高级的</p><p>这里使用数据集为cifar-10</p><p>先下载TensorFlow的models库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个地址访问github下载会比较慢，我把移到了coding.net 这个比较快一点</span></span><br><span class="line">git <span class="built_in">clone</span> https://git.coding.net/Acks/tensorflow_Models.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后</span></span><br><span class="line"><span class="built_in">cd</span> models/tutorials/image/cifar10</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入库</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> cifar10,cifar10_input</span><br></pre></td></tr></table></figure><ul><li>下载cifar10数据集</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cifar10.maybe_download_and_extract()</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集比较大163MB，耐心等待。。</span></span><br><span class="line"><span class="string">下载完成</span></span><br><span class="line"><span class="string">&gt;&gt; Downloading cifar-10-binary.tar.gz 100.0%</span></span><br><span class="line"><span class="string">Successfully downloaded cifar-10-binary.tar.gz 170052171 bytes.</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><p>下载完数据集之后就可以实现进阶的卷积神经网络了！</p><p>这里我简单解释一下思路</p><p>先载入数据，并且对数据进行数据增强，然后构建一个卷积神经网络，再测试</p><p>网络的结构为：<code>conv1-&gt;pool1-&gt;norm1-&gt;conv2-&gt;norm2-&gt;pool2-&gt;fc3-&gt;fc4-&gt;logits</code></p><p>下面是完整代码及说明：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># import sys</span></span><br><span class="line"><span class="comment"># 这里是我下载的models路径 根据自己的路径更改</span></span><br><span class="line"><span class="comment"># sys.path.append('/root/tensorflowdemo/cifar10/data/tensorflow_Models/tutorials/image/cifar10')</span></span><br><span class="line"><span class="keyword">import</span> cifar10, cifar10_input</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义batch_size,训练轮数max_steps</span></span><br><span class="line">max_steps = <span class="number">1000</span>  <span class="comment"># 看自己机器的能力了，1000我服务器都跑了很久</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">data_dir = <span class="string">'cifar_dataset/cifar10_data/cifar-10-batches-bin'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义初始化weight函数 使用tf.truncated_normal截断的正态分布来初始化权重</span></span><br><span class="line"><span class="comment"># 给weight加一个L2的loss 相当于做了一个L2正则化处理</span></span><br><span class="line"><span class="comment"># L1正则会制造稀疏的特征。大部分无用特征的权重会被置为0</span></span><br><span class="line"><span class="comment"># L2正则会让特征的权重不过大，使得特征的权重比较平均</span></span><br><span class="line"><span class="comment"># 使用w1控制L2 loss 的大小，使用tf.nn.l2_loss计算weight的L2 loss</span></span><br><span class="line"><span class="comment"># 再用tf.multiply让L2 loss乘以w1，得到最后的weight loss</span></span><br><span class="line"><span class="comment"># 使用td.add_to_collection吧weight loss统一存到一个collection</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_with_weight_loss</span><span class="params">(shape, stddev, w1)</span>:</span></span><br><span class="line">    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))</span><br><span class="line">    <span class="keyword">if</span> w1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        weight_loss = tf.multiply(tf.nn.l2_loss(var), w1, name=<span class="string">'weight_loss'</span>)</span><br><span class="line">        tf.add_to_collection(<span class="string">'losses'</span>, weight_loss)</span><br><span class="line">    <span class="keyword">return</span> var</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># cifar10_input类中distorted_inputs函数产生训练需要使用的数据</span></span><br><span class="line"><span class="comment"># 包括特征以及对应的label，这里返回的是已经封装好的tensor 每次执行会删除一个batch_size的数量的样本</span></span><br><span class="line"><span class="comment"># 这里已经对数据进行了数据增强 具体实现在cifar10_input.distorted_inputs函数中</span></span><br><span class="line"><span class="comment"># 其中增强的操作包括随机的水平翻转(tf.image.random_flip_left_right)，</span></span><br><span class="line"><span class="comment"># 随机剪切一块24*24大小的图片(tf.image.random_crop)</span></span><br><span class="line"><span class="comment"># 设置随机的亮度和对比度(tf.image.random_brightness tf.image.random_contrast)</span></span><br><span class="line"><span class="comment"># 对数据进行标准化(tf.image.per_image_whitening 对数据进行减去均值除以方差保证数据0均值)</span></span><br><span class="line"><span class="comment"># 原来的一张图片可以变为多张图片，相当于扩大样本量</span></span><br><span class="line"><span class="comment"># 这里对数据增强的操作要耗费大量CPU资源，所以distorted_inputs使用了16个独立的线程来加速任务</span></span><br><span class="line"><span class="comment"># 函数内部会产生线程池，在需要使用时会通过TensorFlow queue进行调度</span></span><br><span class="line">images_train, labels_train = cifar10_input.distorted_inputs(</span><br><span class="line">    data_dir=data_dir, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用cifar10_input.inputs函数输出测试数据，这里不需要对图片进行翻转修改亮度、对比度</span></span><br><span class="line"><span class="comment"># 不过要裁剪图片正中间的24*24大小的区块 并对数据标准化操作</span></span><br><span class="line">images_test, labels_test = cifar10_input.inputs(</span><br><span class="line">    eval_data=<span class="literal">True</span>, data_dir=data_dir, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建输入数据的placeholder，包括特征和label</span></span><br><span class="line"><span class="comment"># 这里batch_size之后定义网络结构时候用到了，所以不能像之前一样可以设为None</span></span><br><span class="line"><span class="comment"># 数据图片尺寸是24*24 颜色通道为3</span></span><br><span class="line">image_holder = tf.placeholder(tf.float32, [batch_size, <span class="number">24</span>, <span class="number">24</span>, <span class="number">3</span>])</span><br><span class="line">label_holder = tf.placeholder(tf.int32, [batch_size])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用之前创建的variable_with_weight_loss函数对卷积核的参数进行创建和初始化</span></span><br><span class="line"><span class="comment"># 第一个卷积核大小5*5 3通道 64卷积核 设置初始化函数的标准差为0.05</span></span><br><span class="line"><span class="comment"># 不对第一个卷积的weight进行L2正则，所以w1=0.0</span></span><br><span class="line"><span class="comment"># 然后使用tf.nn.conv2d函数对数据image_holder进行卷积</span></span><br><span class="line"><span class="comment"># 这里步长为1 padding为SMAE bias初始化为0 再加上bias 最后使用一个ReLU函数进行非线性化</span></span><br><span class="line"><span class="comment"># 之后使用一个尺寸为3*3 步长为2*2的最大池化层处理 这里尺寸和步长不一样可以增加数据的丰富性</span></span><br><span class="line"><span class="comment"># 之后使用tf.nn.lrn函数对LRN结果进行处理</span></span><br><span class="line"><span class="comment"># LRN最早是Alex用CNN参加Imagenet比赛的论文，LRN层模仿了生物神经系统的“侧抑制”机制</span></span><br><span class="line"><span class="comment"># 对局部神经元的活动创建竞争环境，使得其中响应比较大的值变得相对强大，并抑制其他反馈较小的神经元</span></span><br><span class="line"><span class="comment"># 增强了模型的泛化能力 在ImageNet的实验表明 使用LRN后CNN在Top1的错误率可以降低1.4%</span></span><br><span class="line"><span class="comment"># LRN层对于ReLU这种没有上限边界的激活函数会比较有用，因为它会从附近的多个卷积核的响应中挑选比较大的反馈</span></span><br><span class="line"><span class="comment"># 但不适合sigmoid这种有固定边界并且能抑制过大值的激活函数</span></span><br><span class="line">weight1 = variable_with_weight_loss(shape=[<span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">64</span>], stddev=<span class="number">5e-2</span>, w1=<span class="number">0.0</span>)</span><br><span class="line">kernel1 = tf.nn.conv2d(image_holder, weight1, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">bias1 = tf.Variable(tf.constant(<span class="number">0.0</span>, shape=[<span class="number">64</span>]))</span><br><span class="line">conv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))</span><br><span class="line">pool1 = tf.nn.max_pool(</span><br><span class="line">    conv1, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">norm1 = tf.nn.lrn(pool1, <span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里先LRN再池化</span></span><br><span class="line">weight2 = variable_with_weight_loss(shape=[<span class="number">5</span>, <span class="number">5</span>, <span class="number">64</span>, <span class="number">64</span>], stddev=<span class="number">5e-2</span>, w1=<span class="number">0.0</span>)</span><br><span class="line">kernel2 = tf.nn.conv2d(norm1, weight2, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">bias2 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">64</span>]))</span><br><span class="line">conv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))</span><br><span class="line">norm2 = tf.nn.lrn(conv2, <span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>)</span><br><span class="line">pool2 = tf.nn.max_pool(</span><br><span class="line">    conv2, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用一个全连接层</span></span><br><span class="line"><span class="comment"># 这里用tf.reshape函数将每一个样本都变为一维向量。我们使用get_shape哈数</span></span><br><span class="line"><span class="comment"># 获取数据全部扁平化之后的长度 接着是哦你variable_with_weight_loss函数对全连接层的weight进行初始化</span></span><br><span class="line"><span class="comment"># 这里隐含节点为384 标准差为0.04 bias值初始化为0.1</span></span><br><span class="line"><span class="comment"># 因为不希望全连接层不过拟合，因此设了一个非零的weight loss值0.4</span></span><br><span class="line"><span class="comment"># 让这一层的所有参数都被L2正则所约束 最后使用ReLU进行非线性化</span></span><br><span class="line">reshape = tf.reshape(pool2, [batch_size, <span class="number">-1</span>])</span><br><span class="line">dim = reshape.get_shape()[<span class="number">1</span>].value</span><br><span class="line">weight3 = variable_with_weight_loss(shape=[dim, <span class="number">384</span>], stddev=<span class="number">0.04</span>, w1=<span class="number">0.004</span>)</span><br><span class="line">bias3 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">384</span>]))</span><br><span class="line">local3 = tf.nn.relu(tf.matmul(reshape, weight3) + bias3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全连接层</span></span><br><span class="line"><span class="comment"># 隐含节点数量下降一半 其他超参数不变</span></span><br><span class="line">weight4 = variable_with_weight_loss(shape=[<span class="number">384</span>, <span class="number">192</span>], stddev=<span class="number">0.04</span>, w1=<span class="number">0.004</span>)</span><br><span class="line">bias4 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">192</span>]))</span><br><span class="line">local4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后一层</span></span><br><span class="line"><span class="comment"># 先创建这一层的weight 其正态分布标准差为上一个隐含层节点数的倒数</span></span><br><span class="line"><span class="comment"># 并且不计入L2正则 这里不像之前使用softmax输出最后的结果 因为我们把softmax的操作放在了计算loss的部分</span></span><br><span class="line"><span class="comment"># 我们不需要对inference的输出进行softmax处理就可以获得最后的分类结果</span></span><br><span class="line"><span class="comment"># 计算softmax主要是为了计算loss 因此softmax操作整合到后面是比较合适的</span></span><br><span class="line">weight5 = variable_with_weight_loss(shape=[<span class="number">192</span>, <span class="number">10</span>], stddev=<span class="number">1</span> / <span class="number">192</span>, w1=<span class="number">0.0</span>)</span><br><span class="line">bias5 = tf.Variable(tf.constant(<span class="number">0.0</span>, shape=[<span class="number">10</span>]))</span><br><span class="line">logits = tf.add(tf.matmul(local4, weight5), bias5)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建CNN的loss 这里把softmax的计算和cross entropy loss 计算合在了一起</span></span><br><span class="line"><span class="comment"># 即tf.nn.sparse_softmax_cross_entropy_with_logits</span></span><br><span class="line"><span class="comment"># 这里使用tf.reduce_mean对cross entropy计算均值 再使用tf.add_to_collection</span></span><br><span class="line"><span class="comment"># 把cross entropy的loss添加到整体losses的collection</span></span><br><span class="line"><span class="comment"># 最后使用tf.add_n将整体的losses的collection求和 得到最终的loss</span></span><br><span class="line"><span class="comment"># 包括cross entropy loss 还有后两个圈连接层中weight的L2 loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(logits, labels)</span>:</span></span><br><span class="line">    labels = tf.cast(labels, tf.int64)</span><br><span class="line">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        logits=logits, labels=labels, name=<span class="string">'cross_entropy_per_example'</span>)</span><br><span class="line">    cross_entropy_mean = tf.reduce_mean(cross_entropy, name=<span class="string">'cross_entropy'</span>)</span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, cross_entropy_mean)</span><br><span class="line">    <span class="keyword">return</span> tf.add_n(tf.get_collection(<span class="string">'losses'</span>), name=<span class="string">'total_loss'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将logits节点和label_placeholder传入loss函数获得最终的loss</span></span><br><span class="line">loss = loss(logits, label_holder)</span><br><span class="line">train_op = tf.train.AdamOptimizer(<span class="number">1e-3</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用top_k_op函数求输出结果中top k的准确率 默认Top1 也就是输出分数最高的那一类准确率</span></span><br><span class="line">top_k_op = tf.nn.in_top_k(logits, label_holder, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化全部模型参数</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动前面提到的图片数据增强的线程队列 这里一共使用了16个线程来进行加速</span></span><br><span class="line"><span class="comment"># 注意，如果这里不启动线程 那么后续的inference及训练的操作都是无法开始的</span></span><br><span class="line">tf.train.start_queue_runners()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正式开始训练</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(max_steps):</span><br><span class="line">    <span class="comment"># 记录开始的时间 记录每一个step花费的时间</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    iamge_batch, label_batch = sess.run([images_train, labels_train])</span><br><span class="line">    _, loss_value = sess.run(</span><br><span class="line">        [train_op, loss],</span><br><span class="line">        feed_dict=&#123;image_holder: iamge_batch,</span><br><span class="line">                   label_holder: label_batch&#125;)</span><br><span class="line">    duration = time.time() - start_time</span><br><span class="line">    <span class="comment"># 每隔10个step会计算展示当前的loss 每秒训练的样本数量 以及一个batch花费的时间</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        examples_per_sec = batch_size / duration</span><br><span class="line">        sec_per_batch = float(duration)</span><br><span class="line"></span><br><span class="line">        format_str = (<span class="string">'step %d,loss=%.2f (%.1f examples/sec; %.3f sec/batch)'</span>)</span><br><span class="line">        print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">num_examples = <span class="number">10000</span>  <span class="comment"># 10000个测试样本</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">num_iter = int(math.ceil(num_examples / batch_size))</span><br><span class="line">true_count = <span class="number">0</span></span><br><span class="line">total_sample_count = num_iter * batch_size</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> step &lt; num_iter:</span><br><span class="line">    iamge_batch, label_batch = sess.run([images_test, labels_test])</span><br><span class="line">    <span class="comment"># 计算模型在这个batch的top1 上预测正确的样本数</span></span><br><span class="line">    predictions = sess.run(</span><br><span class="line">        [top_k_op],</span><br><span class="line">        feed_dict=&#123;image_holder: images_batch,</span><br><span class="line">                   label_holder: labels_batch&#125;)</span><br><span class="line">    true_count += np.sum(predictions)</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出准确率</span></span><br><span class="line">precision = true_count / total_sample_count</span><br><span class="line">print(<span class="string">'precision @ 1 = %.3f % precision'</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">这里不想浪费本机资源</span></span><br><span class="line"><span class="string">我在没有GPU的服务器(阿里云学生-。-)上面跑的 </span></span><br><span class="line"><span class="string">速度比较慢</span></span><br><span class="line"><span class="string">Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.</span></span><br><span class="line"><span class="string">step 0,loss=4.68 (9.4 examples/sec; 13.604 sec/batch)</span></span><br><span class="line"><span class="string">step 10,loss=3.75 (81.8 examples/sec; 1.565 sec/batch)</span></span><br><span class="line"><span class="string">step 20,loss=3.13 (83.9 examples/sec; 1.527 sec/batch)</span></span><br><span class="line"><span class="string">step 30,loss=2.83 (81.4 examples/sec; 1.573 sec/batch)</span></span><br><span class="line"><span class="string">step 40,loss=2.63 (81.4 examples/sec; 1.573 sec/batch)</span></span><br><span class="line"><span class="string">step 50,loss=2.48 (81.2 examples/sec; 1.577 sec/batch)</span></span><br><span class="line"><span class="string">step 60,loss=2.17 (81.4 examples/sec; 1.573 sec/batch)</span></span><br><span class="line"><span class="string">step 70,loss=2.22 (81.5 examples/sec; 1.570 sec/batch)</span></span><br><span class="line"><span class="string">step 80,loss=2.00 (82.8 examples/sec; 1.545 sec/batch)</span></span><br><span class="line"><span class="string">step 90,loss=2.03 (83.1 examples/sec; 1.541 sec/batch)</span></span><br><span class="line"><span class="string">中间太多省略 在3000次左右loss在1左右</span></span><br><span class="line"><span class="string">step 2900,loss=1.22 (69.8 examples/sec; 1.835 sec/batch)</span></span><br><span class="line"><span class="string">step 2910,loss=1.07 (69.6 examples/sec; 1.838 sec/batch)</span></span><br><span class="line"><span class="string">step 2920,loss=1.06 (70.2 examples/sec; 1.824 sec/batch)</span></span><br><span class="line"><span class="string">step 2930,loss=1.23 (69.3 examples/sec; 1.847 sec/batch)</span></span><br><span class="line"><span class="string">step 2940,loss=0.97 (68.7 examples/sec; 1.863 sec/batch)</span></span><br><span class="line"><span class="string">step 2950,loss=0.99 (69.4 examples/sec; 1.845 sec/batch)</span></span><br><span class="line"><span class="string">step 2960,loss=1.11 (69.7 examples/sec; 1.836 sec/batch)</span></span><br><span class="line"><span class="string">step 2970,loss=1.04 (67.8 examples/sec; 1.887 sec/batch)</span></span><br><span class="line"><span class="string">step 2980,loss=1.15 (69.7 examples/sec; 1.837 sec/batch)</span></span><br><span class="line"><span class="string">step 2990,loss=0.82 (69.2 examples/sec; 1.849 sec/batch)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">后面测试集的结果我用的是跑1000代的模型 - -！</span></span><br><span class="line"><span class="string">不小心把后面image拼错了3000代的结果没保存 心态崩了</span></span><br><span class="line"><span class="string">后面会说到保存模型和Tensor Board。。。。</span></span><br><span class="line"><span class="string">所以就跑了1000重新来看测试的结果的</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> Tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Python </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记(四)</title>
      <link href="/2017/04/08/tensorflow4/"/>
      <url>/2017/04/08/tensorflow4/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>自编码器及多层感知器</b></font></center><a id="more"></a><h1 id="MNIST介绍"><a href="#MNIST介绍" class="headerlink" title="MNIST介绍"></a>MNIST介绍</h1><ul><li><p>MNIST是在机器学习领域中的一个经典问题。该问题解决的是把28x28像素的灰度手写数字图片识别为相应的数字，其中数字的范围从0到9.</p></li><li><p>首先需要下载数据集 <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">LeCun给出了下载地址</a></p></li><li><p>这里有非常详细的MNIST介绍以及可视化的图片演示<a href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/" target="_blank" rel="noopener">Chris Olah’s visualizations of MNIST.</a></p></li></ul><table><thead><tr><th>文件</th><th>内容</th></tr></thead><tbody><tr><td>train-images-idx3-ubyte.gz</td><td>training set images (9912422 bytes)</td></tr><tr><td>train-labels-idx1-ubyte.gz</td><td>training set labels (28881 bytes)</td></tr><tr><td>t10k-images-idx3-ubyte.gz</td><td>test set images (1648877 bytes)</td></tr><tr><td>t10k-labels-idx1-ubyte.gz</td><td>test set labels (4542 bytes)</td></tr></tbody></table><p>TensorFlow直接提供了MNIST的下载</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'data/'</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">文件已存在会这样显示,不存在时会有一行是显示文件大小的</span></span><br><span class="line"><span class="string">Extracting data/train-images-idx3-ubyte.gz</span></span><br><span class="line"><span class="string">Extracting data/train-labels-idx1-ubyte.gz</span></span><br><span class="line"><span class="string">Extracting data/t10k-images-idx3-ubyte.gz</span></span><br><span class="line"><span class="string">Extracting data/t10k-labels-idx1-ubyte.gz</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><h2 id="先查看MNIST数据集的情况"><a href="#先查看MNIST数据集的情况" class="headerlink" title="先查看MNIST数据集的情况"></a>先查看MNIST数据集的情况</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(mnist.train.images.shape,mnist.train.labels.shape) <span class="comment"># 训练集</span></span><br><span class="line">print(mnist.test.images.shape,mnist.test.labels.shape) <span class="comment"># 测试集</span></span><br><span class="line">print(mnist.validation.images.shape,mnist.validation.labels.shape) <span class="comment"># 验证数据集</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">(55000, 784) (55000, 10)</span></span><br><span class="line"><span class="string">(10000, 784) (10000, 10)</span></span><br><span class="line"><span class="string">(5000, 784) (5000, 10)</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><ul><li>使用Softmax Regression实现一个简单的机器学习</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'mnist/data/'</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(mnist.train.images.shape, mnist.train.labels.shape)</span><br><span class="line">print(mnist.test.images.shape, mnist.test.labels.shape)</span><br><span class="line">print(mnist.validation.images.shape, mnist.validation.labels.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession() <span class="comment"># 注册为默认session</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>]) <span class="comment"># None表示不限制条数的输入</span></span><br><span class="line"></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">y = tf.nn.softmax(tf.matmul(x, W) + b) <span class="comment"># softmax regression算法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cross—entropy  </span></span><br><span class="line">y_ = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y),reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># SGD</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化所有变量</span></span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每次随机从训练集选择100个进行训练构成mini-batch</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    batch_xs,batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    train_step.run(&#123;x:batch_xs,y_:batch_ys&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.argmax是从一个tensor中找到最大的序号,tf.argmax(y,1)求各个预测的数字中概率最大的一个</span></span><br><span class="line"><span class="comment"># tf.argmax(y_,1)找样本的真实数字类别,tf.equal表示是否相等</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>),tf.argmax(y_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用tf.cast将之前的correct_prediction输出的bool值转换为float32;之后再求平均</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出准确率</span></span><br><span class="line">print(accuracy.eval(&#123;x:mnist.test.images,y_:mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">输出(每次可能不一样)</span></span><br><span class="line"><span class="string">0.9194</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><ul><li><p>这个可以看成是一个没有隐藏层的最浅的神经网络,一共有4步:</p><ol><li>定义算法公式,也就是神经网络forward时的计算</li><li>定义loss,选定优化器,并制定优化器优化loss</li><li>迭代地对数据进行训练</li><li>在测试集或是验证集上对准确率进行评测</li></ol></li></ul><h1 id="TensorFlow实现自编码器"><a href="#TensorFlow实现自编码器" class="headerlink" title="TensorFlow实现自编码器"></a>TensorFlow实现自编码器</h1><ul><li><p>自编码器</p><p>  传统的机器学习的任务很大程度上依赖于特征工程。但是特征工程往往非常耗时耗力,而且在图像、视频、语音中提取到有效特征更难。而深度学习可以大大缓解机器学习模型对特征工程的依赖。深度学习在早期一度被认为是一种无监督的特征学习,模仿了人脑的对特征逐层抽象提取的过程。</p><p>  早年有学者研究稀疏编码时,收集了大量的黑白风景照片,并从中提取了许多16*16的图像碎片。他们发现机会所有的图像碎片都可以由64种正交的边组合得到。还有学者同事发现声音也存在这种情况。这就是特征的稀疏表达,使用少量的基本特征组合拼装得到更高层抽象的特征。通常我们需要多层的神经网络,对于每一层神经网络来说,前一层的输出都是为加工的像素,而这一层则是对像素进行加工组织成更高阶的特征。</p><p>  <img src="https://ww4.sinaimg.cn/large/006tNc79gy1fef3h7g0l9j30kn0d0gn8.jpg" alt></p><p>  自编码器,即可以使用自身的高阶特征编码自己。自编码器其实也是一种神经网络,它的输入和输入是一致的,它借助稀疏编码的思想,目标是使用稀疏的一些高阶特征重构自己。因此,它的特点非常明显:第一,期望输入/输出一致;第二,希望使用高阶特征来重构自己,而不只是复制像素点。</p><p>  Hinton 在<code>Reducing the dimensionality of data with neural networks</code> <a href="http://www.utstat.toronto.edu/~rsalakhu/papers/science.pdf" target="_blank" rel="noopener">论文</a>中讲解了使用自编码器对数据进行降维的方法。Hinton还提出了基于深度信念网络(Deep Belief Networks,DBN)可使用无监督的逐层训练的贪心算法,为训练很深的网络提供了一个可行的方案:很难直接训练极深的网络,但是可以用无监督的逐层训练提取特征,将网络的权重初始化到一个比较好的位置,辅助后面的监督训练。这个思想是相当于学习一个恒等式y=x,自编码器的输入节点和输出节点的数量是一致的,但是如果只是单纯地逐个重复输入节点则没有意义,所以希望使用少量稀疏的高阶特征来重构输入,所以可以加入几种限制。</p><p>  (1) 如果限制中间隐含层节点的数量,比如让中间隐含层节点的数量小与输入/输出节点的数量,就相当于一个降维的过程。此时如果再给中间隐含层的权重加一个L1正则,则可以通过惩罚系数控制隐含节点的稀疏程度,惩罚系数越大,学到的特征组合越稀疏。至于为什么L1正则化可以增加稀疏性,请看<a href="http://freemind.pluskid.org/machine-learning/sparsity-and-some-basics-of-l1-regularization/" target="_blank" rel="noopener">某大神博客</a>有数学推导。</p><p>  (2) 如果给数据加入噪声,那么就构成了去噪自编码器,我们将从噪声中学习出数据的特征。同样,我们也不可能完全复制节点,完全复制并不能去除我们添加的噪声,无法完全复原数据。所以唯有学习数据频繁出现的模式和结构,将无规律的噪声略去,才可以复原数据。去噪自编码器最常用的噪声是加性高斯噪声。</p></li></ul><ul><li><p>TensorFlow实现去噪自编码器</p><blockquote><p>刚刚用Linux配置sklearn简直要疯。顺带提一下坑的地方,import sklearn一直提示没找到 _bz2 ,查了好久是要先安装bz2的模块<code>yum install bzip2-devel</code><br>  然后重新编译python3.6:<code>./configure</code> <code>make all</code> <code>make install</code> <code>make clean</code> <code>make distclean</code></p></blockquote></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn.preprocessing <span class="keyword">as</span> prep</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br></pre></td></tr></table></figure><p>xavier initialization 是一种参数初始化方法。<br>Xavier在caffe早期版本频繁使用 它的特点是根据某一层网络的输入输出节点数量自动的调整最合适的分布。<br>Yoshua Bengio 在一篇论文中指出,如果学习模型的权重初始化得太小,<br>那么信号将在每层间传递时逐渐缩小而难以产生作用,如果初始化的太大,那信号将在每层间的传递时逐渐放大而导致发散和失效。</p><p>而Xavier初始化就是让初始化得到不大不小的值。<br>从数学上就是使权重0均值,方差为$2 / (n_{in}+n_{out})$,分布可以用均匀分布或者高斯分布。<br>这里创建了一个$(- \sqrt{6 \over {n_{in}+n_{out}}},\sqrt{6 \over {n_{in}+n_{out}}})$ 范围内的均匀分布<br>它的方差根据公式$D(X)={(max-min)}^2 / 12$刚好等于$2 / (n_{in}+n_{out})$</p><p>因此这里实现了标准均匀分布的Xavier初始化器,其中fan_in是输入节点的数量,fan_out是输出节点的数量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xavier_init</span><span class="params">(fan_in, fan_out, constant=<span class="number">1</span>)</span>:</span></span><br><span class="line">    low = -constant * np.sqrt(<span class="number">6.0</span> / (fan_in + fan_out))</span><br><span class="line">    high = constant * np.sqrt(<span class="number">6.0</span> / (fan_in + fan_out))</span><br><span class="line">    <span class="keyword">return</span> tf.random_uniform(</span><br><span class="line">        (fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">n_input, 输入变量数</span><br><span class="line">n_hidden, 隐含层节点数</span><br><span class="line">transfer_function=tf.nn.softplus, 隐含层激活函数,默认tf.nn.softplus</span><br><span class="line">optimizer=tf.train.AdamOptimizer(), 优化器,默认为Adam</span><br><span class="line">scale=0.1, 高斯噪声系数,默认为0.1</span><br><span class="line">其中class内的scale参数做成了一个placeholder</span><br><span class="line">参数初始化使用了接下来定义的_initialize_weifghts函数</span><br><span class="line">这里只用了一个隐含层</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdditiveGausssianNoiseAutoencoder</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 n_input,</span></span></span><br><span class="line"><span class="function"><span class="params">                 n_hidden,</span></span></span><br><span class="line"><span class="function"><span class="params">                 transfer_function=tf.nn.softplus,</span></span></span><br><span class="line"><span class="function"><span class="params">                 optimizer=tf.train.AdamOptimizer<span class="params">()</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 scale=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        self.n_input = n_input</span><br><span class="line">        self.n_hidden = n_hidden</span><br><span class="line">        self.transfer = transfer_function</span><br><span class="line">        self.scale = tf.placeholder(tf.float32)</span><br><span class="line">        self.training_scale = scale</span><br><span class="line">        network_weights = self._initialize_weights()</span><br><span class="line">        self.weights = network_weights</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">定义网络结构</span><br><span class="line">输入x创建了一个维度为n_input的placeholder</span><br><span class="line">建立了一个能提取特征的隐含层,先将输入x加入噪声,即self.x + scale * tf.random_normal((n_input, ))</span><br><span class="line">然后用tf.matmul将计入噪声的输入与隐含层的权重w1相乘</span><br><span class="line">并使用tf.add加上隐含层的偏置b1,最后使用self.transfer对结果进行激活函数处理</span><br><span class="line">经过隐含层之后我们需要在输出层进行数据复原、重建操作(即建立reconstruction层)</span><br><span class="line">这里就不需要激活函数了,直接将隐含层的输出self.hidden乘上输出层的权重w2,再加上输出层的权重b2即可</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.x = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_input])</span><br><span class="line">self.hidden = self.transfer(</span><br><span class="line">    tf.add(</span><br><span class="line">        tf.matmul(self.x + scale * tf.random_normal((n_input, )),</span><br><span class="line">                  self.weights[<span class="string">'w1'</span>]), self.weights[<span class="string">'b1'</span>]))</span><br><span class="line">self.reconstruction = tf.add(</span><br><span class="line">    tf.matmul(self.hidden, self.weights[<span class="string">'w2'</span>]), self.weights[<span class="string">'b2'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">定义自编码器的损失函数</span><br><span class="line">这里使用平方误差作为cost,即用tf.subtract计算输出(self.reconstruction)与输入(self.x)之差,</span><br><span class="line">再使用tf.pow求差的平方,最后使用tf.reduce_sum求和即可得到平方误差。</span><br><span class="line">定义训练操作作为优化器self.optimizer对损失self.cost进行优化。</span><br><span class="line">最后创建Session,并初始化自编码器的全部模型参数</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.cost = <span class="number">0.5</span> * tf.reduce_sum(</span><br><span class="line">    tf.pow(tf.subtract(self.reconstruction, self.x), <span class="number">2.0</span>))</span><br><span class="line">self.optimizer = optimizer.minimize(self.cost)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">self.sess = tf.Session()</span><br><span class="line">self.sess.run(init)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">创建初始化函数</span><br><span class="line">先创建一个字典,将w1,b1,w2,b2存入字典中</span><br><span class="line">最后返回</span><br><span class="line">w1要使用之前创建的xavier_init初始化,所以输入 输入节点数和隐含层节点数</span><br><span class="line">而偏置b1只需要使用tf.zeros全部置0即可</span><br><span class="line">对于输出层,没有激活函数直接将w2,b2全初始化为0</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_initialize_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">   all_weights = dict()</span><br><span class="line">   all_weights[<span class="string">'w1'</span>] = tf.Variable(</span><br><span class="line">       xavier_init(self.n_input, self.n_hidden))</span><br><span class="line">   all_weights[<span class="string">'b1'</span>] = tf.Variable(</span><br><span class="line">       tf.zeros(</span><br><span class="line">           [self.n_hidden], dtype=tf.float32))</span><br><span class="line">   all_weights[<span class="string">'w2'</span>] = tf.Variable(</span><br><span class="line">       tf.zeros(</span><br><span class="line">           [self.n_hidden, self.n_input], dtype=tf.float32))</span><br><span class="line">   all_weights[<span class="string">'b2'</span>] = tf.Variable(</span><br><span class="line">       tf.zeros(</span><br><span class="line">           [self.n_input], dtype=tf.float32))</span><br><span class="line">   <span class="keyword">return</span> all_weights</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">定义计算损失及执行一步训练的函数partial_fit</span><br><span class="line">函数里只需让Session执行两个计算图的节点,分别是损失cost和训练过程中的optimizer</span><br><span class="line">输入的feed_dict包括输入数据x,以及噪声的系数scale</span><br><span class="line">函数partial_fit的作用就是用一个batch的数据进行训练并返回当前的损失cost</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partial_fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    cost, opt = self.sess.run(</span><br><span class="line">        (self.cost, self.optimizer),</span><br><span class="line">        feed_dict=&#123;self.x: X,</span><br><span class="line">                   self.scale: self.training_scale&#125;)</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">只求损失cost的函数</span><br><span class="line">就只让Session执行一个计算图节点self.cost</span><br><span class="line">传入的参数和之前的partial_fit一致</span><br><span class="line">这个函数是在自编码器训练完毕后,在测试集上对模型的性能进行评测时会用到的,</span><br><span class="line">它不会像partial_fit那样触发训练操作</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_total_cost</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.sess.run(</span><br><span class="line">        self.cost, feed_dict=&#123;self.x: X,</span><br><span class="line">                              self.scale: self.training_scale&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">定义了transform函数,返回自编码器隐含层的输出结果</span><br><span class="line">提供一个接口来获取抽象后的特征,自编码器的隐含层的最主要的功能就是学习出数据中的高阶特征</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.sess.run(</span><br><span class="line">        self.hidden, feed_dict=&#123;self.x: X,</span><br><span class="line">                                self.scale: self.training_scale&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">再定义generate函数,它将隐含层的输出结果作为输入,通过之后的重建层将提取到的高阶特征复原为原始数据</span><br><span class="line">这个接口和前面的transform正好将整个自编码器拆分为两部分,这里的generate接口是后半部分</span><br><span class="line">将高阶特征复原为原始数据的步骤</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(self, hidden=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> hidden <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        hidden = np.random_normal(size=self.weights[<span class="string">'b1'</span>])</span><br><span class="line">    <span class="keyword">return</span> self.sess.run(self.reconstruction,</span><br><span class="line">                         feed_dict=&#123;self.hidden: hidden&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">定义reconstruct</span><br><span class="line">它整体运行一遍复原过程</span><br><span class="line">包括提取高阶特征和通过高阶特征提取数据,即包括transform和generate两块</span><br><span class="line">输入是原数据、输出是复原后的数据</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reconstruct</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.sess.run(</span><br><span class="line">        self.reconstruction,</span><br><span class="line">        feed_dict=&#123;self.x: X,</span><br><span class="line">                   self.scale: self.training_scale&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">获取隐含层的权重和偏差</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getWeights</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.sess.run(self.weights[<span class="string">'w1'</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getBiases</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.sess.run(self.weights[<span class="string">'b1'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">加载MNIST数据集</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mnist = input_data.read_data_sets(<span class="string">'data/'</span>, one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">对训练测试数据标准化的函数</span><br><span class="line">标准化让数据变成0均值,标准差为1的分布</span><br><span class="line">方法是先减去平均值,再除以标准差</span><br><span class="line">直接使用sklearn.preprossing的StandardScaler类,现在训练集上进行fit再将这个Scaler用到训练集和测试集上</span><br><span class="line">这里必须要保证训练测试数据都使用完全相同的Scaler,这样才能保证后面模型处理数据的一致性,这就是先在训练集上fit出一个公用的Scaler的原因</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standard_scale</span><span class="params">(X_train, X_test)</span>:</span></span><br><span class="line">    preprocessor = prep.StandardScaler().fit(X_train)</span><br><span class="line">    X_train = preprocessor.transform(X_train)</span><br><span class="line">    X_test = preprocessor.transform(X_test)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_test</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">定义一个获取随机block数据的函数:</span><br><span class="line">取一个从0到len(data)-batch_size之间的随机整数 再以这个随机数作为block的起始位置,然后顺序取到一个batch size的数据</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_random_block_from_data</span><span class="params">(data, batch_size)</span>:</span></span><br><span class="line">    start_index = np.random.randint(<span class="number">0</span>, len(data) - batch_size)</span><br><span class="line">    <span class="keyword">return</span> data[start_index:(start_index + batch_size)]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">使用之前定义的standard_scale函数对训练集、测试集进行标准化变换</span><br><span class="line"></span><br><span class="line">定义参数</span><br><span class="line">epoch 最大训练轮数</span><br><span class="line">batch_size </span><br><span class="line">display_step 每隔几轮显示一次cost</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)</span><br><span class="line">n_samples = int(mnist.train.num_examples)</span><br><span class="line">training_epochs = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">display_step = <span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">创建一个AGN自编码器的实例</span><br><span class="line">定义输入节点数n_input为784 隐含层为节点数200</span><br><span class="line">隐含层的激活函数transfer_function为softplus</span><br><span class="line">优化器为Adam,学习速率为0.001</span><br><span class="line">噪声系数scale为0.01</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">autoencoder = AdditiveGausssianNoiseAutoencoder(</span><br><span class="line">    n_input=<span class="number">784</span>,</span><br><span class="line">    n_hidden=<span class="number">200</span>,</span><br><span class="line">    transfer_function=tf.nn.softplus,</span><br><span class="line">    optimizer=tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>),</span><br><span class="line">    scale=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">开始训练过程</span><br><span class="line">在每一轮循环开始时,将平均损失avg_vost设置为0</span><br><span class="line">计算一共需要的batch数,在每一个batch的循环中,partial_fit训练这个batch的数据计算当前的cost</span><br><span class="line">最后将cost整合到avg_cost中。在每一轮迭代后,显示当前迭代数和这一轮的平均cost</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">    avg_cost = <span class="number">0.</span></span><br><span class="line">    total_batch = int(n_samples / batch_size)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">        batch_xs = get_random_block_from_data(X_train, batch_size)</span><br><span class="line">        cost = autoencoder.partial_fit(batch_xs)</span><br><span class="line">        avg_cost += cost / n_samples * batch_size</span><br><span class="line">    <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Epoch:"</span>, <span class="string">"%04d"</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>,</span><br><span class="line">              <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line">    </span><br><span class="line">print(<span class="string">"Total cost: "</span>+ str(autoencoder.calc_total_cost(X_test)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">最终的输出结果 我这里大约需要<span class="number">2</span>分多钟</span><br><span class="line">Epoch: <span class="number">0001</span> cost = <span class="number">19775.792931818</span></span><br><span class="line">Epoch: <span class="number">0002</span> cost = <span class="number">11964.003212500</span></span><br><span class="line">Epoch: <span class="number">0003</span> cost = <span class="number">9655.042776705</span></span><br><span class="line">Epoch: <span class="number">0004</span> cost = <span class="number">9977.502384091</span></span><br><span class="line">Epoch: <span class="number">0005</span> cost = <span class="number">9495.442856250</span></span><br><span class="line">Epoch: <span class="number">0006</span> cost = <span class="number">8875.182430114</span></span><br><span class="line">Epoch: <span class="number">0007</span> cost = <span class="number">9942.972260227</span></span><br><span class="line">Epoch: <span class="number">0008</span> cost = <span class="number">8377.960607386</span></span><br><span class="line">Epoch: <span class="number">0009</span> cost = <span class="number">8419.574784091</span></span><br><span class="line">Epoch: <span class="number">0010</span> cost = <span class="number">7888.692389773</span></span><br><span class="line">Epoch: <span class="number">0011</span> cost = <span class="number">8359.316435227</span></span><br><span class="line">Epoch: <span class="number">0012</span> cost = <span class="number">9017.994824432</span></span><br><span class="line">Epoch: <span class="number">0013</span> cost = <span class="number">8194.735139773</span></span><br><span class="line">Epoch: <span class="number">0014</span> cost = <span class="number">8732.054455114</span></span><br><span class="line">Epoch: <span class="number">0015</span> cost = <span class="number">8273.645841477</span></span><br><span class="line">Epoch: <span class="number">0016</span> cost = <span class="number">8323.325909091</span></span><br><span class="line">Epoch: <span class="number">0017</span> cost = <span class="number">8407.965837500</span></span><br><span class="line">Epoch: <span class="number">0018</span> cost = <span class="number">7300.461956818</span></span><br><span class="line">Epoch: <span class="number">0019</span> cost = <span class="number">7638.011763636</span></span><br><span class="line">Epoch: <span class="number">0020</span> cost = <span class="number">8552.155968750</span></span><br><span class="line">Total cost: <span class="number">650998.0</span></span><br></pre></td></tr></table></figure><h1 id="TensorFlow实现多层感知器"><a href="#TensorFlow实现多层感知器" class="headerlink" title="TensorFlow实现多层感知器"></a>TensorFlow实现多层感知器</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'data/'</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">in_units = <span class="number">784</span>  <span class="comment"># 输入节点</span></span><br><span class="line">h1_units = <span class="number">300</span>  <span class="comment"># 隐含层输出节点</span></span><br><span class="line">W1 = tf.Variable(tf.truncated_normal([in_units, h1_units], stddev=<span class="number">0.1</span>))</span><br><span class="line">W2 = tf.Variable(tf.zeros([h1_units, <span class="number">10</span>]))</span><br><span class="line">b1 = tf.Variable(tf.zeros([h1_units]))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入x的placeholder,dropout的比率为keep_prob(保留节点的概率)</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, in_units])</span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 隐含层的命名为hidden1</span></span><br><span class="line"><span class="comment"># 激活函数为ReLU</span></span><br><span class="line">hidden1 = tf.nn.relu(tf.matmul(x, W1) + b1)</span><br><span class="line">hidden1_drop = tf.nn.dropout(hidden1, keep_prob)</span><br><span class="line">y = tf.nn.softmax(tf.matmul(hidden1_drop, W2) + b2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里使用Adagrad优化,优化算法可以参考下面cs231n</span></span><br><span class="line"><span class="comment"># http://cs231n.github.io/neural-networks-3/</span></span><br><span class="line"><span class="comment"># 这里有非常非常详细的介绍</span></span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(</span><br><span class="line">    y_ * tf.log(y), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">train_step = tf.train.AdagradOptimizer(<span class="number">0.3</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练,这里keep_prob=0.75指训练时随机保留75%的节点,其余的节点为0</span></span><br><span class="line"><span class="comment"># cs231n提到过使dropout率为0.5最优</span></span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3000</span>):</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    train_step.run(&#123;x: batch_xs, y_: batch_ys, keep_prob: <span class="number">0.75</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测时让keep_prob=1</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">print(accuracy.eval(&#123;</span><br><span class="line">    x: mnist.test.images,</span><br><span class="line">    y_: mnist.test.labels,</span><br><span class="line">    keep_prob: <span class="number">1.0</span></span><br><span class="line">&#125;))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> Tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Python </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记(三)</title>
      <link href="/2017/04/01/tensorflow3/"/>
      <url>/2017/04/01/tensorflow3/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>深层神经网络</b></font></center><a id="more"></a><h1 id="TensorFlow学习笔记-三"><a href="#TensorFlow学习笔记-三" class="headerlink" title="TensorFlow学习笔记(三)"></a>TensorFlow学习笔记(三)</h1><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>在存在的数据集中有些数据可以简单的用一条直线或者一个平面来划分数据，则称这些为线性可分。但是有些数据是不可以分的，这样就没有办法用一条直线或者平面来区分数据。</p><p>这时候可以通过激活函数(Activation function)入一个非线性的激活函数增加模型的表达能力。</p><p>常见的激活函数有</p><ul><li>sigmoid</li></ul><p>$$<br>sigmoid(x)=\frac{1}{1+e^{-x}}<br>$$</p><p><img src="https://ww1.sinaimg.cn/large/006tNc79gy1fe79vfq7yvj308w05ot8n.jpg" alt></p><blockquote><p>cs231n中提到这个一些缺点:<br>会产生梯度消失、不关于原点中心对称会导致$f(\sigma{w_ix_i+b})$中的梯度恒正或恒负收敛速度慢、计算耗时</p></blockquote><ul><li>tanh</li></ul><p>$$<br>tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}<br>$$</p><p><img src="https://ww4.sinaimg.cn/large/006tNc79gy1fe79vhfsqej308w05m3yf.jpg" alt></p><blockquote><p>这里只解决了sigmoid中的不对称的问题、通常情况下能用tanh就不采用sigmoid。</p></blockquote><ul><li>ReLU</li></ul><p>$$<br>ReLU(x)=\begin{cases}<br>0&amp; \text{x ≥ 0}\<br>x&amp; \text{x &lt; 0}<br>\end{cases}<br>$$</p><p><img src="https://ww1.sinaimg.cn/large/006tNc79gy1fe79vim3xoj308n05u3yf.jpg" alt></p><blockquote><p>这个是最常用的一个激活函数<br>优点: 收敛比较之前的激活函数会加速6倍左右、在正数范围内不会发生梯度消失、计算速度快<br>缺点: 不关于0中心对称、x&lt;0时梯度消失、0点梯度未定义<br>初始化时,ReLU神经元将偏置值设为很小的正数(eg.0.0001)而不是0</p></blockquote><ul><li><p>下面这几个都是ReLU的变体</p></li><li><p>Leaky ReLU</p></li></ul><p>$$<br>f(x)=max(0.01x,x)<br>$$</p><ul><li>PReLU</li></ul><p>$$<br>f(x)=max(\alpha x,x)<br>$$</p><ul><li>ELU</li></ul><p>$$<br>f(x)=\begin{cases}<br>x&amp; \text{x&gt;0}\<br>\alpha(e^{x}-1)&amp; \text{x ≤ 0}<br>\end{cases}<br>$$</p><ul><li>Maxout</li></ul><p>$$<br>f(x)=max(w^{T}<em>{1}x+b_1,w^{T}</em>{2}x+b_2)<br>$$</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="noopener">cs231n Loss functions</a>这里有损失函数的详细解释</p><p>之前介绍的只是对于二分类的网络处理。虽然可以设置多个阈值来实现多分类但是实际解决问题不会这么处理。这样就可以应用损失函数解决多分类问题。</p><p>假设神经网络以一个n维数组作为输出结果，假设以识别手写体的数字1为例，则输出结果越接近[0,1,0,0,0,0,0,0,0,0]越好，评估训练得到的向量与这个准确的实际的向量的方法最常采用交叉熵(cross entropy)的评估方法。</p><p>如果给出2个概率分布p,q  通过q来表示p的交叉熵为:</p><p>$$<br>H(p,q)=-\Sigma{p(x)\log{q(x)}}\<br>p:true lable\<br>q:prediction<br>$$</p><p>交叉熵刻画的是两个概率分布之间的距离,然而神经网络的输出不一定是一个概率分布，这时就需要某种方法将神经网络前向传播的结果变成概率分布。softmax回归(softmax regression)是最常用的方法。</p><p>假设神经网络的输出为$y_1,y_2 \cdots y_n$，那么经过softmax回归之后得到的输出为:</p><p>$$<br>softmax(y_i)=\frac{e^{y_i}}{\Sigma^n_{j=1}{e^{y_j}}}<br>$$</p><ul><li>在TensorFlow中提供了交叉熵和softmax回归的函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一般都交叉熵会与softmax一起使用:</span></span><br><span class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_)</span><br></pre></td></tr></table></figure><ul><li>如果解决的不是分类问题，而是预测房价什么的输出是一个实数，这时最常用的是均方误差(MSE,mean squared error)</li></ul><p>$$<br>MSE(y,y’)=\frac{\Sigma^n_{i=1}{(y_i-y’_i)}^2}{n}\<br>y_i是第i个数据正确答案,y’_i是数据网络的预测值。<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mse = tf.reduce_mean(tf.square(y_ - y))</span><br></pre></td></tr></table></figure><h2 id="神经网络优化算法"><a href="#神经网络优化算法" class="headerlink" title="神经网络优化算法"></a>神经网络优化算法</h2><h3 id="梯度下降算法Gradient-Descent"><a href="#梯度下降算法Gradient-Descent" class="headerlink" title="梯度下降算法Gradient Descent"></a>梯度下降算法Gradient Descent</h3><ul><li>Gradient Descent (GD)</li></ul><p>$$<br>\theta_i :=\theta_i-\alpha\frac{\partial}{\partial \theta_i}J(\theta)<br>$$</p><p>$$<br>\begin {aligned}<br>\frac{\partial}{\partial \theta_i}J(\theta) &amp;= \frac{\partial}{\partial \theta_i}{\frac{1}{2}}{(h_\theta(x)-y)^2} \<br>&amp;=(h_\theta(x)-y) \frac{\partial}{\partial \theta_i}(\theta_0x_0+\cdots+\theta_nx_n-y) \<br>&amp;=(h_\theta(x)-y)x_i<br>\end {aligned}  \<br>$$</p><p>所以$\theta_i:=\theta_i-\alpha(h_\theta(x)-y)x_i$,其中$\alpha$为学习速率</p><blockquote><p>性质: 一定会结束,取决于初始值,局部最小值梯度为0. 来自Andrew Ng: Stanford Machine Learning</p></blockquote><ul><li>Batch Gradient Descent</li></ul><p>$$<br>\theta_i:=\theta_i-\alpha\sum_{j=1}^n (h_\theta(x^{(j)})-y^{(j)})x_i^{(j)}<br>$$</p><blockquote><p>遍历所有数据,适用于较小数据</p></blockquote><ul><li>Stochastic Gradient Descent (SGD)</li></ul><blockquote><p>执行此算法时,每次不一定按照局部最小值方向前行,最终在局部最小值附近。</p></blockquote><ul><li>还有一种利用矩阵运算的方法求得$J(\theta)$的最小值</li></ul><p>$$<br>\theta = (x^Tx)^{-1}x^Ty<br>$$</p><blockquote><p>直接给出结论,推导过程较长,这里不写了。。具体可以参考Ng的机器学习课程中有具体推导见 Andrew Ng: Stanford Machine Learning</p></blockquote><p>更多的优化算法的具体情况可以看<a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">cs231n</a></p><p>cs231n有两张图提供了对比</p><p><img src="https://ww1.sinaimg.cn/large/006tNc79gy1feg904kgr9g30h80dc4n1.gif" alt></p><p><img src="https://ww1.sinaimg.cn/large/006tNc79gy1feg901y65fg30h80dc1ca.gif" alt></p><h3 id="在TensorFlow中的优化算法"><a href="#在TensorFlow中的优化算法" class="headerlink" title="在TensorFlow中的优化算法"></a>在TensorFlow中的优化算法</h3><p>在官方的API文档中有专门的一节给出优化算法的API: <a href="https://www.tensorflow.org/versions/master/api_docs/python/train.html#gating-gradients" target="_blank" rel="noopener">Gating Gradients</a></p><ul><li>常用的有:</li></ul><ol><li>tf.train.AdagradOptimizer</li><li>tf.train.MomentumOptimizer</li><li>tf.train.AdamOptimizer</li></ol><h3 id="学习率的设置"><a href="#学习率的设置" class="headerlink" title="学习率的设置"></a>学习率的设置</h3><p>学习率决定了参数每次更新的幅度,如果幅度过大,那么可能导致参数在极优值的两侧来回移动。<br>学习率过大过小都可能导致不理想的优化效果,参数过大可能导致不在最小值收敛,参数过小虽然能保证收敛,但可能会大大降低优化速度。</p><p>TensorFlow提供了一个指数衰减的学习速率<code>tf.train.exponential_decay</code>,通过这个函数可以先使用较大的学习率来快速得到较优解,然后随着迭代的继续减小学习率,是的模型在训练后期更加稳定。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decayed_learning_rate = learning_rate * decay_rate ^ (global_step /decay_steps)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实际的程序代码:</span></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>)</span><br><span class="line">learning_rate = tf.train.exponential_decay(</span><br><span class="line">    <span class="number">0.1</span>, global_step, <span class="number">100</span>, <span class="number">0.96</span>, staircase=<span class="literal">True</span>)</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(</span><br><span class="line">    cross_entropy, global_step=global_step)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">以之前的二分类程序为例,把原来的学习率和改成指数衰减的形式。</span></span><br><span class="line"><span class="string">得到如下结果</span></span><br><span class="line"><span class="string">After 0 trainning steps,cross entropy on all data is 0.00922738</span></span><br><span class="line"><span class="string">After 1000 trainning steps,cross entropy on all data is 0.0072059</span></span><br><span class="line"><span class="string">After 2000 trainning steps,cross entropy on all data is 0.00654774</span></span><br><span class="line"><span class="string">After 3000 trainning steps,cross entropy on all data is 0.00614149</span></span><br><span class="line"><span class="string">After 4000 trainning steps,cross entropy on all data is 0.00588985</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 完整程序如下:</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> RandomState</span><br><span class="line"><span class="comment"># 训练数据batch大小</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line"><span class="comment"># 定义神经网络的参数</span></span><br><span class="line">weights1 = tf.Variable(tf.truncated_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">2</span>, seed=<span class="number">1</span>))</span><br><span class="line">weights2 = tf.Variable(tf.truncated_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">2</span>, seed=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 这里None是方便改变batch大小,训练时把数据分成较小的batch,测试使用全部数据</span></span><br><span class="line"><span class="comment"># 当数据集较小时这样容易测试,但数据集较大时,将大量数据放入一个batch</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">2</span>), name=<span class="string">"x-input"</span>)</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">1</span>), name=<span class="string">"y-input"</span>)</span><br><span class="line"></span><br><span class="line">a = tf.matmul(x, weights1)</span><br><span class="line">y = tf.matmul(a, weights2)</span><br><span class="line"></span><br><span class="line">cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, <span class="number">1e-10</span>, <span class="number">1.0</span>)))</span><br><span class="line"><span class="comment"># learning_rate = 0.1</span></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>)</span><br><span class="line">learning_rate = tf.train.exponential_decay(</span><br><span class="line">    <span class="number">0.1</span>, global_step, <span class="number">100</span>, <span class="number">0.96</span>, staircase=<span class="literal">True</span>)</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(</span><br><span class="line">    cross_entropy, global_step=global_step)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_step = tf.train.AdamOptimizer(learning_rate).minimize(</span></span><br><span class="line"><span class="comment">#     cross_entropy)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机模拟数据集</span></span><br><span class="line">rdm = RandomState(<span class="number">1</span>)</span><br><span class="line">dataset_size = <span class="number">128</span></span><br><span class="line">X = rdm.rand(dataset_size, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 这里用x1+x2&lt;1表示为正样本(1),其他为负样本(0)</span></span><br><span class="line">Y = [[int(x1 + x2 &lt; <span class="number">1</span>)] <span class="keyword">for</span> (x1, x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_w = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_w)</span><br><span class="line">    print(sess.run(weights1))</span><br><span class="line">    print(sess.run(weights2))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练轮数</span></span><br><span class="line">    step = <span class="number">5000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(step):</span><br><span class="line">        <span class="comment"># 每次选取batch_size个样本进行训练</span></span><br><span class="line">        start = (i * batch_size) % dataset_size</span><br><span class="line">        end = min(start + batch_size, dataset_size)</span><br><span class="line">        <span class="comment"># 对选取的样本训练神经网络并更新参数</span></span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x: X[start:end], y_: Y[start:end]&#125;)</span><br><span class="line">        <span class="comment"># 每隔1000次 将输出所有数据的交叉熵。</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            total_cross_entropy = sess.run(cross_entropy,</span><br><span class="line">                                           feed_dict=&#123;x: X,</span><br><span class="line">                                                      y_: Y&#125;)</span><br><span class="line">            print(<span class="string">"After %d trainning steps,cross entropy on all data is %g"</span> %</span><br><span class="line">                  (i, total_cross_entropy))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在训练之后输出神经网络的值</span></span><br><span class="line">    print(sess.run(weights1))</span><br><span class="line">    print(sess.run(weights2))</span><br></pre></td></tr></table></figure><h3 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h3><p>在真是的应用中,常常会遇到模型模拟了训练数据,而我们的目标确实让模型去对未知的数据做出判断。所谓过拟合是指当模型过于复杂之后,它会”记住”每个训练数据。如果模型的参数比训练的总数据还多,会导致损失函数最终为0。这种情况虽然对训练集的拟合较好,但是对于验证集则很差。</p><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>为了避免过拟合的问题,一个最常用的方法是正则化(regularization)。正则化是在损失函数中加入刻画模型复杂程度的指标。假设刻画模型在训练数据上的表现的损失函数为$J(\theta)$,那么优化时,不直接优化$J(\theta)$,而是优化$J(\theta)+\lambda R(w)$。其中$R(w)$刻画的是模型的复杂程度,而$\lambda$表示模型复杂损失在总损失中的比例。<br>这里$\theta$表示神经网络中所有的参数,包括边上的权重w和偏置b。一般来说模型复杂度只由权重w决定。常用的刻画模型复杂度的函数$R(w)$有两种。</p><p><a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="noopener">cs231n</a>里有正则化的详细介绍</p><ul><li>L1正则化(L1 Regularization)</li></ul><p>$$\sum_k \sum_l (w_{k,l})$$</p><ul><li>L2正则化(L2 Regularization)也称Ridge(岭回归)</li></ul><p>$$\sum_k \sum_l (w_{k,l})^2$$</p><blockquote><p>L0正则化的值是模型参数中非零参数的个数。<br>L1正则化表示各个参数绝对值之和。<br>L2正则化标识各个参数的平方的和的开方值。<br>可能这些公式不同的地方公式会不同,我用的是cs231n课程中的写法,但都表示同一个意思。</p></blockquote><p>还有一些可能会用到的</p><ul><li>Elastic Net 这是一种可以看出L1+L2的合成。</li></ul><p>$$R(w)=\sum_k \sum_l \beta w_{k,l}^2+|w_{k,l}|$$</p><ul><li>Max norm constraints 类似于限制每个神经元w的最大值。<a href="https://arxiv.org/pdf/1406.3190.pdf" target="_blank" rel="noopener">论文</a></li></ul><blockquote><p>Max norm constraints. Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector $\vec{w}$ of every neuron to satisfy $\Vert \vec{w} \Vert_2 &lt; c$are on orders of c 3 or 4. Some people report improvements when using this form of regularization. One of its appealing properties is that network cannot “explode” even when the learning rates are set too high because the updates are always bounded.</p></blockquote><ul><li>Dropout 随机失活一部分神经元</li></ul><center><img src="http://cs231n.github.io/assets/nn2/dropout.jpeg"></center><p><a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="noopener">cs231n中提到的正则化</a></p><ul><li>在TensorFlow中提供了带正则化的损失函数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这是正则化函数的定义</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">w = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>]), stddev=<span class="number">1</span>, seed=<span class="number">1</span>)</span><br><span class="line">y = tf.matmul(x, w)</span><br><span class="line">loss = tf.reduce_mean(tf.squre(y_ - y)) + tf.contrib.layers.l2_regularizer(<span class="keyword">lambda</span>)(w)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">其中loss为定义的损失函数,第一个部分是均方误差损失函数，第二个部分是正则化。</span></span><br><span class="line"><span class="string">lambda参数表示正则化项的权重，w是计算正则化损失的参数。</span></span><br><span class="line"><span class="string">TensorFlow提供了tf.contrib.layers.l2_regularizer函数,他可以返回一个函数，</span></span><br><span class="line"><span class="string">这个函数计算给定参数的L2正则化值，类似的还有tf.contrib.layers.l1_regularizer</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里是使用L1、L2正则化的实例</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">weights = tf.constant([[<span class="number">1.0</span>, <span class="number">-2.0</span>], [<span class="number">-3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(tf.contrib.layers.l1_regularizer(<span class="number">0.5</span>)(weights))) <span class="comment"># 0.5是正则化项的权重</span></span><br><span class="line">    print(sess.run(tf.contrib.layers.l2_regularizer(<span class="number">0.5</span>)(weights))) <span class="comment"># l2时TensorFlow会除以2,使得求导结果简洁</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"># 输出</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">5.0</span></span><br><span class="line"><span class="string">7.5</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><p>在简单的数据网络这样可以很好地计算带正则化的损失函数，但是参数增多时，这样的方式可能导致损失函数的loss的定义很长，可读性差且容易出错。但更主要的是，当网络结构复杂之后定义网络结构的部分和计算损失函数的部分可能不在一个函数中，这样通过变量这种方式计算损失函数就不方便了。为了解决这个问题，可以使用TensorFlow中提供的集合(collection)。它可以在一个计算图中(tf.Graph)中保存一组实体(比如张量)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, ld)</span>:</span></span><br><span class="line">    <span class="comment"># 生成一个变量</span></span><br><span class="line">    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)</span><br><span class="line">    <span class="comment"># add_to_collection这个函数将这个新生成的变量L2正则化损失加入集合</span></span><br><span class="line">    <span class="comment"># 第一个参数是集合名字,第二个参数是加入的内容</span></span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, tf.contrib.layers.l2_regularizer(ld)(var))</span><br><span class="line">    <span class="comment"># 返回生成的变量</span></span><br><span class="line">    <span class="keyword">return</span> var</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">1</span>))</span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line"><span class="comment"># 定义每一层网络中节点的个数</span></span><br><span class="line">layer_dimension = [<span class="number">2</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># 数据网络的层数</span></span><br><span class="line">n_layers = len(layer_dimension)</span><br><span class="line"><span class="comment"># 这个变量维护前向传播时最深的节点,开始的时候就是输入层</span></span><br><span class="line">cur_layer = x</span><br><span class="line"><span class="comment"># 当前层节点个数</span></span><br><span class="line">in_dimension = layer_dimension[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 生成五层全连接的神经网络</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n_layers):</span><br><span class="line">    <span class="comment"># layer_dimension[i]:下一层节点个数</span></span><br><span class="line">    out_dimension = layer_dimension[i]</span><br><span class="line">    weight = get_weight([in_dimension, out_dimension], <span class="number">0.001</span>)</span><br><span class="line">    bias = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[out_dimension]))</span><br><span class="line">    <span class="comment"># 使用ReLU</span></span><br><span class="line">    cur_layer = tf.nn.relu(tf.matmul(cur_layer, weight) + bias)</span><br><span class="line">    in_dimension = layer_dimension[i]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在神经网络前向传播时已经将所有的L2正则化损失加入了图的集合</span></span><br><span class="line"><span class="comment"># 这里只需要刻画模型在训练集上的损失函数</span></span><br><span class="line">mse_loss = tf.reduce_mean(tf.square(y_ - cur_layer))</span><br><span class="line"><span class="comment"># 将均方误差损失函数加入集合</span></span><br><span class="line">tf.add_to_collection(<span class="string">'losses'</span>, mse_loss)</span><br><span class="line"><span class="comment"># get_collection返回一个列表,这个列表是所有集合中的元素</span></span><br><span class="line"><span class="comment"># 这个例子中这些元素就是损失函数的不同部分,将他们加起来就可以得到最终的损失函数</span></span><br><span class="line">loss = tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> Tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Python </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记(二)</title>
      <link href="/2017/03/31/tensorflow2/"/>
      <url>/2017/03/31/tensorflow2/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>实现第一个神经网络</b></font></center><a id="more"></a><h1 id="实现一个简单的神经网络"><a href="#实现一个简单的神经网络" class="headerlink" title="实现一个简单的神经网络"></a>实现一个简单的神经网络</h1><h2 id="前向传播算法"><a href="#前向传播算法" class="headerlink" title="前向传播算法"></a>前向传播算法</h2><p>假设存在如下图所示的三层神经网络:</p><p><img src="https://ww4.sinaimg.cn/large/006tNc79gy1fe64aske70j30dq0aigm7.jpg" alt></p><p>前向传播可以表示成矩阵乘法。将输入$x_1$、$x_2$组织成一个$1\times2$的矩阵,而$W^{(1)}$组织成一个$2\times3$的矩阵:</p><p>$$<br>W^{(1)}=\begin{bmatrix}<br>W^{(1)}<em>{1,1}&amp;W^{(1)}</em>{1,2}&amp;W^{(1)}<em>{1,3}\<br>W^{(1)}</em>{2,1}&amp;W^{(1)}<em>{2,2}&amp;W^{(1)}</em>{2,3}\<br>\end{bmatrix}<br>$$</p><p>这样通过矩阵乘法可以得到隐藏层三个节点所组成的向量取值:</p><p>[这里之前出了点公式的问题，原因是 mathjax 解析中括号改了。。]</p><p>$$<br>a^{(1)}=[a_{11},a_{12},a_{13}]=xW^{(1)}=[x1,x2]\begin{bmatrix}<br>W^{(1)}<em>{1,1}&amp;W^{(1)}</em>{1,2}&amp;W^{(1)}<em>{1,3}\<br>W^{(1)}</em>{2,1}&amp;W^{(1)}<em>{2,2}&amp;W^{(1)}</em>{2,3}\<br>\end{bmatrix}\<br>=[W^{(1)}<em>{1,1}x_1+W^{(1)}</em>{2,1}x_2+W^{(1)}<em>{1,2}x_1+W^{(1)}</em>{2,2}x_2+W^{(1)}<em>{1,3}x_1+W^{(1)}</em>{2,3}x_2]<br>$$</p><p>类似的输出层可以表示为:</p><p>$$<br>y=a^{(1)}W^{(2)}=[a_{11},a_{12},a_{13}]\begin{bmatrix}W^{(2)}<em>{1,1}\W^{(2)}</em>{2,1}\<br>W^{(2)}<em>{3,1}\<br>\end{bmatrix}=[W^{(2)}</em>{1,1}a_{11}+W^{(2)}<em>{2,1}a</em>{12}+W^{(2)}<em>{3,1}a</em>{13}]<br>$$</p><p>这样就将前向传播的算法通过矩阵的乘法表示出来了。在TensorFlow中提供了一个函数直接可以进行这样的矩阵计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">b = tf.matmul(a, w2)</span><br></pre></td></tr></table></figure><h2 id="神经网络的参数"><a href="#神经网络的参数" class="headerlink" title="神经网络的参数"></a>神经网络的参数</h2><ul><li>下面给出在TensorFlow中声明了一个$2*3$的矩阵变量的方法:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">weights = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">2</span>))</span><br><span class="line"><span class="comment"># tf.random_normal([2, 3], stddev=2)函数是产生一个均值为0,方差为2的随机数,</span></span><br><span class="line"><span class="comment"># 其中可以通过添加mean参数指定平均值,没有指定时默认为0。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机变量中可以通过seed参数设定随机数种子,可以保证每次运行结果相同:</span></span><br><span class="line"><span class="comment"># tf.random_normal([2, 3], stddev=2, seed=1)</span></span><br></pre></td></tr></table></figure><blockquote><p><code>tf.random_normal</code> 正态分布,参数为:平均值、标准差、取值类型<br><br>官方API:<a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/constant_op.html#random_normal" target="_blank" rel="noopener">tf.random_normal</a><br><br><code>tf.truncated_normal</code> 正态分布,但如果随机出来的值偏离平均值超过两2个标准差,重新随机<br><br>官方API:<a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/constant_op.html#truncated_normal" target="_blank" rel="noopener">tf.truncated_normal</a><br><br><code>tf.random_uniform</code> 平均分布,参数为:最小、最大取值、取值类型<br><br>官方API:<a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/constant_op.html#random_uniform" target="_blank" rel="noopener">tf.random_uniform</a><br><br><code>tf.random_gamma</code> Gamma分布,参数为:形状参数alpha、尺度参数beta、取值类型<br><br>官方API:<a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/constant_op.html#random_gamma" target="_blank" rel="noopener">tf.random_gamma</a></p></blockquote><ul><li>NN中通常会使用常数初始化bias</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">biases = tf.Variable(tf.zeros([<span class="number">3</span>]))</span><br></pre></td></tr></table></figure><ul><li>TensorFlow中也支持使用其他变量初始值来初始化新变量:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w2 = tf.Variable(weights.initialized_value())</span><br><span class="line">w3 = tf.Variable(weights.initialized_value() * <span class="number">2</span>)</span><br></pre></td></tr></table></figure><ul><li>将前向传播过程写成代码:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 声明weights1 weights2</span></span><br><span class="line">weights1 = tf.Variable(tf.truncated_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">2</span>, seed=<span class="number">1</span>))</span><br><span class="line">weights2 = tf.Variable(tf.truncated_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">2</span>, seed=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 假设输如向量为一个1*2的常量:</span></span><br><span class="line">x = tf.constant([<span class="number">0.7</span>, <span class="number">0.9</span>], shape=[<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 前向传播算法</span></span><br><span class="line">a = tf.matmul(x, weights1)</span><br><span class="line">y = tf.matmul(a, weights2)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># 不能直接求y,要先初始化参数</span></span><br><span class="line">sess.run(weights1.initializer)</span><br><span class="line">sess.run(weights2.initializer)</span><br><span class="line">print(sess.run(y))</span><br><span class="line">sess.close()</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">输出</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">[[ 11.53417969]]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><blockquote><p>如果存在很多变量,再对每个变量初始化会导致很麻烦,TensorFlow提供了一种便捷的方式初始化:</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">init_w = global_variables_initializer()</span><br><span class="line">sess.run(init_w)</span><br><span class="line"><span class="comment"># init_w = tf.initialize_all_variables()这个是1.0之前的API</span></span><br></pre></td></tr></table></figure><h2 id="简单的监督学习"><a href="#简单的监督学习" class="headerlink" title="简单的监督学习"></a>简单的监督学习</h2><p>使用监督学习的方式设置神经网络参数需要有标注好的训练数据集。</p><p>之前所有的变量的取值都是随机的,但是在实际过程中,可能需要更好的设置参数的值,这时就需要一种优化算法。而神经网络优化算法中最常用的方法是反向传播算法。</p><p>反向传播算法实现了一个迭代的过程,在每次迭代的开始,首先需要选取一小部分训练数据,这部分的叫做batch。然后,这个batch的样例会通过前向传播算法得到预测结果,因为训练数据都有标注,所以计算出当前计算的值和真实值之间的差距,反向传播算法会更新神经网络参数的取值,使得在这个batch上神经网络模型的预测结果和真实答案更接近。</p><p>这里要注意,如果每轮迭代中选取的数据都要通过常量来表示,那么TensorFlow的计算图将会太大。因为每生成一个常量,TensorFlow将会在计算图中增加一个节点。一般来说一个神经网络的训练要经过非常多次的迭代,这样就会产生非常大的计算图,利用率很低。为了避免这个问题,TensorFlow提供了placeholder机制用于提供输入数据,而只需要将数据通过placeholder传入计算图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="number">1</span>, <span class="number">2</span>), name=<span class="string">"input"</span>)</span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">init_w = tf.global_variables_initializer()</span><br><span class="line">sess.run(init_w)</span><br><span class="line"><span class="comment"># 这行会出错</span></span><br><span class="line">print(sess.run(y))</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">InvalidArgumentError (see above <span class="keyword">for</span> traceback): You must feed a value <span class="keyword">for</span> placeholder tensor <span class="string">'input'</span> <span class="keyword">with</span> dtype float <span class="keyword">and</span> shape [<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"> [[Node: input = Placeholder[dtype=DT_FLOAT, shape=[<span class="number">1</span>,<span class="number">2</span>], _device=<span class="string">"/job:localhost/replica:0/task:0/cpu:0"</span>]()]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这样就会正常输出</span></span><br><span class="line">print(sess.run(y, feed_dict=&#123;x: [[<span class="number">0.7</span>, <span class="number">0.9</span>]]&#125;))</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">[[<span class="number">-1.25927997</span>]]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">这里替换了原来通过常量定义的输入x。在新的程序中计算前向传播结果时,</span></span><br><span class="line"><span class="string">需要提供一个feed_dict来指定x的取值。feed_dict是一个字典,</span></span><br><span class="line"><span class="string">在字典中需要给出每个placeholder的值。如果没有某个需要的placeholder的取值,</span></span><br><span class="line"><span class="string">那么程序在运行时会报错</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><p>如果需要一次性计算多个样例前向传播结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="number">3</span>, <span class="number">2</span>), name=<span class="string">"input"</span>)</span><br><span class="line">print(sess.run(y, feed_dict=&#123;x: [[<span class="number">0.7</span>, <span class="number">0.9</span>],[<span class="number">0.1</span>,<span class="number">0.4</span>],[<span class="number">0.5</span>,<span class="number">0.8</span>]&#125;))</span><br></pre></td></tr></table></figure><p>利用损失函数反向传播调整参数<br>之后详细介绍</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, <span class="number">1e-10</span>, <span class="number">1.0</span>)))</span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line"><span class="comment"># 反向传播算法</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)</span><br></pre></td></tr></table></figure><h2 id="完整神经网络样例"><a href="#完整神经网络样例" class="headerlink" title="完整神经网络样例"></a>完整神经网络样例</h2><ul><li>这是一个完整程序训练二分类问题</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> RandomState</span><br><span class="line"><span class="comment"># 训练数据batch大小</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line"><span class="comment"># 定义神经网络的参数</span></span><br><span class="line">weights1 = tf.Variable(tf.truncated_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">2</span>, seed=<span class="number">1</span>))</span><br><span class="line">weights2 = tf.Variable(tf.truncated_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">2</span>, seed=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 这里None是方便改变batch大小,训练时把数据分成较小的batch,测试使用全部数据</span></span><br><span class="line"><span class="comment"># 当数据集较小时这样容易测试,但数据集较大时,将大量数据放入一个batch</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">2</span>), name=<span class="string">"x-input"</span>)</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">1</span>), name=<span class="string">"y-input"</span>)</span><br><span class="line"></span><br><span class="line">a = tf.matmul(x, weights1)</span><br><span class="line">y = tf.matmul(a, weights2)</span><br><span class="line"></span><br><span class="line">cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, <span class="number">1e-10</span>, <span class="number">1.0</span>)))</span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机模拟数据集</span></span><br><span class="line">rdm = RandomState(<span class="number">1</span>)</span><br><span class="line">dataset_size = <span class="number">128</span></span><br><span class="line">X = rdm.rand(dataset_size, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 这里用x1+x2&lt;1表示为正样本(1),其他为负样本(0)</span></span><br><span class="line">Y = [[int(x1 + x2 &lt; <span class="number">1</span>)] <span class="keyword">for</span> (x1, x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_w = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_w)</span><br><span class="line">    print(sess.run(weights1))</span><br><span class="line">    print(sess.run(weights2))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练轮数</span></span><br><span class="line">    step = <span class="number">5000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(step):</span><br><span class="line">        <span class="comment"># 每次选取batch_size个样本进行训练</span></span><br><span class="line">        start = (i * batch_size) % dataset_size</span><br><span class="line">        end = min(start + batch_size, dataset_size)</span><br><span class="line">        <span class="comment"># 对选取的样本训练神经网络并更新参数</span></span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x: X[start:end], y_: Y[start:end]&#125;)</span><br><span class="line">        <span class="comment"># 每隔1000次 将输出所有数据的交叉熵。</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            total_cross_entropy = sess.run(cross_entropy,</span><br><span class="line">                                           feed_dict=&#123;x: X,</span><br><span class="line">                                                      y_: Y&#125;)</span><br><span class="line">            print(<span class="string">"After %d trainning steps,cross entropy on all data is %g"</span> %</span><br><span class="line">                  (i, total_cross_entropy))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在训练之后输出神经网络的值</span></span><br><span class="line">    print(sess.run(weights1))</span><br><span class="line">    print(sess.run(weights2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">输出</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">[[-1.62263644  2.96919751  0.13065873]</span></span><br><span class="line"><span class="string"> [ 0.1984968   1.27939415  3.22174239]]</span></span><br><span class="line"><span class="string">[[-1.62263644]</span></span><br><span class="line"><span class="string"> [ 2.96919751]</span></span><br><span class="line"><span class="string"> [ 0.13065873]]</span></span><br><span class="line"><span class="string">After 0 trainning steps,cross entropy on all data is 0.00922738</span></span><br><span class="line"><span class="string">After 1000 trainning steps,cross entropy on all data is 0.00651872</span></span><br><span class="line"><span class="string">After 2000 trainning steps,cross entropy on all data is 0.0046435</span></span><br><span class="line"><span class="string">After 3000 trainning steps,cross entropy on all data is 0.00296163</span></span><br><span class="line"><span class="string">After 4000 trainning steps,cross entropy on all data is 0.00143799</span></span><br><span class="line"><span class="string">可以看出这里交叉熵越来越小，表示离真实值越来越接近</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[[-2.77095127  4.07181215  1.52858329]</span></span><br><span class="line"><span class="string"> [-0.9112885   2.34183121  4.59349394]]</span></span><br><span class="line"><span class="string">[[-2.86135674]</span></span><br><span class="line"><span class="string"> [ 4.07957268]</span></span><br><span class="line"><span class="string"> [ 1.23969376]]</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> Tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Python </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记(一)</title>
      <link href="/2017/03/30/tensorflow1/"/>
      <url>/2017/03/30/tensorflow1/</url>
      
        <content type="html"><![CDATA[<center>[TensorFlow API](https://www.tensorflow.org/versions/r0.12/how_tos/variable_scope/index.html)</center><a id="more"></a><h1 id="TensorFlow学习笔记-一"><a href="#TensorFlow学习笔记-一" class="headerlink" title="TensorFlow学习笔记(一)"></a>TensorFlow学习笔记(一)</h1><p><a href="https://www.tensorflow.org" target="_blank" rel="noopener">TensorFlow</a>是目前最火的深度学习框架。</p><p>TensorFlow的环境搭建官网和其他博客都有较多例子，这里不再重复。</p><p>本机实验环境<br><code>macOS Sierra 10.12.3</code><br><code>tensorflow 1.0.0 CPU版本</code><br><code>Python 3.6.0</code></p><h2 id="TensorFlow测试样例"><a href="#TensorFlow测试样例" class="headerlink" title="TensorFlow测试样例"></a>TensorFlow测试样例</h2><p>首先TensorFlow支持C、C++、Python等语言。这里只介绍Python语言的TensorFlow的样例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了简洁，常采用tf来代替tensorflow作为模块名</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>],name=<span class="string">"a"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.constant([<span class="number">2.0</span>,<span class="number">3.0</span>],name=<span class="string">"b"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result = a + b</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sess = tf.Session()</span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn'</span>t compiled to use SSE4<span class="number">.2</span> instructions, but these are available on your machine <span class="keyword">and</span> could speed up CPU computations.</span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn'</span>t compiled to use AVX2 instructions, but these are available on your machine <span class="keyword">and</span> could speed up CPU computations.</span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; sess.run(result)</span></span><br><span class="line"><span class="string">array([ 3.,  5.], dtype=float32)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&gt;&gt;&gt; type(result)</span></span><br><span class="line"><span class="string">&lt;class '</span>tensorflow.python.framework.ops.Tenso<span class="string">r'&gt;</span></span><br></pre></td></tr></table></figure><ul><li>要输出相加的结果，不能简单的输出result，而是要先生成一个会话(session)，并且通过这个会话来计算结果。这就是一个非常简单的TensorFlow模型。</li></ul><h2 id="TensorFlow入门"><a href="#TensorFlow入门" class="headerlink" title="TensorFlow入门"></a>TensorFlow入门</h2><h3 id="TensorFlow计算图"><a href="#TensorFlow计算图" class="headerlink" title="TensorFlow计算图"></a>TensorFlow计算图</h3><p>TensorFlow包括俩个重要概念——Tensor和Flow。Tensor就是张量。Flow是流。TensorFlow是一个通过计算图形式来表述计算的编程系统。TensorFlow中的每一个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。<br>官网给出了一个演示数据流的图。</p><center><img src="https://www.tensorflow.org/images/tensors_flowing.gif"></center><h3 id="TensorFlow计算图的使用"><a href="#TensorFlow计算图的使用" class="headerlink" title="TensorFlow计算图的使用"></a>TensorFlow计算图的使用</h3><p>TensorFlow程序一般可以分为2个阶段。第一个阶段要定义计算图中所有的计算。第二个阶段为执行计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>], name=<span class="string">"a"</span>)</span><br><span class="line">b = tf.constant([<span class="number">2.0</span>, <span class="number">3.0</span>], name=<span class="string">"b"</span>)</span><br><span class="line">result = a + b</span><br><span class="line"></span><br><span class="line">print(a.graph <span class="keyword">is</span> tf.get_default_graph()) <span class="comment"># 输出为True</span></span><br><span class="line"><span class="comment"># 通过a.graph可以查看张量所属的计算图，如果没有指定，则为当前默认的计算图。所以输出为True。</span></span><br><span class="line"><span class="comment"># tf.get_default_graph()表示获取当前默认的计算图</span></span><br></pre></td></tr></table></figure><p>除了使用默认的计算图，TensorFlow还支持通过使用tf.Graph来生成新的计算图，不同计算图上的张量和运算都不会共享。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">g1 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g1.as_default():</span><br><span class="line">    <span class="comment"># 设置v=0</span></span><br><span class="line">    v = tf.get_variable(<span class="string">"v"</span>, initializer=tf.zeros(shape=[<span class="number">1</span>]))</span><br><span class="line">    <span class="comment"># 这个在1.0版本之前写成tf.zeros_initializer(shape=[1])下同</span></span><br><span class="line">g2 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g2.as_default():</span><br><span class="line">    <span class="comment"># 设置v=1</span></span><br><span class="line">    v = tf.get_variable(<span class="string">"v"</span>, initializer=tf.ones(shape=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在计算图g1中读取变量"v"的取值</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g1) <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    <span class="comment"># 这里在1.0版本之前是tf.initalizer_all_variables().run()下同</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">""</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">        <span class="comment"># 在计算图g1中，变量"v"的取值应该为0</span></span><br><span class="line">        print(sess.run(tf.get_variable(<span class="string">"v"</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在计算图g2中读取变量"v"的取值</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g2) <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">""</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">        <span class="comment"># 在计算图g2中，变量"v"的取值应该为1</span></span><br><span class="line">        print(sess.run(tf.get_variable(<span class="string">"v"</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行结果</span></span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn'</span>t compiled to use SSE4<span class="number">.2</span> instructions, but these are available on your machine <span class="keyword">and</span> could speed up CPU computations.</span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn'</span>t compiled to use AVX2 instructions, but these are available on your machine <span class="keyword">and</span> could speed up CPU computations.</span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">[ 0.]</span></span><br><span class="line"><span class="string">[ 1.]</span></span><br></pre></td></tr></table></figure><p>上面代码产生了两个计算图，每个计算图中定义了一个名字为”v”的变量。在计算g1中，将v初始化0;在计算图g2中,将v初始化为1。当运行不同的计算图时,变量v的值也不一样。TensorFlow中的计算图不仅仅可以用来隔离张量和计算,它还提供了管理张量和计算的机制。计算图可以通过tf.Graph.device函数来指定运行计算的设备。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例如</span></span><br><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.device(<span class="string">'/gpu:0'</span>):</span><br><span class="line">    result = a + b</span><br><span class="line"><span class="comment"># 后续有更详细的介绍</span></span><br></pre></td></tr></table></figure><h3 id="TensorFlow数据模型——张量"><a href="#TensorFlow数据模型——张量" class="headerlink" title="TensorFlow数据模型——张量"></a>TensorFlow数据模型——张量</h3><p>在TensorFlow中所有的数据都通过张量的形式来表示。从功能角度的看，张量可以被简单理解为多维数组。零阶张量表示标量，也就是一个数。一阶张量是向量，也就可以看成一维数组；第n阶张量也可以看成n维数组。但张量并不是直接采用数组的形式，它只是对TensorFlow中运算结果的引用。<code>张量中并没有真正保存数字，保存的是如何得到这些过程的计算过程</code>。张量的类型当然也可以是字符串，这里先不讨论。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>], name=<span class="string">"a"</span>)</span><br><span class="line">b = tf.constant([<span class="number">2.0</span>, <span class="number">3.0</span>], name=<span class="string">"b"</span>)</span><br><span class="line">result = tf.add(a, b, name=<span class="string">"add"</span>)</span><br><span class="line">print(result)</span><br><span class="line"><span class="comment"># 输出为</span></span><br><span class="line">Tensor(<span class="string">"add:0"</span>, shape=(<span class="number">2</span>,), dtype=float32)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">从这里可以看出TensorFlow中的张量和Numpy的数组不同。</span></span><br><span class="line"><span class="string">TensorFlow计算的结果不是一个具体的数字，而是一个张量的结构。</span></span><br><span class="line"><span class="string">一个张量中主要保存了三个属性 name、shape、type</span></span><br><span class="line"><span class="string">name具体形式为 node:src_output</span></span><br><span class="line"><span class="string">其中 node 为节点的名称</span></span><br><span class="line"><span class="string">src_output 表示当前张量来自节点的第几个输出</span></span><br><span class="line"><span class="string">例如 add:0 表示result这个张量是计算节点add输出的第一个结果(以0为开始)</span></span><br><span class="line"><span class="string">type 是张量的类型，每一个张量都会有一个唯一的类型。</span></span><br><span class="line"><span class="string">TensorFlow会对所有参与运算的张量进行类型的检查，当发现类型不匹配时会报错。比如:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>], name=<span class="string">"a"</span>)</span><br><span class="line">b = tf.constant([<span class="number">2.0</span>, <span class="number">3.0</span>], name=<span class="string">"b"</span>)</span><br><span class="line">result = tf.add(a, b, name=<span class="string">"add"</span>)</span><br><span class="line">print(result)</span><br><span class="line"><span class="comment"># 报错</span></span><br><span class="line">ValueError: Tensor conversion requested dtype int32 <span class="keyword">for</span> Tensor <span class="keyword">with</span> dtype float32: <span class="string">'Tensor("b:0", shape=(2,), dtype=float32)'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 但是如果在constant里面加入一个参数dtype=tf.float32则程序就正常运行</span></span><br><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>], name=<span class="string">"a"</span>, dtype=tf.float32)</span><br><span class="line">b = tf.constant([<span class="number">2.0</span>, <span class="number">3.0</span>], name=<span class="string">"b"</span>)</span><br><span class="line">result = tf.add(a, b, name=<span class="string">"add"</span>)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">Tensor(<span class="string">"add:0"</span>, shape=(<span class="number">2</span>,), dtype=float32)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">如果不指定默认类型，则TensorFlow会给出默认的类型，</span></span><br><span class="line"><span class="string">比如无小数点的会被默认为int32，有小数点的会被默认为float32。</span></span><br><span class="line"><span class="string">TensorFlow支持14种类型</span></span><br><span class="line"><span class="string">浮点数tf.float32 tf.float64 </span></span><br><span class="line"><span class="string">整数tf.int8 tf.int16 tf.int32 tf.int64 tf.uint8</span></span><br><span class="line"><span class="string">布尔型tf.bool </span></span><br><span class="line"><span class="string">复数tf.complex64 tf.complex128</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><h3 id="TensorFlow运行模型——会话-Session"><a href="#TensorFlow运行模型——会话-Session" class="headerlink" title="TensorFlow运行模型——会话(Session)"></a>TensorFlow运行模型——会话(Session)</h3><ul><li>明确调用会话生成函数和关闭会话函数，使用这种模式要明确调用Session.close函数关闭会话而释放资源。然而如果程序异常退出时则不会调用close导致资源泄露。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>], name=<span class="string">"a"</span>, dtype=tf.float32)</span><br><span class="line">b = tf.constant([<span class="number">2.0</span>, <span class="number">3.0</span>], name=<span class="string">"b"</span>)</span><br><span class="line">result = tf.add(a, b, name=<span class="string">"add"</span>)</span><br><span class="line"><span class="comment"># 创建一个会话</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(result)</span><br><span class="line"><span class="comment"># 关闭会话使得本次运行中使用的资源可以被释放</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure><ul><li>使用python的上下文管理器的机制，将所有的计算放在with的内部就可以自动释放所有资源。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(result)</span><br></pre></td></tr></table></figure><ul><li>TensorFlow会自动生成默认的计算图，如果没有特殊指定，运算会自动加入这个计算图中，TensorFlow中的会话也有类似机制，但不会自动生成默认的会话，而是需要手动指定。当默认的会话被指定之后可以通过tf.Tensor.eval函数来计算一个张量的取值。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    print(result.eval())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 </span></span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn'</span>t compiled to use SSE4<span class="number">.2</span> instructions, but these are available on your machine <span class="keyword">and</span> could speed up CPU computations.</span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn'</span>t compiled to use AVX2 instructions, but these are available on your machine <span class="keyword">and</span> could speed up CPU computations.</span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">[ 3.  5.]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 这2个命令都可以输出一样的结果</span></span><br><span class="line"><span class="string">print(sess.run(result))</span></span><br><span class="line"><span class="string">print(result.eval(session=sess))</span></span><br></pre></td></tr></table></figure><ul><li>TensorFlow提供了一个直接构建默认会话的函数。使用这个函数会自动将生成的会话注册为默认会话。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">print(result.eval())</span><br><span class="line">sess.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 无论哪种方法都可以通过ConfigProto Protocol Buffer来配置需要生成的会话:</span></span><br><span class="line"></span><br><span class="line">config = tf.ConfigProto(allow_soft_placement=<span class="literal">True</span>, log_device_placement=<span class="literal">True</span>)</span><br><span class="line">sess1 = tf.InteractiveSession(config=config)</span><br><span class="line">sess2 = tf.Session(config=config)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">通过ConfigProto可以配置类似并行的线程数、GPU分配策略、运算超时时间等参数</span></span><br><span class="line"><span class="string">在这些参数中，最长使用的有两个</span></span><br><span class="line"><span class="string">1)allow_soft_placement,默认为False,但为True时表示符合以下条件时GPU运算可以放在CPU上进行:</span></span><br><span class="line"><span class="string">1.运算无法在GPU运行</span></span><br><span class="line"><span class="string">2.没有GPU资源(运算被指定在第二个GPU运行，但设备只有一个GPU)</span></span><br><span class="line"><span class="string">3.运算输入包含CPU计算结果的引用</span></span><br><span class="line"><span class="string">2)log_device_placement,当它为True时日志会记录每个节点被安排在哪个设备上以方便调试,但是在生产环境中设置为False可以减少日志量。</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> Tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Python </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬虫学习笔记(四)</title>
      <link href="/2017/03/23/request5/"/>
      <url>/2017/03/23/request5/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>正则表达式 和 Re库</b></font></center><a id="more"></a><h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><h2 id="正则表达式-regular-expression-regex-RE"><a href="#正则表达式-regular-expression-regex-RE" class="headerlink" title="正则表达式(regular expression   regex   RE)"></a>正则表达式(regular expression   regex   RE)</h2><p>正则表达式是用来简洁表达一组字符串的表达式。</p><ul><li>通用的字符串表达框架</li><li>简洁表达一组字符串的表达式</li><li>针对字符串表达“简洁”和“特征”思想的工具</li><li>判断某字符串的特征归属</li></ul><h3 id="作用"><a href="#作用" class="headerlink" title="作用:"></a>作用:</h3><ul><li>表达文本类型的特征(病毒、入侵等)</li><li>同时查找或替换一组字符串</li><li>匹配字符串的全部或部分</li></ul><h3 id="使用正则表达式："><a href="#使用正则表达式：" class="headerlink" title="使用正则表达式："></a>使用正则表达式：</h3><ul><li>编译:将符合正则表达式语法的字符串转换成正则表达式特征。</li></ul><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fjo295abgoj310c12sn5b.jpg" alt></p><p><code>ps: 因为hexo的markdown解析器的bug解析不了html中的&#39;*&#39;之前直接用表格但是又会出现&#39;|&#39;解析不了的情况,html转义字符又没有这个,只能这样了。</code></p><p><code>真是垃圾解析器。我改了2天,换了解析器都不行。。</code></p><h3 id="正则表达式语法实例"><a href="#正则表达式语法实例" class="headerlink" title="正则表达式语法实例"></a>正则表达式语法实例</h3><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fjo2a6wqyqj30v80cu40z.jpg" alt></p><h3 id="经典正则表达式实例"><a href="#经典正则表达式实例" class="headerlink" title="经典正则表达式实例"></a>经典正则表达式实例</h3><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fjo2ah9bd5j30qe0gkgop.jpg" alt></p><h1 id="Re库"><a href="#Re库" class="headerlink" title="Re库"></a>Re库</h1><ul><li><p>raw string类型(原生字符串类型)</p><blockquote><p>表示为<code>r&#39;text&#39;</code><br><br>  例如<code>r&#39;[1-9]\d{5}&#39;</code><br><br>  <code>r&#39;\d{3}-\d{8}|\d{4}-\d{7}&#39;</code><br><br>  raw string 是不包含转义符的字符串</p></blockquote></li><li><p>string类型,更繁琐</p><blockquote><p>例如 <code>&#39;[1-9]\\d{5}&#39;</code><br><br>  <code>&#39;\\d{3}-\\d{8}|\\d{4}-\\d{7}&#39;</code></p></blockquote></li></ul><h2 id="Re库主要功能函数"><a href="#Re库主要功能函数" class="headerlink" title="Re库主要功能函数"></a>Re库主要功能函数</h2><table><thead><tr><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>re.search()</td><td>在一个字符串中搜索匹配正则表达式的第一个位置,返回match对象</td></tr><tr><td>re.match()</td><td>在一个字符串的开始位置起匹配正则表达式,返回match对象</td></tr><tr><td>re.findall()</td><td>搜索字符串,以列表类型返回全部能匹配的字符串</td></tr><tr><td>re.split()</td><td>将一个字符串按照正则表达式匹配结果进行分割,返回列表类型</td></tr><tr><td>re.finditer()</td><td>搜索字符串,返回一个匹配结果的迭代类型,每个迭代元素是match对象</td></tr><tr><td>re.sub()</td><td>在一个字符串中替换所有匹配正则表达式的子串,返回替换后的字符串</td></tr></tbody></table><h3 id="re-search-pattern-string-flags-0"><a href="#re-search-pattern-string-flags-0" class="headerlink" title="re.search(pattern,string,flags=0)"></a><code>re.search(pattern,string,flags=0)</code></h3><ul><li>在一个字符串中搜索匹配正则表达式的第一个位置,返回match对象.<ul><li>pattern: 正则表达式的字符串或原生字符串表示</li><li>string: 待匹配字符串</li><li>flags: 正则表达式使用时的控制标记</li></ul></li></ul><blockquote><table><thead><tr><th>常用标记</th><th>说明</th></tr></thead><tbody><tr><td>re.I re.IGNORECASE</td><td>忽略正则表达式的大小写,[A-Z]能够匹配小写字符</td></tr><tr><td>re.M re.MULTILINE</td><td>正则表达式中的^操作符能够将给定字符串的每行当作匹配开始</td></tr><tr><td>re.S re.DOTALL</td><td>正则表达式中的.操作符能够匹配所有字符,默认匹配除换行符外的所有字符</td></tr></tbody></table></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>match = re.search(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'SZU 518060'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">if</span> match:</span><br><span class="line"><span class="meta">... </span>    print(match.group(<span class="number">0</span>))</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="number">518060</span></span><br></pre></td></tr></table></figure><h3 id="re-match-pattern-string-flags-0"><a href="#re-match-pattern-string-flags-0" class="headerlink" title="re.match(pattern,string,flags=0)"></a><code>re.match(pattern,string,flags=0)</code></h3><ul><li><p>在一个字符串的开始位置起匹配正则表达式,返回match对象.</p><ul><li>pattern: 正则表达式的字符串或原生字符串表示</li><li>string: 待匹配字符串</li><li>flags: 正则表达式使用时的控制标记</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>match = re.match(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'518060 SZU'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">if</span> match:</span><br><span class="line"><span class="meta">... </span>    match.group(<span class="number">0</span>)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="string">'518060'</span></span><br></pre></td></tr></table></figure><h3 id="re-findall-pattern-string-flags-0"><a href="#re-findall-pattern-string-flags-0" class="headerlink" title="re.findall(pattern,string,flags=0)"></a><code>re.findall(pattern,string,flags=0)</code></h3><ul><li><p>搜索字符串,以列表类型返回全部能匹配的字符串.</p><ul><li>pattern: 正则表达式的字符串或原生字符串表示</li><li>string: 待匹配字符串</li><li>flags: 正则表达式使用时的控制标记</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ls = re.findall(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'SZU518060 CZ239000'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ls</span><br><span class="line">[<span class="string">'518060'</span>, <span class="string">'239000'</span>]</span><br></pre></td></tr></table></figure><h3 id="re-split-pattern-string-maxsplit-0-flags-0"><a href="#re-split-pattern-string-maxsplit-0-flags-0" class="headerlink" title="re.split(pattern,string,maxsplit=0,flags=0)"></a><code>re.split(pattern,string,maxsplit=0,flags=0)</code></h3><ul><li><p>将一根字符串按照正则表达式匹配结果进行分割,返回列表类型.</p><ul><li>pattern: 正则表达式的字符串或原生字符串表示</li><li>string: 待匹配字符串</li><li>maxsplit: 最大分割数,剩余部分作为最后一个元素输出</li><li>flags: 正则表达式使用时的控制标记</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re.split(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'SZU518060 CZ239000'</span>)</span><br><span class="line">[<span class="string">'SZU'</span>, <span class="string">' CZ'</span>, <span class="string">''</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re.split(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'SZU518060 CZ239000'</span>,maxsplit=<span class="number">1</span>)</span><br><span class="line">[<span class="string">'SZU'</span>, <span class="string">' CZ239000'</span>]</span><br></pre></td></tr></table></figure><h3 id="re-finditer-pattern-string-flags-0"><a href="#re-finditer-pattern-string-flags-0" class="headerlink" title="re.finditer(pattern,string,flags=0)"></a><code>re.finditer(pattern,string,flags=0)</code></h3><ul><li><p>搜索字符串,返回一个匹配结果的迭代类型,每个迭代元素是match对象.</p><ul><li>pattern: 正则表达式的字符串或原生字符串表示</li><li>string: 待匹配字符串</li><li>flags: 正则表达式使用时的控制标记</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> m <span class="keyword">in</span> re.finditer(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'SZU518060 CZ239000'</span>):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">if</span> m:</span><br><span class="line"><span class="meta">... </span>            print(m.group(<span class="number">0</span>))</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="number">518060</span></span><br><span class="line"><span class="number">239000</span></span><br></pre></td></tr></table></figure><h3 id="re-sub-pattern-repl-string-count-0-flags-0"><a href="#re-sub-pattern-repl-string-count-0-flags-0" class="headerlink" title="re.sub(pattern,repl,string,count=0,flags=0)"></a><code>re.sub(pattern,repl,string,count=0,flags=0)</code></h3><ul><li><p>在一个字符串中替换所有匹配正则表达式的子串,返回替换后的字符串.</p><ul><li>pattern: 正则表达式的字符串或原生字符串表示</li><li>repl: 替换匹配字符串的字符串</li><li>string: 待匹配字符串</li><li>count: 匹配的最大替换次数</li><li>flags: 正则表达式使用时的控制标记</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re.sub(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'zipcode'</span>,<span class="string">'BIT100081 TSU100084'</span>)</span><br><span class="line"><span class="string">'BITzipcode TSUzipcode'</span></span><br></pre></td></tr></table></figure><h2 id="Re库的另一种等价用法"><a href="#Re库的另一种等价用法" class="headerlink" title="Re库的另一种等价用法"></a>Re库的另一种等价用法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 函数式用法:一次性操作</span></span><br><span class="line">rst = re.search(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'SZ 518060'</span>)</span><br><span class="line"><span class="comment"># 面向对象用法:编译后多次操作</span></span><br><span class="line">pat = re.compile(<span class="string">r'[1-9]\d&#123;5&#125;'</span>)</span><br><span class="line">rst = pat.search(<span class="string">'SZ 518060'</span>)</span><br></pre></td></tr></table></figure><p><code>regex = re.compile(pattern,flags=0)</code></p><ul><li><p>将正则表达式的字符串形式编译成正则表达式对象</p><ul><li>pattern: 正则表达式的字符串或原生字符串表示</li><li>flags: 正则表达式使用时的控制标记</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>regex = re.compile(<span class="string">r'[1-9]\d&#123;5&#125;'</span>)</span><br><span class="line"><span class="comment"># 这里regex才是正则表达式</span></span><br><span class="line"><span class="comment"># r'[1-9]\d&#123;5&#125;'这种只是表示字符串</span></span><br></pre></td></tr></table></figure><center><b>等价的函数用法</b></center><table><thead><tr><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>regex.search()</td><td>在一个字符串中搜索匹配正则表达式的第一个位置,返回match对象</td></tr><tr><td>regex.match()</td><td>在一个字符串的开始位置起匹配正则表达式,返回match对象</td></tr><tr><td>regex.findall()</td><td>搜索字符串,以列表类型返回全部能匹配的字符串</td></tr><tr><td>regex.split()</td><td>将一个字符串按照正则表达式匹配结果进行分割,返回列表类型</td></tr><tr><td>regex.finditer()</td><td>搜索字符串,返回一个匹配结果的迭代类型,每个迭代元素是match对象</td></tr><tr><td>regex.sub()</td><td>在一个字符串中替换所有匹配正则表达式的子串,返回替换后的字符串</td></tr></tbody></table><h2 id="Match对象"><a href="#Match对象" class="headerlink" title="Match对象"></a>Match对象</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>match = re.search(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'BIT 100081'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(match)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">_sre</span>.<span class="title">SRE_Match</span>'&gt;</span></span><br></pre></td></tr></table></figure><center>Match对象的属性</center><table><thead><tr><th>属性</th><th>说明</th></tr></thead><tbody><tr><td>.string</td><td>待匹配的文本</td></tr><tr><td>.re</td><td>匹配时使用的pattern对象(正则表达式)</td></tr><tr><td>.pos</td><td>正则表达式搜索文本的开始位置</td></tr><tr><td>.endpos</td><td>正则表达式搜索文本的结束位置</td></tr></tbody></table><center>Match对象的方法</center><table><thead><tr><th>方法</th><th>说明</th></tr></thead><tbody><tr><td>.group(0)</td><td>获得匹配后的字符串</td></tr><tr><td>.start()</td><td>匹配字符串在原始字符串的开始位置</td></tr><tr><td>.end()</td><td>匹配字符串在原始字符串的结束位置</td></tr><tr><td>.span()</td><td>返回(.start(),.end())</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = re.search(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'BIT 100081 SZU518060'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.string</span><br><span class="line"><span class="string">'BIT 100081 SZU518060'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.re</span><br><span class="line">re.compile(<span class="string">'[1-9]\\d&#123;5&#125;'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.pos</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.endpos</span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.group(<span class="number">0</span>)</span><br><span class="line"><span class="string">'100081'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.start()</span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.end()</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.span()</span><br><span class="line">(<span class="number">4</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure><h2 id="Re库的贪婪匹配和最小匹配"><a href="#Re库的贪婪匹配和最小匹配" class="headerlink" title="Re库的贪婪匹配和最小匹配"></a>Re库的贪婪匹配和最小匹配</h2><h3 id="贪婪匹配"><a href="#贪婪匹配" class="headerlink" title="贪婪匹配"></a>贪婪匹配</h3><p>Re库默认采用贪婪匹配,即输出匹配最长子串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>match = re.search(<span class="string">r'PY.*N'</span>,<span class="string">'PYANBNCNDN'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>match.group(<span class="number">0</span>)</span><br><span class="line"><span class="string">'PYANBNCNDN'</span></span><br></pre></td></tr></table></figure><h3 id="最小匹配"><a href="#最小匹配" class="headerlink" title="最小匹配"></a>最小匹配</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>match = re.search(<span class="string">r'PY.*?N'</span>,<span class="string">'PYANBNCNDN'</span>) <span class="comment"># 在*后面加？</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>match.group(<span class="number">0</span>)</span><br><span class="line"><span class="string">'PYAN'</span></span><br></pre></td></tr></table></figure><h4 id="最小匹配操作符"><a href="#最小匹配操作符" class="headerlink" title="最小匹配操作符"></a>最小匹配操作符</h4><table><thead><tr><th>操作符</th><th>说明</th></tr></thead><tbody><tr><td>*?</td><td>前一个字符0次或无限次扩展,最小匹配</td></tr><tr><td>+?</td><td>前一个字符1次或无限次扩展,最小匹配</td></tr><tr><td>??</td><td>前一个字符0次或1次扩展,最小匹配</td></tr><tr><td>{m,n}?</td><td>扩展前一个字符m至n次(含n),最小匹配</td></tr></tbody></table><h2 id="常用正则表达式"><a href="#常用正则表达式" class="headerlink" title="常用正则表达式"></a>常用正则表达式</h2><p><a href="http://deerchao.net/tutorials/regex/common.htm" target="_blank" rel="noopener">常用正则表达式</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> Spider </category>
          
          <category> Requests </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python一些常用的方法技巧</title>
      <link href="/2017/03/12/pytick/"/>
      <url>/2017/03/12/pytick/</url>
      
        <content type="html"><![CDATA[<center><font color="gray">基于python3</font></center><a id="more"></a><h1 id="列表元素操作"><a href="#列表元素操作" class="headerlink" title="列表元素操作"></a>列表元素操作</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 插入元素</span></span><br><span class="line">a.insert(位置,<span class="string">'值'</span>)</span><br><span class="line">a.insert(<span class="number">0</span>,<span class="string">'hello'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除元素</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="string">'f'</span>, <span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'g'</span>, <span class="string">'d'</span>, <span class="string">'z'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">del</span> a[<span class="number">0</span>] <span class="comment"># 永久删除</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'g'</span>, <span class="string">'d'</span>, <span class="string">'z'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># pop()可删除列表末尾的元素,相当于弹出栈顶元素</span></span><br><span class="line"><span class="comment"># pop(位置)可删除指定位置的元素</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'g'</span>, <span class="string">'d'</span>, <span class="string">'z'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ap = a.pop(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ap</span><br><span class="line"><span class="string">'b'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'g'</span>, <span class="string">'d'</span>, <span class="string">'z'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据值删除元素</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'g'</span>, <span class="string">'d'</span>, <span class="string">'z'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.remove(<span class="string">'g'</span>) </span><br><span class="line"><span class="comment"># remove()只删除第一个指定的值,如果要删除的值可能在列表中出现多次,那么就要循环判断是否删除了所有这样的值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'z'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 组织列表</span></span><br><span class="line"><span class="comment"># sort()永久性修改列表元素的排序</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="string">'f'</span>,<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'g'</span>,<span class="string">'d'</span>,<span class="string">'z'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.sort()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'f'</span>, <span class="string">'g'</span>, <span class="string">'z'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># sorted()对列表进行临时排序</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="string">'f'</span>,<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'g'</span>,<span class="string">'d'</span>,<span class="string">'z'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(sorted(a))</span><br><span class="line">[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'f'</span>, <span class="string">'g'</span>, <span class="string">'z'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反序排列reverse()</span></span><br><span class="line"><span class="comment"># 并不是字母倒叙排列,而是按照原来元素的排列顺序</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="string">'f'</span>, <span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'g'</span>, <span class="string">'d'</span>, <span class="string">'z'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.reverse()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="string">'z'</span>, <span class="string">'d'</span>, <span class="string">'g'</span>, <span class="string">'c'</span>, <span class="string">'b'</span>, <span class="string">'a'</span>, <span class="string">'f'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确定列表的长度(从1开始的,不用考虑差1)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="string">'z'</span>, <span class="string">'d'</span>, <span class="string">'g'</span>, <span class="string">'c'</span>, <span class="string">'b'</span>, <span class="string">'a'</span>, <span class="string">'f'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(a)</span><br><span class="line"><span class="number">7</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建数值列表:range()</span></span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>):</span><br><span class="line">print(value)</span><br><span class="line"><span class="comment"># 这里产生1-4的值</span></span><br></pre></td></tr></table></figure><h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 禁止函数修改列表</span></span><br><span class="line"><span class="comment"># 利用切片表示法[:]创建列表的副本</span></span><br><span class="line">function_name(list_name[:])</span><br><span class="line"><span class="comment"># 这种函数所修改的不会影响到列表list_name</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 传递任意数量的实参</span></span><br><span class="line">function_name(*tuple)</span><br><span class="line"><span class="comment"># 形参名 *tuple 中的星号让python创建一个tuple的空元组</span></span><br><span class="line"><span class="comment"># 并将收到的所有值都封装到这个元组中。</span></span><br><span class="line"><span class="comment"># 例如</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fu</span><span class="params">(*tuples)</span>:</span></span><br><span class="line"><span class="meta">... </span>    print(<span class="string">"this is start:"</span>)</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">for</span> tuple <span class="keyword">in</span> tuples:</span><br><span class="line"><span class="meta">... </span>            print(<span class="string">'-'</span> + tuple)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fu(<span class="string">'I'</span>)</span><br><span class="line">this <span class="keyword">is</span> start:</span><br><span class="line">-I</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fu(<span class="string">'like'</span>,<span class="string">'you'</span>)</span><br><span class="line">this <span class="keyword">is</span> start:</span><br><span class="line">-like</span><br><span class="line">-you</span><br><span class="line"></span><br><span class="line"><span class="comment"># 传递任意数量的关键字实参</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">build_profile</span><span class="params">(first, last, **user_info)</span>:</span></span><br><span class="line"><span class="meta">... </span>     profile = &#123;&#125;</span><br><span class="line"><span class="meta">... </span>     profile[<span class="string">'first_name'</span>] = first</span><br><span class="line"><span class="meta">... </span>     profile[<span class="string">'last_name'</span>] = last</span><br><span class="line"><span class="meta">... </span>     <span class="keyword">for</span> key,value <span class="keyword">in</span> user_info.items():</span><br><span class="line"><span class="meta">... </span>             profile[key] = value</span><br><span class="line"><span class="meta">... </span>     <span class="keyword">return</span> profile</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>user_profile = build_profile(<span class="string">'albert'</span>, <span class="string">'einstein'</span>,location=<span class="string">'princeton'</span>,filed=<span class="string">'physics'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(user_profile)</span><br><span class="line">&#123;<span class="string">'location'</span>: <span class="string">'princeton'</span>, <span class="string">'first_name'</span>: <span class="string">'albert'</span>, <span class="string">'last_name'</span>: <span class="string">'einstein'</span>, <span class="string">'filed'</span>: <span class="string">'physics'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">####################################################</span></span><br><span class="line"><span class="comment"># 如果一个.py文件只有函数,就构成了模块:</span></span><br><span class="line"><span class="comment"># example.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exfun</span><span class="params">()</span>:</span></span><br><span class="line">print(<span class="string">"hello!"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># demo.py:</span></span><br><span class="line"><span class="keyword">import</span> examples</span><br><span class="line">examples.exfun()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>hello!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入特定的函数:</span></span><br><span class="line"><span class="keyword">from</span> module_name <span class="keyword">import</span> function_name</span><br><span class="line"><span class="comment"># 使用分号导入任意数量的函数</span></span><br><span class="line"><span class="keyword">from</span> module_name <span class="keyword">import</span> function_name1, function_name2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用as给函数指定别名:</span></span><br><span class="line"><span class="keyword">from</span> module_name <span class="keyword">import</span> function_name <span class="keyword">as</span> fn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用as给模块指定别名:</span></span><br><span class="line"><span class="keyword">import</span> module_name <span class="keyword">as</span> mn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入模块所有函数:</span></span><br><span class="line"><span class="keyword">from</span> module_name <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><h1 id="类"><a href="#类" class="headerlink" title="类"></a>类</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># python3 创建类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassName</span><span class="params">()</span>:</span></span><br><span class="line">--snip--</span><br><span class="line"></span><br><span class="line"><span class="comment"># python2.7 创建类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassName</span><span class="params">(object)</span>:</span></span><br><span class="line">--snip--</span><br><span class="line"></span><br><span class="line"><span class="comment"># python3 继承</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassName</span><span class="params">(object)</span>:</span></span><br><span class="line">--snip--</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassName2</span><span class="params">(ClassName)</span>:</span></span><br><span class="line">--snip--</span><br><span class="line"></span><br><span class="line"><span class="comment"># 例如:</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Car</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, make, model, year)</span>:</span></span><br><span class="line">        self.make = make</span><br><span class="line">        self.model = model</span><br><span class="line">        self.year = year</span><br><span class="line">        self.odometer_reading = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_descriptive_name</span><span class="params">(self)</span>:</span></span><br><span class="line">        long_name = str(self.year) + <span class="string">' '</span> + self.make + <span class="string">' '</span> + self.model</span><br><span class="line">        <span class="keyword">return</span> long_name.title()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">increment_odometer</span><span class="params">(self, miles)</span>:</span></span><br><span class="line">        self.odometer_reading += miles</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ElectricCar</span><span class="params">(Car)</span>:</span></span><br><span class="line">    <span class="string">"""电动汽车独特之处"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, make, model, year)</span>:</span></span><br><span class="line">        <span class="string">"""初始化父类属性"""</span></span><br><span class="line">        super().__init__(make, model, year)</span><br><span class="line"></span><br><span class="line">my_tesla = ElectricCar(<span class="string">'tesla'</span>, <span class="string">'model s'</span>, <span class="number">2016</span>)</span><br><span class="line">print(my_tesla.get_descriptive_name())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># python2.7 继承</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassName</span><span class="params">()</span>:</span></span><br><span class="line">--snip--</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassName2</span><span class="params">(ClassName)</span>:</span></span><br><span class="line">--snip--</span><br><span class="line"></span><br><span class="line"><span class="comment"># python2.7 super()需要两个实参:子类名和对象self。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ElectricCar</span><span class="params">(Car)</span>:</span></span><br><span class="line">    <span class="string">"""电动汽车独特之处"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, make, model, year)</span>:</span></span><br><span class="line">        <span class="string">"""初始化父类属性"""</span></span><br><span class="line">        super(ElectricCar, self).__init__(make, model, year)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 导入类的方法和导入函数的方法类似</span></span><br></pre></td></tr></table></figure><h1 id="文件和异常"><a href="#文件和异常" class="headerlink" title="文件和异常"></a>文件和异常</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Linux 和 OS X中文件路径使用斜杠(/)</span></span><br><span class="line"><span class="comment"># 例如</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'text_file / filename.txt'</span>) <span class="keyword">as</span> file_object:</span><br><span class="line"><span class="comment"># 在Windows中使用反斜杠(\)</span></span><br><span class="line"><span class="comment"># 例如</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'text_file \ filename.txt'</span>) <span class="keyword">as</span> file_object:</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入空文件</span></span><br><span class="line">filename = <span class="string">'programing.txt'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(filename, <span class="string">'w'</span>) <span class="keyword">as</span> file_object:</span><br><span class="line">    file_object.write(<span class="string">"I love programing."</span>) <span class="comment"># write()不会再写入的文本末尾加换行符</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 'w'写入模式   'r'只读模式   'r+'读写  'a'附加模式</span></span><br><span class="line"><span class="comment"># 以'w'打开文件时，如果指定的问价已经存在，python将在返回文件对象前清空该文件</span></span><br><span class="line"><span class="comment"># 写入多行需要在行后加入换行符'\n'还可偶一使用空格制表符空行来设置输出的格式</span></span><br><span class="line"><span class="comment"># 如果以附加模式('a')打开文件，python不会在返回文件对象前清空文件，而写入到文件的行都讲添加到文件末尾。如果指定的文件不存在，python将会创建一个空白文件。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 异常</span></span><br><span class="line"><span class="comment"># 使用try-except代码块</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">print(<span class="number">5</span>/<span class="number">0</span>)</span><br><span class="line"><span class="keyword">except</span> ZeroDivisionError:</span><br><span class="line">print(<span class="string">'you can not divide by zero!'</span>)</span><br><span class="line"><span class="comment"># 这时就不会出现traceback 输出为"you can not divide by zero!"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># try-except-else</span></span><br><span class="line"><span class="comment"># python尝试执行try代码块中的代码;只有可能引发异常的代码才需要放在try语句中,</span></span><br><span class="line"><span class="comment"># 有时候一些仅在try代码块成功执行时才需要运行的代码;这些代码应放在else代码块中。</span></span><br><span class="line"><span class="comment"># except代码块告诉python如果它尝试执行try代码块中的代码时引发了指定异常,该怎么办.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用json保存和读取用户生成的数据</span></span><br><span class="line"><span class="comment"># demo1.py</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">username = input(<span class="string">"what is your name?"</span>)</span><br><span class="line">filename = <span class="string">'username.json'</span></span><br><span class="line"><span class="keyword">with</span> open(filename, <span class="string">'w'</span>) <span class="keyword">as</span> f_obj:</span><br><span class="line">    json.dump(username, f_obj)</span><br><span class="line">    print(<span class="string">"we'll remember you when you come back , "</span> + username + <span class="string">"!"</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># demo2.py</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">filename = <span class="string">'username.json'</span></span><br><span class="line"><span class="keyword">with</span> open(filename) <span class="keyword">as</span> fn:</span><br><span class="line">    username = json.load(fn)</span><br><span class="line">    print(<span class="string">"welcome back,"</span> + username + <span class="string">"!"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者用FileNotFoundError异常处理</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">filename = <span class="string">'username.json'</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">with</span> open(filename) <span class="keyword">as</span> fn:</span><br><span class="line">        username = json.load(fn)</span><br><span class="line"><span class="keyword">except</span> FileNotFoundError:</span><br><span class="line">    username = input(<span class="string">"what is your name?"</span>)</span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'w'</span>) <span class="keyword">as</span> fn:</span><br><span class="line">        json.dump(username, f_obj)</span><br><span class="line">        print(<span class="string">"we'll remember you when you come back , "</span> + username + <span class="string">"!"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"welcome back,"</span> + username + <span class="string">"!"</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬虫学习笔记(三)</title>
      <link href="/2017/03/10/request4/"/>
      <url>/2017/03/10/request4/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>信息标记的处理</b></font></center><a id="more"></a><h1 id="信息标记的处理"><a href="#信息标记的处理" class="headerlink" title="信息标记的处理"></a>信息标记的处理</h1><h2 id="信息标记的三种形式"><a href="#信息标记的三种形式" class="headerlink" title="信息标记的三种形式"></a>信息标记的三种形式</h2><center><font color="red" size="5">XML JSON YAML</font></center><ul><li><p>XML (eXtensible Markup Language)<br><br>  XML通过标签进行信息标记</p></li><li><p>JSON (JavaScript Object Notation)<br><br>  有类型的键值对key:value<br><br>  双引号表示字符串,数字直接写<br><br>  例如<br></p>  <figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">"name":"beijing"</span><br><span class="line">"name":["beijing","shoudu"]</span><br><span class="line">"name":&#123;</span><br><span class="line">"newName":"xiaoming"</span><br><span class="line">"oldName":"xiaohua"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>YAML (YAML Ain’t Markup Language)<br><br>  无类型键值对key:value<br>  如</p>  <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span><span class="string">beijing</span></span><br><span class="line"><span class="attr">name:</span>      <span class="comment"># 缩进表所属</span></span><br><span class="line"><span class="attr">newName:xiaoming</span></span><br><span class="line"><span class="attr">oldName:xiaohua</span></span><br><span class="line"><span class="attr">name:</span>      <span class="comment"># 表并列</span></span><br><span class="line"><span class="bullet">-</span><span class="string">beijing</span></span><br><span class="line"><span class="bullet">-</span><span class="string">shoudu</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="三种信息标记的比较"><a href="#三种信息标记的比较" class="headerlink" title="三种信息标记的比较"></a>三种信息标记的比较</h2><ul><li><p>XML 最早的通用信息标记语言，可扩展性好，但繁琐。</p><ul><li>Internet上的信息交互与传递。</li></ul></li><li><p>JSON 信息有类型，适合程序处理js，较XML简洁。</p><ul><li>移动应用云端和节点的信息通信，无注释。</li></ul></li><li><p>YAML 信息无类型，文本信息比例最高，可读性好。</p><ul><li>各类系统的配置文件，有注释易读 </li></ul></li></ul><h2 id="信息提取的一般方法"><a href="#信息提取的一般方法" class="headerlink" title="信息提取的一般方法"></a>信息提取的一般方法</h2><h3 id="1-完整解析信息的标记形式，再提取关键信息。"><a href="#1-完整解析信息的标记形式，再提取关键信息。" class="headerlink" title="1. 完整解析信息的标记形式，再提取关键信息。"></a>1. 完整解析信息的标记形式，再提取关键信息。</h3><font color="orange" size="3">XML JSON YAML</font><p>需要标记解析器，例如bs4库的标签树遍历</p><ul><li>优点:信息解析准确</li><li>缺点:提取过程繁琐，速度慢</li></ul><h3 id="2-无视标记形式，直接搜索关键信息。"><a href="#2-无视标记形式，直接搜索关键信息。" class="headerlink" title="2. 无视标记形式，直接搜索关键信息。"></a>2. 无视标记形式，直接搜索关键信息。</h3><font color="red" size="3">搜索</font><p>对信息的文本查找函数即可。</p><ul><li>优点:提取过程简洁，速度较快。</li><li>缺点:提取结果准确性与信息内容相关。</li></ul><h3 id="3-结合形式解析与搜索方法，提取关键信息"><a href="#3-结合形式解析与搜索方法，提取关键信息" class="headerlink" title="3. 结合形式解析与搜索方法，提取关键信息"></a>3. 结合形式解析与搜索方法，提取关键信息</h3><font color="green" size="3">XML JSON YAML 搜索</font><p>需要标记解析器及文本查找函数。</p><p>实例：<br><br>提取HTML中所有URL链接<br><br>思路：<br><br>1)搜索到所有 <code>&lt;a&gt;</code> 标签<br><br>2)解析 <code>&lt;a&gt;</code> 标签格式，提取href后的链接内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>demo = requests.get(<span class="string">"http://python123.io/ws/demo.html"</span>).text</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(demo,<span class="string">"html.parser"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> link <span class="keyword">in</span> soup.find_all(<span class="string">'a'</span>):</span><br><span class="line"><span class="meta">... </span>    print(link.get(<span class="string">'href'</span>))</span><br><span class="line"><span class="meta">... </span></span><br><span class="line">http://www.icourse163.org/course/BIT<span class="number">-268001</span></span><br><span class="line">http://www.icourse163.org/course/BIT<span class="number">-1001870001</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><h2 id="基于bs4库的HTML内容查找方法"><a href="#基于bs4库的HTML内容查找方法" class="headerlink" title="基于bs4库的HTML内容查找方法"></a>基于bs4库的HTML内容查找方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>demo = requests.get(<span class="string">"http://python123.io/ws/demo.html"</span>).text</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>demo</span><br><span class="line"><span class="string">'&lt;html&gt;&lt;head&gt;&lt;title&gt;This is a python demo page&lt;/title&gt;&lt;/head&gt;\r\n&lt;body&gt;\r\n&lt;p class="title"&gt;&lt;b&gt;The demo python introduces several python courses.&lt;/b&gt;&lt;/p&gt;\r\n&lt;p class="course"&gt;Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\r\n&lt;a href="http://www.icourse163.org/course/BIT-268001" class="py1" id="link1"&gt;Basic Python&lt;/a&gt; and &lt;a href="http://www.icourse163.org/course/BIT-1001870001" class="py2" id="link2"&gt;Advanced Python&lt;/a&gt;.&lt;/p&gt;\r\n&lt;/body&gt;&lt;/html&gt;'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(demo,<span class="string">"html.parser"</span>)</span><br></pre></td></tr></table></figure><p><code>&lt;&gt;.find_all(name,attrs,recursive,string,**kargs)</code></p><p>返回列表类型，存储查找的结果。</p><ul><li><code>name:对标签名称的检索字符串。</code></li></ul><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(<span class="string">'a'</span>)</span><br><span class="line">[&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt;, &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all([<span class="string">'a'</span>,<span class="string">'b'</span>])</span><br><span class="line">[&lt;b&gt;The demo python introduces several python courses.&lt;/b&gt;, &lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt;, &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> tag <span class="keyword">in</span> soup.find_all(<span class="literal">True</span>):</span><br><span class="line"><span class="meta">... </span>    print(tag.name)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line">html</span><br><span class="line">head</span><br><span class="line">title</span><br><span class="line">body</span><br><span class="line">p</span><br><span class="line">b</span><br><span class="line">p</span><br><span class="line">a</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re  <span class="comment"># 正则表达式库</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> tag <span class="keyword">in</span> soup.find_all(re.compile(<span class="string">'b'</span>)):</span><br><span class="line"><span class="meta">... </span>    print(tag.name)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line">body</span><br><span class="line">b</span><br></pre></td></tr></table></figure></blockquote><ul><li><code>attrs:对标签属性值的检索字符串，可标注属性检索。</code></li></ul><blockquote><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(<span class="string">'p'</span>,<span class="string">'course'</span>)</span><br><span class="line">[&lt;p class="course"&gt;Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:</span><br><span class="line">&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt; and &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;.&lt;/p&gt;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(id=<span class="string">'link1'</span>)</span><br><span class="line">[&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(id=<span class="string">'link'</span>)</span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(id=re.compile(<span class="string">'link'</span>))</span><br><span class="line">[&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt;, &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;]</span><br></pre></td></tr></table></figure></blockquote><ul><li><code>recursive:是否对子孙全部检索,默认True</code></li></ul><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(<span class="string">'a'</span>)</span><br><span class="line">[&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt;, &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(<span class="string">'a'</span>,recursive=<span class="literal">False</span>)</span><br><span class="line">[]</span><br></pre></td></tr></table></figure></blockquote><ul><li><code>string:&lt;&gt;...&lt;/&gt;中字符串区域的检索字符串。</code></li></ul><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup</span><br><span class="line">&lt;html&gt;&lt;head&gt;&lt;title&gt;This is a python demo page&lt;/title&gt;&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;p class="title"&gt;&lt;b&gt;The demo python introduces several python courses.&lt;/b&gt;&lt;/p&gt;</span><br><span class="line">&lt;p class="course"&gt;Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:</span><br><span class="line">&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt; and &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;&lt;/html&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(string = <span class="string">"Basic Python"</span>)</span><br><span class="line">[<span class="string">'Basic Python'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(string = re.compile(<span class="string">"python"</span>))</span><br><span class="line">[<span class="string">'This is a python demo page'</span>, <span class="string">'The demo python introduces several python courses.'</span>]</span><br></pre></td></tr></table></figure></blockquote><p><strong>注：</strong></p><p><code>&lt;tag&gt;(..)</code>等价于 <code>&lt;tag&gt;.find_all(..)</code></p><p><code>soup(..)</code>等价于<code>soup.find_all(..)</code></p><h3 id="扩展方法"><a href="#扩展方法" class="headerlink" title="扩展方法"></a>扩展方法</h3><table><thead><tr><th>方法</th><th>说明</th></tr></thead><tbody><tr><td>&lt;&gt;.find()</td><td>搜索且只返回一个结果,字符串类型,同.find_all()参数</td></tr><tr><td>&lt;&gt;.find_parents()</td><td>在先辈节点中搜索,返回列表类型,同.find_all()参数</td></tr><tr><td>&lt;&gt;.find_parent()</td><td>在先辈节点中返回一个结果,字符串类型,同.find()参数</td></tr><tr><td>&lt;&gt;.find_next_siblings()</td><td>在后续平行节点中搜索,返回列表类型,同.find_all()参数</td></tr><tr><td>&lt;&gt;.find_next_sibling()</td><td>在后续平行节点中返回一个结果,字符串类型,同.find参数</td></tr><tr><td>&lt;&gt;.find_previous_siblings()</td><td>在前序平行节点中搜索,返回列表类型,同.find_all()参数</td></tr><tr><td>&lt;&gt;.find_previous_sibling()</td><td>在前序平行节点中返回一个结果,字符串类型,同.find()参数</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> Spider </category>
          
          <category> Requests </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬虫学习笔记(二)</title>
      <link href="/2017/03/07/request3/"/>
      <url>/2017/03/07/request3/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>Beautiful Soup 库</b></font></center><a id="more"></a><h1 id="Beautiful-Soup-库"><a href="#Beautiful-Soup-库" class="headerlink" title="Beautiful Soup 库"></a>Beautiful Soup 库</h1><p>demo  html: <a href="http://python123.io/ws/demo.html" target="_blank" rel="noopener">http://python123.io/ws/demo.html</a></p><p>这里是基本用法示例。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.get(<span class="string">"http://python123.io/ws/demo.html"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.text</span><br><span class="line"><span class="string">'&lt;html&gt;&lt;head&gt;&lt;title&gt;This is a python demo page&lt;/title&gt;&lt;/head&gt;\r\n&lt;body&gt;\r\n&lt;p class="title"&gt;&lt;b&gt;The demo python introduces several python courses.&lt;/b&gt;&lt;/p&gt;\r\n&lt;p class="course"&gt;Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\r\n&lt;a href="http://www.icourse163.org/course/BIT-268001" class="py1" id="link1"&gt;Basic Python&lt;/a&gt; and &lt;a href="http://www.icourse163.org/course/BIT-1001870001" class="py2" id="link2"&gt;Advanced Python&lt;/a&gt;.&lt;/p&gt;\r\n&lt;/body&gt;&lt;/html&gt;'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>demo = r.text</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(demo,<span class="string">'html.parser'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(soup.prettify())</span><br><span class="line">&lt;html&gt;</span><br><span class="line"> &lt;head&gt;</span><br><span class="line">  &lt;title&gt;</span><br><span class="line">   This <span class="keyword">is</span> a python demo page</span><br><span class="line">  &lt;/title&gt;</span><br><span class="line"> &lt;/head&gt;</span><br><span class="line"> &lt;body&gt;</span><br><span class="line">  &lt;p class="title"&gt;</span><br><span class="line">   &lt;b&gt;</span><br><span class="line">    The demo python introduces several python courses.</span><br><span class="line">   &lt;/b&gt;</span><br><span class="line">  &lt;/p&gt;</span><br><span class="line">  &lt;p class="course"&gt;</span><br><span class="line">   Python <span class="keyword">is</span> a wonderful general-purpose programming language. You can learn Python <span class="keyword">from</span> novice to professional by tracking the following courses:</span><br><span class="line">   &lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;</span><br><span class="line">    Basic Python</span><br><span class="line">   &lt;/a&gt;</span><br><span class="line">   <span class="keyword">and</span></span><br><span class="line">   &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;</span><br><span class="line">    Advanced Python</span><br><span class="line">   &lt;/a&gt;</span><br><span class="line">   .</span><br><span class="line">  &lt;/p&gt;</span><br><span class="line"> &lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(<span class="string">'&lt;p&gt;data&lt;/p&gt;'</span>,<span class="string">'html.parser'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(soup.prettify())</span><br><span class="line">&lt;p&gt;</span><br><span class="line"> data</span><br><span class="line">&lt;/p&gt;</span><br></pre></td></tr></table></figure><h2 id="Beautiful-Soup库的理解"><a href="#Beautiful-Soup库的理解" class="headerlink" title="Beautiful Soup库的理解"></a>Beautiful Soup库的理解</h2><h3 id="Beautiful-Soup库的引用"><a href="#Beautiful-Soup库的引用" class="headerlink" title="Beautiful Soup库的引用"></a>Beautiful Soup库的引用</h3><p>Beautiful Soup也叫beautifulsoup4或bs4</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> bs4</span><br></pre></td></tr></table></figure><h3 id="Beautiful-Soup类"><a href="#Beautiful-Soup类" class="headerlink" title="Beautiful Soup类"></a>Beautiful Soup类</h3><p>HTML/XML &lt;-&gt; 标签树 &lt;-&gt; Beautiful Soup类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(<span class="string">"&lt;html&gt;data&lt;/html&gt;"</span>, <span class="string">"html.parser"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup2 = BeautifulSoup(open(<span class="string">"D://demo.html"</span>), <span class="string">"html.parser"</span>)</span><br></pre></td></tr></table></figure><ul><li>BeautifulSoup对于一个HTML/XML文档的全部内容。</li></ul><h3 id="Beautiful-Soup库解析器"><a href="#Beautiful-Soup库解析器" class="headerlink" title="Beautiful Soup库解析器"></a>Beautiful Soup库解析器</h3><table><thead><tr><th>解析器</th><th>使用方法</th><th>条件</th></tr></thead><tbody><tr><td>bs4的HTML解析器</td><td>BeautifulSoup(mk,’html.parser)</td><td>安装bs4库</td></tr><tr><td>lxml的HTML解析器</td><td>BeautifulSoup(mk,’lxml’)</td><td>pip install lxml</td></tr><tr><td>lxml的XML解析器</td><td>BeautifulSoup(mk,’xml’)</td><td>pip install lxml</td></tr><tr><td>html5lib的解析器</td><td>BeautifulSoup(mk,’html5lib’)</td><td>pip install html5lib</td></tr></tbody></table><h3 id="Beautiful-Soup类的基本元素"><a href="#Beautiful-Soup类的基本元素" class="headerlink" title="Beautiful Soup类的基本元素"></a>Beautiful Soup类的基本元素</h3><table><thead><tr><th>基本元素</th><th>说明</th></tr></thead><tbody><tr><td>Tag</td><td>标签,最基本的信息组织单元,分别用&lt;&gt;&lt;/&gt;标明开头和结尾</td></tr><tr><td>Name</td><td>标签的名字,&lt;p&gt;…&lt;/p&gt;的名字是’p’,格式:&lt;tag&gt;.name</td></tr><tr><td>Attributes</td><td>标签的属性,字典形式组织,格式:&lt;tag&gt;.attrs</td></tr><tr><td>NavigableString</td><td>标签内非属性字符串,&lt;&gt;…&lt;/&gt;中字符串,格式&lt;tag&gt;.string</td></tr><tr><td>Comment</td><td>标签内字符串的注释部分,一种特殊的Comment类型</td></tr></tbody></table><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;p&gt;..&lt;/p&gt;:标签tag</span><br><span class="line"></span><br><span class="line">&lt;p class=&apos;title&apos;&gt;...&lt;/p&gt;</span><br><span class="line"> ↑    ↑</span><br><span class="line">名称  属性</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">r = requests.get(<span class="string">"http://python123.io/ws/demo.html"</span>)</span><br><span class="line">demo = r.text</span><br><span class="line">soup = BeautifulSoup(demo, <span class="string">'html.parser'</span>)</span><br><span class="line">print(soup.a.name)</span><br><span class="line">print(soup.a.parent.name)</span><br><span class="line">print(soup.a.parent.parent.name)</span><br><span class="line">tag = soup.a</span><br><span class="line">print(tag.attrs)</span><br><span class="line">print(tag.attrs[<span class="string">'class'</span>])</span><br><span class="line">print(tag.attrs[<span class="string">'href'</span>])</span><br><span class="line">print(type(tag.attrs))</span><br><span class="line">print(type(tag))</span><br><span class="line">print(tag.string)</span><br><span class="line">print(soup.p)</span><br><span class="line">print(soup.p.string)</span><br><span class="line">print(type(soup.p.string))</span><br></pre></td></tr></table></figure></blockquote><h3 id="标签树的下行遍历"><a href="#标签树的下行遍历" class="headerlink" title="标签树的下行遍历"></a>标签树的下行遍历</h3><table><thead><tr><th>属性</th><th>说明</th></tr></thead><tbody><tr><td>.contents</td><td>子节点的列表，将<tag>所有儿子结点存入列表</tag></td></tr><tr><td>.children</td><td>子节点的迭代类型，与.contents类似，用于循环遍历</td></tr><tr><td>.descendants</td><td>子孙节点的迭代类型，包含所有子孙节点，用于循环遍历</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> soup.body.children:   <span class="comment"># 遍历儿子节点</span></span><br><span class="line">print(child)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> soup.body.descendants:<span class="comment"># 遍历子孙节点</span></span><br><span class="line">print(child)</span><br></pre></td></tr></table></figure><h3 id="标签树的上行遍历"><a href="#标签树的上行遍历" class="headerlink" title="标签树的上行遍历"></a>标签树的上行遍历</h3><table><thead><tr><th>属性</th><th>说明</th></tr></thead><tbody><tr><td>.parent</td><td>节点的父亲标签</td></tr><tr><td>.parents</td><td>节点的先辈标签的迭代类型，用于循环遍历先辈节点</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(demo, <span class="string">"html.parser"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> parent <span class="keyword">in</span> soup.a.parents:</span><br><span class="line">  <span class="keyword">if</span> parent <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"> print(parent)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line"> print(parent.name)</span><br><span class="line"><span class="comment"># 遍历所有先辈节点，包括soup本身，所以要区别判断</span></span><br></pre></td></tr></table></figure><h3 id="标签树的平行遍历"><a href="#标签树的平行遍历" class="headerlink" title="标签树的平行遍历"></a>标签树的平行遍历</h3><table><thead><tr><th>属性</th><th>说明</th></tr></thead><tbody><tr><td>.next_sibling</td><td>返回按照HTML文本顺序的下一个平行节点标签</td></tr><tr><td>.previous_sibling</td><td>返回按照HTML文本顺序的上一个平行节点标签</td></tr><tr><td>.next_siblings</td><td>迭代类型，返回按照HTML文本顺序的后续所有平行节点标签</td></tr><tr><td>.previous_siblings</td><td>迭代类型，返回按照HTML文本顺序的前续所有平行节点标签</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> sibling <span class="keyword">in</span> soup.a.next_siblings: <span class="comment"># 遍历后续节点</span></span><br><span class="line">print(sibling)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sibling <span class="keyword">in</span> soup.a.previous_siblings:  <span class="comment"># 遍历前续节点</span></span><br><span class="line">print(sibling)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> Spider </category>
          
          <category> Requests </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬虫学习笔记(一)</title>
      <link href="/2017/03/03/request2/"/>
      <url>/2017/03/03/request2/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>Requests库</b></font></center><a id="more"></a><h1 id="Requests库"><a href="#Requests库" class="headerlink" title="Requests库"></a>Requests库</h1><table><thead><tr><th>Requests库的7个主要方法</th><th>功能</th></tr></thead><tbody><tr><td>requests.request()</td><td>构造一个请求，支撑以下各方法的基础方法</td></tr><tr><td>requests.get()</td><td>获取HTML网页的主要方法，对于与HTTP的GET</td></tr><tr><td>requests.head()</td><td>获取HTML网页头信息的方法，对应于HTTP的HEAD</td></tr><tr><td>requests.post()</td><td>向HTML网页提交POST请求的方法，对应于HTTP的POST</td></tr><tr><td>requests.put()</td><td>向HTML网页提交PUT请求的方法，对应于HTTP的PUT</td></tr><tr><td>requests.patch()</td><td>向HTML网页提交局部修改请求，对应于HTTP的PATCH</td></tr><tr><td>requests.delete()</td><td>向HTML页面提交删除请求，对应于HTTP的DELETE</td></tr></tbody></table><h2 id="get方法"><a href="#get方法" class="headerlink" title="get方法"></a>get方法</h2><p><font size="4"><code>r = requests.get(url, params=None, **kwargs)</code><br> </font></p><table><thead><tr><th align="center">参数</th><th>说明</th></tr></thead><tbody><tr><td align="center">url:</td><td>拟获取页面的url链接</td></tr><tr><td align="center">params:</td><td>url中的额外参数,字典或字节流格式,可选</td></tr><tr><td align="center">**kwargs:</td><td>12个控制访问的参数</td></tr></tbody></table><table><thead><tr><th>Response对象属性</th><th>功能</th></tr></thead><tbody><tr><td>r.status_code</td><td>HTTP请求的返回状态,200表示连接成功,404表示失败</td></tr><tr><td>r.text</td><td>HTTP响应内容的字符串形式,即,url对应的页面内容</td></tr><tr><td>r.encoding</td><td>从HTTP header中猜测的响应内容编码方式</td></tr><tr><td>r.apparent_encoding</td><td>从内容中分析出的响应内容编码方式(备选编码方式)</td></tr><tr><td>r.content</td><td>HTTP响应内容的二进制形式</td></tr></tbody></table><p><font size="5">理解Response的编码</font></p><blockquote><table><thead><tr><th>属性</th><th>功能</th></tr></thead><tbody><tr><td>r.encoding</td><td>从HTTP header中猜测的响应内容编码方式</td></tr><tr><td>r.apparent_encoding</td><td>从内容中分析出的响应内容编码方式(备选编码方式)</td></tr></tbody></table></blockquote><blockquote><p><code>r.encoding: 如果header中不存在charset,则默认编码为ISO-8859-1</code></p></blockquote><h2 id="爬取网页的通用代码框架"><a href="#爬取网页的通用代码框架" class="headerlink" title="爬取网页的通用代码框架"></a>爬取网页的通用代码框架</h2><h3 id="一些基本的Requests中的异常"><a href="#一些基本的Requests中的异常" class="headerlink" title="一些基本的Requests中的异常"></a>一些基本的Requests中的异常</h3><table><thead><tr><th>异常</th><th>说明</th></tr></thead><tbody><tr><td>requests.ConnectionError</td><td>网络连接错误异常,如DNS查询失败、拒绝连接等</td></tr><tr><td>requests.HTTPError</td><td>HTTP错误异常</td></tr><tr><td>requests.URLRequired</td><td>URL缺失异常</td></tr><tr><td>requests.TooManyRedirects</td><td>超过最大重定向次数,产生重定向异常</td></tr><tr><td>requests.ConnectTimeout</td><td>连接远程服务器超时异常</td></tr><tr><td>requests.Timeout</td><td>请求URL超时,产生超时异常</td></tr><tr><td>r.raise_for_status()</td><td>如果不是200,产生异常requests.HTTPError</td></tr></tbody></table><center><font size="4">通用代码框架:</font></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, timeout=<span class="number">30</span>)</span><br><span class="line">        r.raise_for_status()  <span class="comment"># 如果状态不是200,引发HTTPError异常</span></span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"产生异常"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    url = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    print(getHTMLText(url))</span><br></pre></td></tr></table></figure><h2 id="HTTP协议"><a href="#HTTP协议" class="headerlink" title="HTTP协议"></a>HTTP协议</h2><p><font size="3"><b>超文本传输协议（HTTP，HyperText Transfer Protocol)<br><br>HTTP是一个基于“请求与响应”模式的、无状态的应用层协议。<br><br>HTTP协议采用URL作为定位网络资源的标识。<br><br>URL格式 <a href="http://host[" target="_blank" rel="noopener">http://host[</a><font color="orange">:port</font>][<font color="orange">path</font>]<br><br><font color="orange">host:</font> 合法的Internet主机域名或IP地址<br><br><font color="orange">port:</font> 端口号,缺失端口为80<br><br><font color="orange">path:</font> 请求资源的路径</b></font></p><center>HTTP协议对资源的操作</center><table><thead><tr><th>方法</th><th>说明</th></tr></thead><tbody><tr><td>GET</td><td>请求获取URL位置的资源</td></tr><tr><td>HEAD</td><td>请求获取URL位置的资源的响应消息报告,即获得该资源的头部消息</td></tr><tr><td>POST</td><td>请求向URL位置的资源后附加新的数据</td></tr><tr><td>PUT</td><td>请求想URL位置存储一个资源,覆盖URL位置的资源</td></tr><tr><td>PATCH</td><td>请求局部更新URL位置的资源,即改变该处资源的部分内容</td></tr><tr><td>DELETE</td><td>请求删除URL位置存储的资源</td></tr></tbody></table><p>理解PATCH和PUT的区别</p><p>假设URL位置有一组数据UserInfo,包括UserID、UserName等20个字段。<br>需求: 用户修改了UserName,其他不变。</p><ul><li>采用PATCH,仅向URL提交UserName的局部更新请求。</li><li>采用PUT,必须将所有20个字段一并提交到URL,未提交的字段被删除</li></ul><p>PATCH的最主要好处: 节省网络带宽</p><p>HTTP协议与Requests库是一一对应的</p><p>Requests库的post()方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>payload = &#123;<span class="string">'key1'</span>:<span class="string">'value1'</span>,<span class="string">'key2'</span>:<span class="string">'value2'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.post(<span class="string">'http://httpbin.org/post'</span>,data = payload)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(r.text)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"args"</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">"data"</span>: <span class="string">""</span>, </span><br><span class="line">  <span class="string">"files"</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">"form"</span>: &#123;</span><br><span class="line">    <span class="string">"key1"</span>: <span class="string">"value1"</span>, </span><br><span class="line">    <span class="string">"key2"</span>: <span class="string">"value2"</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">"headers"</span>: &#123;</span><br><span class="line">    <span class="string">"Accept"</span>: <span class="string">"*/*"</span>, </span><br><span class="line">    <span class="string">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>, </span><br><span class="line">    <span class="string">"Content-Length"</span>: <span class="string">"23"</span>, </span><br><span class="line">    <span class="string">"Content-Type"</span>: <span class="string">"application/x-www-form-urlencoded"</span>, </span><br><span class="line">    <span class="string">"Host"</span>: <span class="string">"httpbin.org"</span>, </span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"python-requests/2.11.1"</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">"json"</span>: null, </span><br><span class="line">  <span class="string">"origin"</span>: <span class="string">"116.7.245.180"</span>, </span><br><span class="line">  <span class="string">"url"</span>: <span class="string">"http://httpbin.org/post"</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 向URL POST一个字典 自动编码为form(表单)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.post(<span class="string">'http://httpbin.org/post'</span>,data=<span class="string">'ABC'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(r.text)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"args"</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">"data"</span>: <span class="string">"ABC"</span>, </span><br><span class="line">  <span class="string">"files"</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">"form"</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">"headers"</span>: &#123;</span><br><span class="line">    <span class="string">"Accept"</span>: <span class="string">"*/*"</span>, </span><br><span class="line">    <span class="string">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>, </span><br><span class="line">    <span class="string">"Content-Length"</span>: <span class="string">"3"</span>, </span><br><span class="line">    <span class="string">"Host"</span>: <span class="string">"httpbin.org"</span>, </span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"python-requests/2.11.1"</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">"json"</span>: null, </span><br><span class="line">  <span class="string">"origin"</span>: <span class="string">"116.7.245.180"</span>, </span><br><span class="line">  <span class="string">"url"</span>: <span class="string">"http://httpbin.org/post"</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 向URL POST一个字符串 自动编码为data</span></span><br></pre></td></tr></table></figure><h2 id="Requests库的各个方法"><a href="#Requests库的各个方法" class="headerlink" title="Requests库的各个方法"></a>Requests库的各个方法</h2><h3 id="requests-request-method-url-kwargs"><a href="#requests-request-method-url-kwargs" class="headerlink" title="requests.request(method, url, **kwargs)"></a>requests.request(method, url, **kwargs)</h3><ul><li>method: 请求方式,对应get/put/post等7种</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">requests.request(<span class="string">'GET'</span>, url, **kwargs)</span><br><span class="line">requests.request(<span class="string">'HEAD'</span>, url, **kwargs)</span><br><span class="line">requests.request(<span class="string">'POST'</span>, url, **kwargs)</span><br><span class="line">requests.request(<span class="string">'PUT'</span>, url, **kwargs)</span><br><span class="line">requests.request(<span class="string">'PATCH'</span>, url, **kwargs)</span><br><span class="line">requests.request(<span class="string">'DELETE'</span>, url, **kwargs)</span><br><span class="line">requests.request(<span class="string">'OPTIONS'</span>, url, **kwargs)</span><br></pre></td></tr></table></figure><ul><li><p>url: 拟获取页面的url链接</p></li><li><p>**kwargs: 控制访问的参数,共13个,均为可选项</p></li></ul><table><thead><tr><th>参数</th><th>功能</th></tr></thead><tbody><tr><td>params</td><td>字典或字节序列,作为参数增加到url中</td></tr><tr><td>data</td><td>字典、字节序列或文件对象,作为Request的内容</td></tr><tr><td>json</td><td>JSON格式的数据,作为Request的内容</td></tr><tr><td>headers</td><td>字典,HTTP定制头</td></tr><tr><td>cookies</td><td>字典或CookieJar,Request中的cookie</td></tr><tr><td>auth</td><td>元组,支持HTTP认证功能</td></tr><tr><td>files</td><td>字典类型,传输文件</td></tr><tr><td>timeout</td><td>设定超时时间,秒为单位</td></tr><tr><td>proxies</td><td>字典类型,设定访问代理服务器,可以增加登陆认证</td></tr><tr><td>allow_redirects</td><td>True/False,均默认为True,重定向开关</td></tr><tr><td>stream</td><td>True/False,默认为True,获取内容立即下载开关</td></tr><tr><td>verify</td><td>True/False,默认为True,认证SSL证书开关</td></tr><tr><td>cert</td><td>本地SSL证书路径</td></tr></tbody></table><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>fs = &#123;<span class="string">'file'</span>: open(<span class="string">'data.xls'</span>, <span class="string">'rb'</span>)&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.request(<span class="string">'POST'</span>,<span class="string">'http://python123.io/ws'</span>, files = fs)</span><br><span class="line"> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pxs = &#123;<span class="string">'http'</span>: <span class="string">'http://user:pass@10.10.10.1:1234'</span></span><br><span class="line">   <span class="string">'https'</span>: <span class="string">'https://0.10.10.1:1234'</span>, proxies = pxs&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.request(<span class="string">'GET'</span>,<span class="string">'https://www.baidu.com'</span>, proxies = pxs)</span><br></pre></td></tr></table></figure></blockquote><h3 id="requests-get-url-params-None-kwargs"><a href="#requests-get-url-params-None-kwargs" class="headerlink" title="requests.get(url, params=None, **kwargs)"></a>requests.get(url, params=None, **kwargs)</h3><ul><li><p>url: 拟获取页面的url链接</p></li><li><p>params: url中的额外参数,字典或字节流格式,可选</p></li><li><p>**kwargs: 12个控制访问的参数 (除了params)</p></li></ul><h3 id="requests-head-url-kwargs"><a href="#requests-head-url-kwargs" class="headerlink" title="requests.head(url, **kwargs)"></a>requests.head(url, **kwargs)</h3><ul><li><p>url: 拟获取页面的url链接</p></li><li><p>**kwargs: 13个控制访问的参数 </p></li></ul><h3 id="requests-post-url-data-None-json-None-kwargs"><a href="#requests-post-url-data-None-json-None-kwargs" class="headerlink" title="requests.post(url, data=None, json=None, **kwargs)"></a>requests.post(url, data=None, json=None, **kwargs)</h3><ul><li><p>url: 拟更新页面的url链接</p></li><li><p>data: 字典、字节序列或文件,Request的内容</p></li><li><p>json: JSON格式的数据,Request的内容</p></li><li><p>**kwargs: 11个控制访问的参数 </p></li></ul><h3 id="requests-put-url-data-None-kwargs"><a href="#requests-put-url-data-None-kwargs" class="headerlink" title="requests.put(url, data=None, **kwargs)"></a>requests.put(url, data=None, **kwargs)</h3><ul><li><p>url: 拟更新页面的url链接</p></li><li><p>data: 字典、字节序列或文件,Request的内容</p></li><li><p>**kwargs: 12个控制访问的参数 </p></li></ul><h3 id="requests-patch-url-data-None-kwargs"><a href="#requests-patch-url-data-None-kwargs" class="headerlink" title="requests.patch(url, data=None, **kwargs)"></a>requests.patch(url, data=None, **kwargs)</h3><ul><li><p>url: 拟更新页面的url链接</p></li><li><p>data: 字典、字节序列或文件,Request的内容</p></li><li><p>**kwargs: 12个控制访问的参数 </p></li></ul><h3 id="requests-delete-url-kwargs"><a href="#requests-delete-url-kwargs" class="headerlink" title="requests.delete(url, **kwargs)"></a>requests.delete(url, **kwargs)</h3><ul><li><p>url: 拟删除页面的url链接</p></li><li><p>**kwargs: 13个控制访问的参数 </p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> Spider </category>
          
          <category> Requests </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python Requests库学习笔记</title>
      <link href="/2017/03/01/request/"/>
      <url>/2017/03/01/request/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>Requests库学习笔记</b></font></center><a id="more"></a><h1 id="Requests库学习笔记"><a href="#Requests库学习笔记" class="headerlink" title="Requests库学习笔记"></a>Requests库学习笔记</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><a href="http://docs.python-requests.org/en/master/" target="_blank" rel="noopener">Requests: HTTP for Humans</a><br></p><p>该库作者(<a href="https://www.kennethreitz.org" target="_blank" rel="noopener">KENNETH REITZ</a>)</p><p><a href="http://httpbin.org" target="_blank" rel="noopener">http://httpbin.org</a></p><h2 id="利用-GitHub-API-学习"><a href="#利用-GitHub-API-学习" class="headerlink" title="利用 GitHub API 学习"></a>利用 GitHub API 学习</h2><p><a href="https://developer.github.com" target="_blank" rel="noopener">https://developer.github.com</a></p><h3 id="请求方法"><a href="#请求方法" class="headerlink" title="请求方法"></a>请求方法</h3><font color="red" size="2"><ul><li>GET: 查看资源<br></li><li>POST: 增加资源</li></ul></font><font color="blue" size="2"><ul><li>PUT: 修改资料<br> </li><li>DELETE: 删除资源</li></ul></font><font color="orange" size="2"><ul><li>HEAD: 查看响应头<br> </li><li>OPTIONS: 查看可用请求方法</li></ul></font><h3 id="带参数的请求"><a href="#带参数的请求" class="headerlink" title="带参数的请求"></a>带参数的请求</h3><ol><li><p>URL Parameters: URL参数</p><ul><li><a href="https://list.tmall.com/search_product.html" target="_blank" rel="noopener">https://list.tmall.com/search_product.html</a>?<font color="red">cat=50514037&amp;…</font></li><li>params: requests.get(url,<code>params</code>={‘key1’:’value1’})</li></ul></li><li><p>表单参数提交：</p><ul><li>Content-Type: application/x-www-form-urlencoded</li><li>内容: key1=value1&amp;key2=value2</li><li>requests.post(url,data={‘key1’:’value1’,’key2’:’value2’})</li></ul></li><li><p>json参数提交：</p><ul><li>Content-Type: application/json</li><li>内容: ‘{‘key1’:’value1’,’key2’:’value2’}’</li><li>requests.post(url,json={‘key1’:’value1’,’key2’:’value2’})</li></ul></li></ol><h3 id="请求异常处理"><a href="#请求异常处理" class="headerlink" title="请求异常处理"></a>请求异常处理</h3><p><code>requests库</code>包括的异常都继承自<code>requests.exceptions.RequestException</code>类<br></p><center>这是request库包含的所有异常</center><ul><li>BaseHTTPError</li><li>ChunkedEncodingError</li><li>ConnectTimeout</li><li>ConnectionError</li><li>ContentDecodingError</li><li>HTTPError</li><li>InvalidSchema</li><li>InvalidURL</li><li>MissingSchema</li><li>ProxyError</li><li>ReadTimeout</li><li>RequestException</li><li>RetryError</li><li>SSLError</li><li>StreamConsumedError</li><li>Timeout</li><li>TooMangRedirects</li><li>URLRequired</li></ul><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">其中常见的</span><br><span class="line">ConnectionError  由于网络原因，无法建立连接。</span><br><span class="line">HTTPError 如果响应的状态码不为<span class="number">200</span>,Response.<span class="keyword">raise</span>\_for\_status()会抛出HTTPError 异常。</span><br><span class="line">Timeout 超时异常。</span><br><span class="line">TooManyRedirects 若请求超过了设定的最大重定向次数，则会抛出一TooManyRedirects 异常。</span><br></pre></td></tr></table></figure></blockquote><ul><li>请求超时处理<br></li></ul><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">requests.get(url,timeout=(<span class="number">3</span>,<span class="number">7</span>))   <span class="comment"># TCP协议三次握手,对于1过程等待3秒,2过程等待7秒</span></span><br><span class="line">requests.get(url,timeout=<span class="number">10</span>)   <span class="comment"># 总过程10秒</span></span><br><span class="line"></span><br><span class="line">&gt; requests.exceptions.Timeout</span><br></pre></td></tr></table></figure></blockquote><ul><li>HTTPError</li></ul><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; requests.exceptions.HTTPError</span><br></pre></td></tr></table></figure></blockquote><h3 id="自定义-Request"><a href="#自定义-Request" class="headerlink" title="自定义 Request"></a>自定义 Request</h3><p>通过Request Session实现</p><blockquote></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Request, Session</span><br></pre></td></tr></table></figure><h3 id="这是实现之前的一个实例"><a href="#这是实现之前的一个实例" class="headerlink" title="这是实现之前的一个实例"></a>这是实现之前的一个实例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这是调用github一些的api写的实例</span></span><br><span class="line"><span class="comment">#! /usr/bin/.env python3</span></span><br><span class="line"><span class="comment">#-*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://api.github.com'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_uri</span><span class="params">(endpoint)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'/'</span>.join([url, endpoint])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">better_print</span><span class="params">(json_str)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(json.loads(json_str), indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">request_method</span><span class="params">()</span>:</span></span><br><span class="line">    response = requests.get(build_uri(<span class="string">'user/emails'</span>),</span><br><span class="line">                            auth=(<span class="string">'usrname'</span>, <span class="string">'password'</span>)) <span class="comment"># GitHub的用户名及密码</span></span><br><span class="line">    print(better_print(response.text))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">params_request</span><span class="params">()</span>:</span></span><br><span class="line">    response = requests.get(build_uri(<span class="string">'users'</span>), params=&#123;<span class="string">'since'</span>: <span class="number">11</span>&#125;)</span><br><span class="line">    print(better_print(response.text))</span><br><span class="line">    print(response.headers)</span><br><span class="line">    print(response.url)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">json_request</span><span class="params">()</span>:</span></span><br><span class="line">    response = requests.patch(</span><br><span class="line">        build_uri(<span class="string">'user'</span>),</span><br><span class="line">        auth=(<span class="string">'usrname'</span>, <span class="string">'password'</span>), <span class="comment"># GitHub的用户名及密码</span></span><br><span class="line">        json=&#123;<span class="string">'name'</span>: <span class="string">'xxx'</span>,</span><br><span class="line">              <span class="string">'email'</span>: <span class="string">'111@qq.com'</span>&#125;) <span class="comment"># 想改的名字和邮箱</span></span><br><span class="line">    print(better_print(response.text))</span><br><span class="line">    print(response.headers)</span><br><span class="line">    print(response.body)</span><br><span class="line">    print(response.status_code)</span><br><span class="line">    print(response.reason)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeout_request</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(build_uri(<span class="string">'user/emails'</span>), timeout=<span class="number">0.1</span>)</span><br><span class="line">        response.raise_for_status()</span><br><span class="line">    <span class="keyword">except</span> requests.exceptions.Timeout <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">    <span class="keyword">except</span> requests.exceptions.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(requests.text)</span><br><span class="line">        print(response.text)</span><br><span class="line">        print(response.status_code)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hard_requests</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">from</span> requests <span class="keyword">import</span> Request, Session</span><br><span class="line">    s = Session()</span><br><span class="line">    headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">'fake1.3.4'</span>&#125;  <span class="comment"># 伪造UA</span></span><br><span class="line">    req = requests.Request(</span><br><span class="line">        <span class="string">'GET'</span>,</span><br><span class="line">        build_uri(<span class="string">'user/emails'</span>),</span><br><span class="line">        auth=(<span class="string">'usrname'</span>, <span class="string">'password'</span>), <span class="comment"># GitHub的用户名及密码</span></span><br><span class="line">        headers=headers)</span><br><span class="line">    prepped = req.prepare()</span><br><span class="line">    print(prepped.body)</span><br><span class="line">    print(prepped.headers)</span><br><span class="line"></span><br><span class="line">    resp = s.send(prepped, timeout=<span class="number">5</span>)</span><br><span class="line">    print(resp.status_code)</span><br><span class="line">    print(resp.request.headers)</span><br><span class="line">    print(resp.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    hard_requests()</span><br></pre></td></tr></table></figure><h3 id="响应基本API"><a href="#响应基本API" class="headerlink" title="响应基本API"></a>响应基本API</h3><p><a href="http://baike.baidu.com/link?url=OSkx-CgOXOO8O7TLAPJ7Zy-bXYfOCkVVwYIINzepYb2JugoMU7LzQevcjLwXWPeqfQr92kSihgvqEY82bRPdqbM4OZgK_vjl3Nm7ZQfsX8L6wPolI1Ei5U1dEhboAP7n" target="_blank" rel="noopener">百度百科：HTTP状态码</a></p><p>1XX:消息 <br>2XX:成功 <br>3XX:重定向 <br>4XX:请求(客户端)错误 <br>5XX:服务器错误 <br>600:源站没有返回响应头部，只返回实体内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response = requests.get(<span class="string">'https://api.github.com'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.status_code</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.reason</span><br><span class="line"><span class="string">'OK'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.headers</span><br><span class="line">&#123;<span class="string">'X-GitHub-Media-Type'</span>: <span class="string">'github.v3; format=json'</span>, <span class="string">'X-RateLimit-Reset'</span>: <span class="string">'1488289215'</span>, <span class="string">'Content-Type'</span>: <span class="string">'application/json; charset=utf-8'</span>, <span class="string">'ETag'</span>: <span class="string">'W/"7dc470913f1fe9bb6c7355b50a0737bc"'</span>, <span class="string">'Date'</span>: <span class="string">'Tue, 28 Feb 2017 12:40:15 GMT'</span>, <span class="string">'Content-Security-Policy'</span>: <span class="string">"default-src 'none'"</span>, <span class="string">'Transfer-Encoding'</span>: <span class="string">'chunked'</span>, <span class="string">'Server'</span>: <span class="string">'GitHub.com'</span>, <span class="string">'Content-Encoding'</span>: <span class="string">'gzip'</span>, <span class="string">'Cache-Control'</span>: <span class="string">'public, max-age=60, s-maxage=60'</span>, <span class="string">'X-Content-Type-Options'</span>: <span class="string">'nosniff'</span>, <span class="string">'X-XSS-Protection'</span>: <span class="string">'1; mode=block'</span>, <span class="string">'Access-Control-Expose-Headers'</span>: <span class="string">'ETag, Link, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval'</span>, <span class="string">'X-GitHub-Request-Id'</span>: <span class="string">'6E6D:0DDE:61E3A4:7ACD2E:58B56FA3'</span>, <span class="string">'Strict-Transport-Security'</span>: <span class="string">'max-age=31536000; includeSubdomains; preload'</span>, <span class="string">'X-Served-By'</span>: <span class="string">'07ff1c8a09e44b62e277fae50a1b1dc4'</span>, <span class="string">'Access-Control-Allow-Origin'</span>: <span class="string">'*'</span>, <span class="string">'X-Frame-Options'</span>: <span class="string">'deny'</span>, <span class="string">'Vary'</span>: <span class="string">'Accept, Accept-Encoding'</span>, <span class="string">'X-RateLimit-Remaining'</span>: <span class="string">'59'</span>, <span class="string">'Status'</span>: <span class="string">'200 OK'</span>, <span class="string">'X-RateLimit-Limit'</span>: <span class="string">'60'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.url</span><br><span class="line"><span class="string">'https://api.github.com/'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.history</span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response = requests.get(<span class="string">'http://api.github.com'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.history</span><br><span class="line">[&lt;Response [<span class="number">301</span>]&gt;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.elapsed</span><br><span class="line">datetime.timedelta(<span class="number">0</span>, <span class="number">1</span>, <span class="number">664629</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.request</span><br><span class="line">&lt;PreparedRequest [GET]&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response</span><br><span class="line">&lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.encoding</span><br><span class="line"><span class="string">'utf-8'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.raw.read(<span class="number">10</span>)</span><br><span class="line"><span class="string">b''</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.json()[<span class="string">'team_url'</span>]</span><br><span class="line"><span class="string">'https://api.github.com/teams'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.content</span><br><span class="line"><span class="string">b'&#123;"current_user_url":"https://api.github.com/user","current_user_authorizations_html_url":"https://github.com/settings/connections/applications&#123;/client_id&#125;","authorizations_url":"https://api.github.com/authorizations","code_search_url":"https://api.github.com/search/code?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;","commit_search_url":"https://api.github.com/search/commits?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;","emails_url":"https://api.github.com/user/emails","emojis_url":"https://api.github.com/emojis","events_url":"https://api.github.com/events","feeds_url":"https://api.github.com/feeds","followers_url":"https://api.github.com/user/followers","following_url":"https://api.github.com/user/following&#123;/target&#125;","gists_url":"https://api.github.com/gists&#123;/gist_id&#125;","hub_url":"https://api.github.com/hub","issue_search_url":"https://api.github.com/search/issues?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;","issues_url":"https://api.github.com/issues","keys_url":"https://api.github.com/user/keys","notifications_url":"https://api.github.com/notifications","organization_repositories_url":"https://api.github.com/orgs/&#123;org&#125;/repos&#123;?type,page,per_page,sort&#125;","organization_url":"https://api.github.com/orgs/&#123;org&#125;","public_gists_url":"https://api.github.com/gists/public","rate_limit_url":"https://api.github.com/rate_limit","repository_url":"https://api.github.com/repos/&#123;owner&#125;/&#123;repo&#125;","repository_search_url":"https://api.github.com/search/repositories?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;","current_user_repositories_url":"https://api.github.com/user/repos&#123;?type,page,per_page,sort&#125;","starred_url":"https://api.github.com/user/starred&#123;/owner&#125;&#123;/repo&#125;","starred_gists_url":"https://api.github.com/gists/starred","team_url":"https://api.github.com/teams","user_url":"https://api.github.com/users/&#123;user&#125;","user_organizations_url":"https://api.github.com/user/orgs","user_repositories_url":"https://api.github.com/users/&#123;user&#125;/repos&#123;?type,page,per_page,sort&#125;","user_search_url":"https://api.github.com/search/users?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;"&#125;'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.text</span><br><span class="line"><span class="string">'&#123;"current_user_url":"https://api.github.com/user","current_user_authorizations_html_url":"https://github.com/settings/connections/applications&#123;/client_id&#125;","authorizations_url":"https://api.github.com/authorizations","code_search_url":"https://api.github.com/search/code?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;","commit_search_url":"https://api.github.com/search/commits?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;","emails_url":"https://api.github.com/user/emails","emojis_url":"https://api.github.com/emojis","events_url":"https://api.github.com/events","feeds_url":"https://api.github.com/feeds","followers_url":"https://api.github.com/user/followers","following_url":"https://api.github.com/user/following&#123;/target&#125;","gists_url":"https://api.github.com/gists&#123;/gist_id&#125;","hub_url":"https://api.github.com/hub","issue_search_url":"https://api.github.com/search/issues?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;","issues_url":"https://api.github.com/issues","keys_url":"https://api.github.com/user/keys","notifications_url":"https://api.github.com/notifications","organization_repositories_url":"https://api.github.com/orgs/&#123;org&#125;/repos&#123;?type,page,per_page,sort&#125;","organization_url":"https://api.github.com/orgs/&#123;org&#125;","public_gists_url":"https://api.github.com/gists/public","rate_limit_url":"https://api.github.com/rate_limit","repository_url":"https://api.github.com/repos/&#123;owner&#125;/&#123;repo&#125;","repository_search_url":"https://api.github.com/search/repositories?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;","current_user_repositories_url":"https://api.github.com/user/repos&#123;?type,page,per_page,sort&#125;","starred_url":"https://api.github.com/user/starred&#123;/owner&#125;&#123;/repo&#125;","starred_gists_url":"https://api.github.com/gists/starred","team_url":"https://api.github.com/teams","user_url":"https://api.github.com/users/&#123;user&#125;","user_organizations_url":"https://api.github.com/user/orgs","user_repositories_url":"https://api.github.com/users/&#123;user&#125;/repos&#123;?type,page,per_page,sort&#125;","user_search_url":"https://api.github.com/search/users?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;"&#125;'</span></span><br></pre></td></tr></table></figure><h2 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h2><h3 id="自动下载图片-文件"><a href="#自动下载图片-文件" class="headerlink" title="自动下载图片/文件"></a>自动下载图片/文件</h3><ol><li>浏览器模拟</li><li>构建request</li><li>读取流data</li><li>存入数据</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''demo:下载图片</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment">#-*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_image</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'User-Agent'</span>:</span><br><span class="line">        <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">    url = <span class="string">'https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1488296531013&amp;di=689048e17449c05f7d55698526a71ccc&amp;imgtype=0&amp;src=http%3A%2F%2Fpic.baike.soso.com%2Fp%2F20130515%2F20130515150325-379335333.jpg'</span></span><br><span class="line">    response = requests.get(url, headers=headers, stream=<span class="literal">True</span>)  <span class="comment"># 流传入，这种方法没有关闭流</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'demo.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> fd:</span><br><span class="line">        <span class="keyword">for</span> chunk <span class="keyword">in</span> response.iter_content(<span class="number">128</span>):</span><br><span class="line">            fd.write(chunk)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_image_improved</span><span class="params">()</span>:</span>  <span class="comment"># 这里使用了一个python的语法糖,来关闭流</span></span><br><span class="line">    <span class="comment"># 伪造headers信息</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'User-Agent'</span>:</span><br><span class="line">        <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 限定url,这是我们需要下载图片的地址</span></span><br><span class="line">    url = <span class="string">'https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1488296531013&amp;di=689048e17449c05f7d55698526a71ccc&amp;imgtype=0&amp;src=http%3A%2F%2Fpic.baike.soso.com%2Fp%2F20130515%2F20130515150325-379335333.jpg'</span></span><br><span class="line">    response = requests.get(url, headers=headers, stream=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">from</span> contextlib <span class="keyword">import</span> closing</span><br><span class="line">    <span class="keyword">with</span> closing(requests.get(url, headers=headers, stream=<span class="literal">True</span>)) <span class="keyword">as</span> response:</span><br><span class="line">        <span class="comment"># 打开文件</span></span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'demo1.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> fd:</span><br><span class="line">            <span class="comment"># 每128写入一次</span></span><br><span class="line">            <span class="keyword">for</span> chunk <span class="keyword">in</span> response.iter_content(<span class="number">128</span>):</span><br><span class="line">                fd.write(chunk)</span><br><span class="line"></span><br><span class="line">download_image_improved()</span><br></pre></td></tr></table></figure><h3 id="事件钩子-Event-Hooks"><a href="#事件钩子-Event-Hooks" class="headerlink" title="事件钩子(Event Hooks)"></a>事件钩子(Event Hooks)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_key_info</span><span class="params">(response, *args, **kwargs)</span>:</span></span><br><span class="line">    print(response.headers[<span class="string">'Content-Type'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    requests.get(<span class="string">'https://www.baidu.com'</span>, hooks=dict(response=get_key_info))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure><h3 id="HTTP认证"><a href="#HTTP认证" class="headerlink" title="HTTP认证"></a>HTTP认证</h3><center>![](https://ww2.sinaimg.cn/large/006tKfTcgy1fd6i610odyj31480k6gse.jpg)<br>HTTP基本认证</center><h3 id="OAUTH认证"><a href="#OAUTH认证" class="headerlink" title="OAUTH认证"></a>OAUTH认证</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://api.github.com'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_url</span><span class="params">(endpoint)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'/'</span>.join([url, endpoint])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">basic_auth</span><span class="params">()</span>:</span></span><br><span class="line">    response = requests.get(construct_url(<span class="string">'user'</span>),</span><br><span class="line">                            auth=(<span class="string">'usrname'</span>, <span class="string">'password'</span>))</span><br><span class="line">    print(response.text)</span><br><span class="line">    print(response.request.headers)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">basic_oauth</span><span class="params">()</span>:</span>                      <span class="comment"># 第一种实现</span></span><br><span class="line">    headers = &#123;<span class="string">'Authorization'</span>:<span class="string">'token xxxxPersonal access tokens'</span>&#125;   <span class="comment">#这里token是GitHub的Personal access tokens</span></span><br><span class="line">    response = requests.get(construct_url(<span class="string">'user/emails'</span>),headers=headers)</span><br><span class="line">    print(response.request.headers)</span><br><span class="line">    print(response.text)</span><br><span class="line">    print(response.status_code)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> requests.auth <span class="keyword">import</span> AuthBase      <span class="comment"># 第二种利用面向对象实现</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GithubAuth</span><span class="params">(AuthBase)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, token)</span>:</span></span><br><span class="line">        self.token = token</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, r)</span>:</span></span><br><span class="line">        <span class="comment"># requests 加 headers</span></span><br><span class="line">        r.headers[<span class="string">'Authorization'</span>]=<span class="string">' '</span>.join([<span class="string">'token'</span>,self.token])</span><br><span class="line">        <span class="keyword">return</span> r</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">oauth_advanced</span><span class="params">()</span>:</span></span><br><span class="line">    auth = GithubAuth(<span class="string">'xxxxPersonal access tokens'</span>)</span><br><span class="line">    response = requests.get(construct_url(<span class="string">'user/emails'</span>),auth=auth)</span><br><span class="line">    print(response.request.headers)</span><br><span class="line">    print(response.text)</span><br><span class="line">    print(response.status_code)</span><br><span class="line"></span><br><span class="line">basic_oauth()</span><br><span class="line">oauth_advanced()</span><br></pre></td></tr></table></figure><h3 id="代理-Proxy"><a href="#代理-Proxy" class="headerlink" title="代理(Proxy)"></a>代理(Proxy)</h3><ol><li>启动代理服务Heroku</li><li>在主机1080端口启动Socks服务</li><li>将请求转发到1080端口</li><li>获取相应资源</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'socks5://127.0.0.1:1080'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'socks5://127.0.0.1:1080'</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">'https://www.facebook.com'</span></span><br><span class="line">response = requests.get(url, timeout=<span class="number">3</span>)  <span class="comment"># 这里没有应用代理访问不了</span></span><br><span class="line">response = requests.get(url, proxies=proxies, timeout=<span class="number">3</span>) <span class="comment"># 这里成功访问</span></span><br><span class="line">print(response.status_code) <span class="comment"># 返回200</span></span><br></pre></td></tr></table></figure><h3 id="Session-和-Cookie"><a href="#Session-和-Cookie" class="headerlink" title="Session 和 Cookie"></a>Session 和 Cookie</h3><ul><li>Session  服务器存储数据</li><li>Cookie 浏览器存储数据</li></ul><blockquote><p>Session 的作用就是它在 Web服务器上保持用户的状态信息供在任何时间从任何设备上的页面进行访问。因为浏览器不需要存储任何这种信息，所以可以使用任何浏览器，即使是像 Pad 或手机这样的浏览器设备。</p></blockquote><blockquote><p>“Cookie”是小量信息，由网络服务器发送出来以存储在网络浏览器上，从而下次这位独一无二的访客又回到该网络服务器时，可从该浏览器读回此信息。这是很有用的，让浏览器记住这位访客的特定信息，如上次访问的位置、花费的时间或用户首选项（如样式表）。Cookie 是个存储在浏览器目录的文本文件，当浏览器运行时，存储在 RAM 中。一旦你从该网站或网络服务器退出，Cookie 也可存储在计算机的硬驱上。当访客结束其浏览器对话时，即终止的所有 Cookie。</p></blockquote><hr><p>源码(python3.x)改自iMOOC的课程<br><a href="http://www.imooc.com/learn/736" target="_blank" rel="noopener">Python-走进Requests库</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> Spider </category>
          
          <category> Requests </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dropout一些总结</title>
      <link href="/2017/02/21/AboutDropout/"/>
      <url>/2017/02/21/AboutDropout/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>关于DROPOUT一些总结</b></font></center><a id="more"></a><h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><hr><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p><a href="https://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="noopener">这是Hinton大神的原论文《Improving neural networks by preventing co-adaptation of feature Detectors》</a><br></p><p>从以往的神经网络来看，大规模的网络：</p><ul><li>训练费时</li><li>易过拟合</li></ul><p>这时候Dropout的出现很好的解决了其中的问题。 <br></p><p>从原文来看大量的实验数据说明Dropout确实能够提高准确率。</p><p>下面简单说明一下<strong>Dropout</strong>的原理及作用： <br><br>Dropout是指在训练模型时随机让网络的某一些节点不工作，他们的权重得以保留(只是暂时不更新)，下次可能又更新了。它是为了提高Deep Network的泛化能力，防止CNN的过拟合。<br></p><p>在训练开始时，我们随机地“删除”一半的隐藏单元，视它们为不存在。</p><p><img src="https://ww1.sinaimg.cn/large/006tKfTcgy1fcy6rfw5r2j30gp075q40.jpg" alt><br><br>这张图左边表示没有Dropout的普通2层全连接结构，用公式记做<img src="http://chart.googleapis.com/chart?cht=tx&chl= r = a(Wv)" style="border:none;">,其中的a为激活函数。<br></p><p>右图表示在第二个全连接层加入了Dropout的示意图，用公式记做<img src="http://chart.googleapis.com/chart?cht=tx&chl= r = m.\ast a(Wv)" style="border:none;">，即在训练时随机(以概率1-p)使某些网络的结点不工作(将隐含的结点输出清0)。</p><blockquote><p>其中v是n*1维的列向量，W是d*n维的矩阵，m是个d*1的01列向量，a(x)是一个满足a(0)=0的激活函数，这里的m和a(Wv)相乘是对应元素相乘。</p></blockquote><p>它为什么有助于防止过拟合呢？可以简单地这样解释，运用了Dropout的训练过程，相当于训练了很多个只有半数隐层单元的神经网络（后面简称为“半数网络”），每一个这样的半数网络，都可以给出一个分类结果，这些结果有的是正确的，有的是错误的。随着训练的进行，大部分半数网络都可以给出正确的分类结果，那么少数的错误分类结果就不会对最终结果造成大的影响。<br>从实验结果来看，它会降低训练集的效果，但会提高验证集的效果。<br><br><br><img src="https://ww4.sinaimg.cn/large/006tKfTcgy1fcy6pjg26qj30o109twgb.jpg" alt><br><br></p><blockquote><p>右图是Dropout的一种延伸的方法<a href="http://yann.lecun.com/exdb/publis/pdf/wan-icml-13.pdf" target="_blank" rel="noopener">DropConnect</a>，用表达式描述为：<img src="http://chart.googleapis.com/chart?cht=tx&chl= r = a((M.\ast W)v)" style="border:none;"><br><br>二者的区别很明显：Dropout是将随机输出置0，而DropConnect是将权重随机置0.</p><blockquote><p>与Dropout不同的是，这个不是随机将隐含层节点的输出清0，而是将节点中的每个与其相连的输入权值以1-p的概率清0。（一个是输出，一个是输入）</p></blockquote></blockquote><br>### 对于Dropout *Hintion给出的解释*：1. 由于每次用输入网络的样本进行权值更新时，隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现，这样权值的更新不再依赖于有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。2. 可以将Dropout看作是模型平均的一种。对于每次输入到网络中的样本（可能是一个样本，也可能是一个Batch的样本），其对应的网络结构都是不同的，但所有的这些不同的网络结构又同时share隐含节点的权值。这样不同的样本就对应不同的模型，是Bagging的一种极端情况。个人感觉这个解释稍微靠谱些，和Bagging，Boosting理论有点像，但又不完全相同。Bagging->一个特例是Dropout，Dropout->一个特例是Naive Bayes3. Native Bayes是Dropout的一个特例。Native bayes有个错误的前提，即假设各个特征之间相互独立，这样在训练样本比较少的情况下，单独对每个特征进行学习，测试时将所有的特征都相乘，且在实际应用时效果还不错。而Droput每次不是训练一个特征，而是一部分隐含层特征。4. 还有一个比较有意思的解释是，Dropout类似于性别在生物进化中的角色，物种为了使适应不断变化的环境，性别的出现有效的阻止了过拟合，即避免环境改变时物种可能面临的灭亡。<pre><code>&gt;这是原文：</code></pre><blockquote><p>Finally, there is an intriguing similarity between dropout and a recent theory of the role of sex in evolution (17). One possible interpretation of the theory of mixability articulated in (17) is that sex breaks up sets of co-adapted genes and this means that achieving a function by using a large set of co-adapted genes is not nearly as robust as achieving the same function, perhaps less than optimally, in multiple alternative ways, each of which only uses a small number of co-adapted genes. This allows evolution to avoid dead-ends in which improvements in fitness require co- ordinated changes to a large number of co-adapted genes. It also reduces the proba- bility that small changes in the environment will cause large decreases in fitness a phenomenon which is known as “overfitting” in the field of machine learning.</p></blockquote><br><h3 id="其他说明及资料"><a href="#其他说明及资料" class="headerlink" title="其他说明及资料"></a>其他说明及资料</h3><p><a href="http://blog.csdn.net/stdcoutzyx/article/details/49022443" target="_blank" rel="noopener">这篇博文写的非常全面</a></p><ul><li><p>防止过拟合：</p><ul><li>效果变差时提前终止训练</li><li>L1和L2正则化</li><li>软权值共享</li><li>Dropout</li></ul></li><li><p>Dropout率的选择</p><ul><li>隐含结点的Dropout率等于0.5的时候效果最好，原因是0.5的时候随机生成的网络结构最多</li><li>Dropout也可以作为一种添加噪声的方法，直接对input进行操作。输出层设为更接近1的数。使得输入变化不会太大（0.8）</li></ul></li><li><p>原文没有给出具体的数学原理 <br><br>这里有一篇论文是用贝叶斯机率说明Dropout的原理：<br><br><a href="http://mlg.eng.cam.ac.uk/yarin/PDFs/Dropout_as_a_Bayesian_approximation.pdf" target="_blank" rel="noopener">Dropout as a Bayesian Approximation: Insights and Applications</a></p></li><li><p>这里：<br><a href="https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning" target="_blank" rel="noopener">How does the dropout method work in deep learning?</a></p></li><li><p><a href="https://arxiv.org/pdf/1312.6197.pdf" target="_blank" rel="noopener">An empirical analysis of dropout in piecewise linear networks</a></p></li></ul><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><p><a href="http://www.cnblogs.com/tornadomeet/p/3258122.html" target="_blank" rel="noopener">http://www.cnblogs.com/tornadomeet/p/3258122.html</a><br><a href="http://blog.csdn.net/shuzfan/article/details/50580915" target="_blank" rel="noopener">http://blog.csdn.net/shuzfan/article/details/50580915</a><br><a href="http://blog.csdn.net/u012162613/article/details/44261657" target="_blank" rel="noopener">http://blog.csdn.net/u012162613/article/details/44261657</a><br><a href="http://blog.csdn.net/stdcoutzyx/article/details/49022443" target="_blank" rel="noopener">http://blog.csdn.net/stdcoutzyx/article/details/49022443</a></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Read Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Fast RCNN</title>
      <link href="/2017/02/21/FastRCNN/"/>
      <url>/2017/02/21/FastRCNN/</url>
      
        <content type="html"><![CDATA[<center><font size="5" color="red"><b>FAST-RCNN</b></font></center><a id="more"></a><h1 id="FAST-RCNN"><a href="#FAST-RCNN" class="headerlink" title="FAST-RCNN"></a>FAST-RCNN</h1><hr><p>这篇文章主要介绍比起传统的R-CNN的优点，它是R-CNN和SPPnet的进阶版。主要摘自<a href="http://blog.csdn.net/liumaolincycle/article/details/50203333" target="_blank" rel="noopener">这里</a></p><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><ul><li>训练要经过多阶段。首先要提取特征微调ConvNet，再用线性SVM处理proposal，计算得到的ConvNet特征，然后进行用bounding box回归。</li><li>训练时间和空间开销大。要从每一张图像上提取大量proposal，还要从每个proposal中提取特征，并存到磁盘中。 </li><li>测试时间开销大。同样是要从每个测试图像上提取大量proposal，再从每个proposal中提取特征来进行检测过程，可想而知是很慢的。<br></li></ul><h3 id="SPPnet"><a href="#SPPnet" class="headerlink" title="SPPnet:"></a>SPPnet:</h3><ul><li>SPP已有一定的速度提升，它在ConvNet的最后一个卷积层才提取proposal，但是依然有不足之处。和R-CNN一样，它的训练要经过多个阶段，特征也要存在磁盘中，另外，SPP中的微调只更新spp层后面的全连接层，对很深的网络这样肯定是不行的。</li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="针对R-CNN和SPPnet缺点，FRCNN有如下优点："><a href="#针对R-CNN和SPPnet缺点，FRCNN有如下优点：" class="headerlink" title="针对R-CNN和SPPnet缺点，FRCNN有如下优点："></a>针对R-CNN和SPPnet缺点，FRCNN有如下优点：</h3><ul><li><p>比R-CNN更高的检测质量(mAP)</p></li><li><p>把多个任务的损失函数写到一起，实现单级训练</p></li><li><p>在训练时可更新所有层</p></li><li><p>不需要在磁盘中存储特征</p><p>  整个结构如下图所示<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgy1fcxudqu6sjj30ey05tt9q.jpg" alt></p><blockquote><p>Fast R-CNN architecture. An input image and multi- ple regions of interest (RoIs) are input into a fully convolutional network. Each RoI is pooled into a fixed-size feature map and then mapped to a feature vector by fully connected layers (FCs). The network has two output vectors per RoI: softmax probabilities and per-class bounding-box regression offsets. The architecture is trained end-to-end with a multi-task loss.</p></blockquote></li></ul><p>大概过程就是，用<a href="http://blog.csdn.net/charwing/article/details/27180421" target="_blank" rel="noopener">selective search</a>在一张图片中生成约2000个object proposal，即RoI。把它们整体输入到全卷积的网络中，在最后一个卷积层上对每个ROI求映射关系，并用一个RoI pooling layer来统一到相同的大小。继续经过两个全连接层（FC）得到特征向量。特征向量经由各自的FC层，得到两个输出向量：第一个是分类，使用softmax，第二个是每一类的<a href="http://blog.csdn.net/u013832707/article/details/53641055" target="_blank" rel="noopener">bounding box回归</a>。</p><h3 id="ROI-POOLING-LAYER"><a href="#ROI-POOLING-LAYER" class="headerlink" title="ROI POOLING LAYER"></a>ROI POOLING LAYER</h3><p>ROI(Regions of interest)<br>是SPP pooling层的简化版，只有一级“金字塔”</p><blockquote><p>The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small fea- ture map with a fixed spatial extent of H × W (e.g., 7 × 7), where H and W are layer hyper-parameters that are inde- pendent of any particular RoI. In this paper, an RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple (r, c, h, w) that specifies its top-left corner (r, c) and its height and width (h, w).</p></blockquote><blockquote><p>RoI max pooling works by dividing the h × w RoI win- dow into an H × W grid of sub-windows of approximate size h/H × w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. Pool- ing is applied independently to each feature map channel, as in standard max pooling. The RoI layer is simply the special-case of the spatial pyramid pooling layer used in <code>SPPnets</code> in which there is only one pyramid level. We use the pooling sub-window calculation given in <code>SPPnets</code>.</p></blockquote><br>[这里有池化的具体介绍](http://blog.csdn.net/mao_kun/article/details/50507376) <br>空间金字塔池化可以把任何尺度的图像的卷积特征转化成相同维度，这不仅可以让CNN处理任意尺度的图像，还能避免cropping和warping操作，导致一些信息的丢失，具有非常重要的意义。<h3 id="PRE-TRAINED-NETWORD"><a href="#PRE-TRAINED-NETWORD" class="headerlink" title="PRE-TRAINED NETWORD"></a>PRE-TRAINED NETWORD</h3><p>用了三个预训练的<code>ImageNet网络</code>(CaffeNet/VGG_CNN_M_1024/VGG16),预训练的网络初始化FRCNN要经过三次变形：</p><ul><li><p>最后一个max pooling层替换为RoI pooling层，设置 H 和 W 与第一个全连接层兼容(e.g.H = W = 7 for VGG16)。</p><blockquote><p>First, the last max pooling layer is replaced by a RoI pooling layer that is configured by setting H and W to be compatible with the net’s first fully connected layer (e.g., H =W =7forVGG16).</p></blockquote></li><li><p>最后一个全连接层和softmax（原本是1000个类）替换为softmax的对K+1个类别的分类层，和bounding box 回归层。</p><blockquote><p>Second,the network’s last fully connected layer and soft- max (which were trained for 1000-way ImageNet classifi- cation) are replaced with the two sibling layers described earlier (a fully connected layer and softmax over K + 1 cat- egories and category-specific bounding-box regressors). </p></blockquote></li><li><p>输入修改为两种数据：一组N个图形，R个RoI，batch size和ROI数、图像分辨率都是可变的。</p><blockquote><p>Third, the network is modified to take two data inputs: a list of images and a list of RoIs in those images.</p></blockquote></li></ul><h3 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h3><h4 id="论文中提到了几个点："><a href="#论文中提到了几个点：" class="headerlink" title="论文中提到了几个点："></a>论文中提到了几个点：</h4><ul><li>Multi-task loss</li><li>Mini-batch sampling</li><li>Back-propagation through RoI pooling layers</li><li>SGD hyper-parameters</li></ul><p>直接看上面那个<a href="http://blog.csdn.net/liumaolincycle/article/details/50203333" target="_blank" rel="noopener">博客</a>的中文分析吧</p><h2 id="另外"><a href="#另外" class="headerlink" title="另外"></a>另外</h2><p><a href="https://github.com/rbgirshick/fast-rcnn" target="_blank" rel="noopener">Fast-RCNN代码</a><br><br><a href="https://arxiv.org/pdf/1504.08083v2.pdf" target="_blank" rel="noopener">原论文</a></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Read Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
